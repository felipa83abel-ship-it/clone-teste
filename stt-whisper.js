/**
 * üé§ WHISPER STT (Speech-to-Text) - M√ìDULO INDEPENDENTE
 *
 * Implementa√ß√£o isolada de transcri√ß√£o com Whisper (OpenAI + Local).
 * - Suporte a whisper-1 (online, API OpenAI)
 * - Suporte a whisper-cpp-local (offline, alta precis√£o)
 * - Captura de √°udio via MediaRecorder + AudioWorklet
 * - Detec√ß√£o de sil√™ncio autom√°tica (sem streaming, mas com VAD)
 * - Transcri√ß√£o batch com auto-trigger por sil√™ncio
 *
 * Uso:
 * - startAudioWhisper(UIElements)
 * - stopAudioWhisper()
 * - switchDeviceWhisper(source, UIElements)
 */

/* ================================ */
//	IMPORTS
/* ================================ */

const { ipcRenderer } = require('electron');
const { getVADEngine } = require('./vad-engine');
const fs = require('node:fs');
const path = require('node:path');
const os = require('node:os');
const { promisify } = require('node:util');
const ffmpeg = require('fluent-ffmpeg');
const OpenAI = require('openai');

const { execFile } = require('node:child_process');
const execFileAsync = promisify(execFile);

const ffmpegStatic = require('ffmpeg-static');
if (ffmpegStatic) {
	ffmpeg.setFfmpegPath(ffmpegStatic);
}

/* ================================ */
//	CONSTANTES
/* ================================ */

// Configura√ß√£o Geral
const INPUT = 'input';
const OUTPUT = 'output';

// Configura√ß√£o de √Åudio 16kHz
const AUDIO_MIME_TYPE = 'audio/webm';
const AUDIO_SAMPLE_RATE = 16000; // Hz

// AudioWorkletProcessor
const STT_AUDIO_WORKLET_PROCESSOR = 'stt-audio-worklet-processor';
const AUDIO_WORKLET_PROCESSOR_PATH = './stt-audio-worklet-processor.js';

// Detec√ß√£o de sil√™ncio
const SILENCE_TIMEOUT_INPUT = 500; // ms para entrada (microfone)
const SILENCE_TIMEOUT_OUTPUT = 700; // ms para sa√≠da (sistema)
const MINIMUM_CAPTURE_BYTES = 2048; // evita WebMs min√∫sculos que quebram o ffmpeg

// Configura√ß√£o Whisper Local
const WHISPER_CLI_EXE = path.join(__dirname, 'whisper-local', 'bin', 'whisper-cli.exe');
const WHISPER_MODEL = path.join(__dirname, 'whisper-local', 'models', 'ggml-tiny.bin');
const WHISPER_LOCAL_TIMEOUT_MS = 10000;
const WHISPER_LOCAL_PARTIAL_TIMEOUT_MS = 1500;
const WHISPER_WARMUP_FILENAME = 'whisper-warmup.wav';
const WARMUP_DURATION_SECONDS = 1;
const WARMUP_SAMPLE_RATE = 16000;

// VAD Engine
let vad = null;

/* ================================ */
//	ESTADO GLOBAL DO WHISPER
/* ================================ */

// whisperState mant√©m seu pr√≥prio estado interno
const whisperState = {
	input: {
		_isActive: false,
		_stream: null,
		_mediaRecorder: null,
		_audioChunks: [],
		_startAt: null,
		_isSwitching: false,
		_processor: null,
		_audioContext: null,
		_source: null,

		isActive() {
			return this._isActive;
		},
		setActive(val) {
			this._isActive = val;
		},
		stream() {
			return this._stream;
		},
		setStream(val) {
			this._stream = val;
		},
		mediaRecorder() {
			return this._mediaRecorder;
		},
		setMediaRecorder(val) {
			this._mediaRecorder = val;
		},
		audioChunks() {
			return this._audioChunks;
		},
		setAudioChunks(val) {
			this._audioChunks = val;
		},
		startAt() {
			return this._startAt;
		},
		setStartAt(val) {
			this._startAt = val;
		},
		isSwitching() {
			return this._isSwitching;
		},
		setIsSwitching(val) {
			this._isSwitching = val;
		},
		processor() {
			return this._processor;
		},
		setProcessor(val) {
			this._processor = val;
		},
		audioContext() {
			return this._audioContext;
		},
		setAudioContext(val) {
			this._audioContext = val;
		},
		source() {
			return this._source;
		},
		setSource(val) {
			this._source = val;
		},

		author: 'Voc√™',
		lastTranscript: '',
		inSilence: false,
		lastPercent: 0,
		shouldFinalizeAskCurrent: false,
		_lastIsSpeech: false,
		_lastVADTimestamp: null,
		lastActive: null,
		vadWindow: [],
		noiseStartTime: null,
		noiseStopTime: null,
	},
	output: {
		_isActive: false,
		_stream: null,
		_mediaRecorder: null,
		_audioChunks: [],
		_startAt: null,
		_isSwitching: false,
		_processor: null,
		_audioContext: null,
		_source: null,

		isActive() {
			return this._isActive;
		},
		setActive(val) {
			this._isActive = val;
		},
		stream() {
			return this._stream;
		},
		setStream(val) {
			this._stream = val;
		},
		mediaRecorder() {
			return this._mediaRecorder;
		},
		setMediaRecorder(val) {
			this._mediaRecorder = val;
		},
		audioChunks() {
			return this._audioChunks;
		},
		setAudioChunks(val) {
			this._audioChunks = val;
		},
		startAt() {
			return this._startAt;
		},
		setStartAt(val) {
			this._startAt = val;
		},
		isSwitching() {
			return this._isSwitching;
		},
		setIsSwitching(val) {
			this._isSwitching = val;
		},
		processor() {
			return this._processor;
		},
		setProcessor(val) {
			this._processor = val;
		},
		audioContext() {
			return this._audioContext;
		},
		setAudioContext(val) {
			this._audioContext = val;
		},
		source() {
			return this._source;
		},
		setSource(val) {
			this._source = val;
		},

		author: 'Outros',
		lastTranscript: '',
		inSilence: false,
		lastPercent: 0,
		shouldFinalizeAskCurrent: false,
		_lastIsSpeech: false,
		_lastVADTimestamp: null,
		lastActive: null,
		vadWindow: [],
		noiseStartTime: null,
		noiseStopTime: null,
	},
};

let openaiClient = null;
let whisperLocalReady = false;
let whisperLocalWarmupPromise = null;

/* ================================ */
//	SERVI√áO WHISPER
/* ================================ */

// Garante que o cliente OpenAI esteja inicializado
async function ensureOpenAIClient() {
	if (openaiClient) return true;
	return initializeOpenAIClient();
}

// Inicializa o cliente OpenAI
async function initializeOpenAIClient(apiKey = null) {
	const key = apiKey || (await ipcRenderer.invoke('GET_API_KEY', 'openai'));
	if (!key || key.trim().length < 10) {
		throw new Error('Chave OpenAI n√£o encontrada ou inv√°lida');
	}

	openaiClient = new OpenAI({ apiKey: key.trim() });
	debugLogWhisper('‚úÖ Cliente OpenAI inicializado dentro do Whisper', false);
	return true;
}

// Reseta o cliente OpenAI (para reautentica√ß√£o)
function resetOpenAIClient() {
	openaiClient = null;
}

// Verifica se os arquivos do Whisper.cpp existem
function checkWhisperFiles() {
	const exeExists = fs.existsSync(WHISPER_CLI_EXE);
	const modelExists = fs.existsSync(WHISPER_MODEL);
	return exeExists && modelExists;
}

// Realiza warm-up do Whisper Local
async function warmupWhisperLocal() {
	if (whisperLocalReady) {
		return true;
	}

	if (whisperLocalWarmupPromise) {
		return whisperLocalWarmupPromise;
	}

	if (!checkWhisperFiles()) {
		throw new Error('Arquivos do Whisper.cpp n√£o foram encontrados para o warm-up');
	}

	const warmupPath = path.join(os.tmpdir(), WHISPER_WARMUP_FILENAME);
	whisperLocalWarmupPromise = (async () => {
		try {
			createWarmupWav(warmupPath);
			await execFileAsync(
				WHISPER_CLI_EXE,
				['-m', WHISPER_MODEL, '-f', warmupPath, '-l', 'pt', '-otxt', '-t', '4', '-np', '-nt'],
				{
					timeout: 30000,
					maxBuffer: 1024 * 1024 * 5,
				},
			);
			whisperLocalReady = true;
			return true;
		} catch (error) {
			console.error('‚ùå Warm-up Whisper falhou:', error.message);
			whisperLocalReady = false;
			throw error;
		} finally {
			removeFileIfExists(warmupPath);
			whisperLocalWarmupPromise = null;
		}
	})();

	return whisperLocalWarmupPromise;
}

function createWarmupWav(filePath) {
	const samples = WARMUP_DURATION_SECONDS * WARMUP_SAMPLE_RATE;
	const byteRate = WARMUP_SAMPLE_RATE * 2;
	const blockAlign = 2;
	const buffer = Buffer.alloc(44 + samples * 2);
	buffer.write('RIFF', 0);
	buffer.writeUInt32LE(36 + samples * 2, 4);
	buffer.write('WAVE', 8);
	buffer.write('fmt ', 12);
	buffer.writeUInt32LE(16, 16);
	buffer.writeUInt16LE(1, 20);
	buffer.writeUInt16LE(1, 22);
	buffer.writeUInt32LE(WARMUP_SAMPLE_RATE, 24);
	buffer.writeUInt32LE(byteRate, 28);
	buffer.writeUInt16LE(blockAlign, 32);
	buffer.writeUInt16LE(16, 34);
	buffer.write('data', 36);
	buffer.writeUInt32LE(samples * 2, 40);
	fs.writeFileSync(filePath, buffer);
}

// Remove arquivo tempor√°rio se existir
function removeFileIfExists(filepath) {
	if (!filepath) return;
	try {
		if (fs.existsSync(filepath)) {
			fs.unlinkSync(filepath);
		}
	} catch (error) {
		console.warn(`‚ö†Ô∏è N√£o foi poss√≠vel remover ${filepath}:`, error.message);
	}
}

// Prepara arquivo WAV a partir do buffer de √°udio WebM
async function prepareWavFile(audioBuffer, tempWebmPath, tempWavPath) {
	fs.writeFileSync(tempWebmPath, Buffer.from(audioBuffer));
	await convertWebMToWAVFile(tempWebmPath, tempWavPath);
}

// Converte WebM para WAV usando ffmpeg
function convertWebMToWAVFile(inputPath, outputPath) {
	return new Promise((resolve, reject) => {
		ffmpeg(inputPath)
			.audioCodec('pcm_s16le')
			.audioFrequency(AUDIO_SAMPLE_RATE)
			.audioChannels(1)
			.format('wav')
			.on('end', resolve)
			.on('error', err => {
				console.error('‚ùå Erro na convers√£o WebM ‚Üí WAV:', err.message);
				reject(err);
			})
			.save(outputPath);
	});
}

// Processa arquivo WAV com Whisper.cpp
async function processWhisperFile(whisperModelPath, tempWavPath, isPartial = false) {
	const args = ['-m', whisperModelPath, '-f', tempWavPath, '-l', 'pt', '-otxt', '-t', '4', '-np', '-nt'];
	if (isPartial) {
		args.push('-d', '3000', '-ml', '50');
	}

	debugLogWhisper(`üöÄ Executando Whisper: ${WHISPER_CLI_EXE} ${args.join(' ')}`, false);
	const timeout = isPartial ? WHISPER_LOCAL_PARTIAL_TIMEOUT_MS : WHISPER_LOCAL_TIMEOUT_MS;
	const { stdout } = await execFileAsync(WHISPER_CLI_EXE, args, {
		timeout,
		maxBuffer: 1024 * 1024 * 5,
	});
	return (stdout || '').trim();
}

// Log detalhado de erros do Whisper Local
function logWhisperError(execError, tempWavPath) {
	console.error(`‚ùå ERRO NA EXECU√á√ÉO DO WHISPER:`);
	console.error(`   C√≥digo: ${execError.code}`);
	console.error(`   Sinal: ${execError.signal}`);
	console.error(`   Mensagem: ${execError.message}`);
	if (execError.stderr) {
		console.error(`   STDERR do processo: ${execError.stderr}`);
	}
	if (execError.stdout) {
		console.error(`   STDOUT do processo: ${execError.stdout}`);
	}
	if (tempWavPath && fs.existsSync(tempWavPath)) {
		const stats = fs.statSync(tempWavPath);
		console.error(`   üìù WAV file existe: ${stats.size} bytes`);
	} else {
		console.error(`   ‚ùå WAV file N√ÉO EXISTE!`);
	}
}

// Transcreve √°udio com Whisper.cpp localmente
async function transcribeWithWhisperLocal(buffer, source) {
	debugLogWhisper(`üöÄ Enviando para Whisper.cpp (local, alta precis√£o)...`, true);

	if (!checkWhisperFiles()) {
		throw new Error('Arquivos do Whisper.cpp n√£o encontrados!');
	}

	await warmupWhisperLocal();

	const tempDir = os.tmpdir();
	const tempWebmPath = path.join(
		tempDir,
		`whisper-${source}-${Date.now()}-${Math.random().toString(36).slice(2)}.webm`,
	);
	const tempWavPath = tempWebmPath.replace('.webm', '.wav');

	try {
		await prepareWavFile(buffer, tempWebmPath, tempWavPath);
		const startTime = Date.now();
		const result = await processWhisperFile(WHISPER_MODEL, tempWavPath);
		debugLogWhisper(`‚úÖ Whisper.cpp conclu√≠do em ${Date.now() - startTime}ms`, true);
		return result;
	} catch (error) {
		logWhisperError(error, tempWavPath);
		throw error;
	} finally {
		removeFileIfExists(tempWebmPath);
		removeFileIfExists(tempWavPath);
	}
}

// Transcreve √°udio com Whisper-1 via OpenAI API
async function transcribeWithWhisperOpenAI(buffer) {
	const startTime = Date.now();
	await ensureOpenAIClient();

	const tempDir = os.tmpdir();
	const tempFilePath = path.join(tempDir, `whisper-openai-${Date.now()}-${Math.random().toString(36).slice(2)}.webm`);
	fs.writeFileSync(tempFilePath, buffer);

	try {
		debugLogWhisper(`üöÄ Enviando para Whisper-1 OpenAI (online)...`, true);
		const transcription = await openaiClient.audio.transcriptions.create({
			file: fs.createReadStream(tempFilePath),
			model: 'whisper-1',
			language: 'pt',
		});
		debugLogWhisper(`‚úÖ Whisper-1 conclu√≠do em ${Date.now() - startTime}ms`, true);
		return transcription.text;
	} catch (error) {
		console.error(`‚ùå Erro OpenAI Whisper:`, error.message);
		if (error.status === 401 || error.message?.includes('authentication')) {
			resetOpenAIClient();
			throw new Error('Chave da API inv√°lida ou expirada. Configure em "API e Modelos"');
		}
		throw error;
	} finally {
		removeFileIfExists(tempFilePath);
	}
}

// Transcreve √°udio com o modelo Whisper configurado
async function transcribeWhisper(audioBlob, source) {
	const sttModel = getConfiguredSTTModel();
	debugLogWhisper(`üé§ Transcri√ß√£o (${sttModel}): ${audioBlob.size} bytes`, true);

	const buffer = Buffer.from(await audioBlob.arrayBuffer());

	try {
		let result;

		if (sttModel === 'whisper-cpp-local') {
			result = await transcribeWithWhisperLocal(buffer, source);
		} else if (sttModel === 'whisper-1') {
			result = await transcribeWithWhisperOpenAI(buffer);
		} else {
			throw new Error(`Modelo Whisper desconhecido: ${sttModel}`);
		}

		debugLogWhisper(
			`üìù Resultado (${result.length} chars): "${result.substring(0, 80)}${result.length > 80 ? '...' : ''}"`,
			false,
		);
		return result;
	} catch (error) {
		console.error(`‚ùå Transcri√ß√£o Whisper falhou (${sttModel}):`, error.message);
		throw new Error(
			`Transcri√ß√£o com ${sttModel} falhou: ${error.message}. Altere o modelo em "Configura√ß√µes ‚Üí API e Modelos"`,
		);
	}
}

/* ================================ */
//	VAD (VOICE ACTIVITY DETECTION)
/* ================================ */

// Atualiza estado VAD
function updateVADState(vars, isSpeech) {
	vars._lastIsSpeech = !!isSpeech;
	vars._lastVADTimestamp = Date.now();
	if (isSpeech) vars.lastActive = Date.now();
}

/* ================================ */
//	WHISPER - INICIAR FLUXO (STT)
/* ================================ */

// // Inicia captura de √°udio do dispositivo de entrada ou sa√≠da com Whisper
async function startWhisper(source, UIElements) {
	const config = {
		input: {
			deviceKey: 'inputSelect',
			accessMessage: 'üé§ Solicitando acesso √† entrada de √°udio (Microfone)...',
			threshold: 0.02,
			startLog: '‚ñ∂Ô∏è Captura Whisper INPUT iniciada',
		},
		output: {
			deviceKey: 'outputSelect',
			accessMessage: 'üîä Solicitando acesso √† sa√≠da de √°udio (VoiceMeter/Stereo Mix)...',
			threshold: 0.005,
			startLog: '‚ñ∂Ô∏è Captura Whisper OUTPUT iniciada',
		},
	};

	const cfg = config[source];
	if (!cfg) throw new Error(`‚ùå Source inv√°lido: ${source}`);

	const vars = whisperState[source];

	if (vars.isActive()) {
		console.warn(`‚ö†Ô∏è Whisper ${source.toUpperCase()} j√° ativo`);
		return;
	}

	try {
		// Obt√©m o dispositivo selecionado no UI
		const deviceId = UIElements[cfg.deviceKey]?.value;

		debugLogWhisper(`üîä Iniciando captura ${source.toUpperCase()} com dispositivo: ${deviceId}`, false);

		// Solicita acesso ao dispositivo selecionado
		debugLogWhisper(cfg.accessMessage, false);

		// Obt√©m stream de √°udio
		const stream = await navigator.mediaDevices.getUserMedia({
			audio: {
				deviceId: { exact: deviceId },
				echoCancellation: true,
				noiseSuppression: true,
				autoGainControl: false,
			},
		});

		debugLogWhisper(`‚úÖ Acesso ao √°udio ${source.toUpperCase()} autorizado`, true);

		// Cria MediaRecorder para captura de √°udio (ANTES de AudioWorklet para ter refer√™ncia)
		const mediaRecorder = new MediaRecorder(stream, { mimeType: AUDIO_MIME_TYPE });

		// Acumula chunks conforme s√£o capturados
		const audioChunks = [];
		mediaRecorder.ondataavailable = event => {
			if (event.data.size > 0) {
				audioChunks.push(event.data);
			}
		};

		// Quando para, envia para transcri√ß√£o
		mediaRecorder.onstop = async () => {
			debugLogWhisper(`üõë MediaRecorder parado para ${source.toUpperCase()}`, false);

			if (audioChunks.length > 0) {
				const audioBlob = new Blob(audioChunks, { type: AUDIO_MIME_TYPE });
				if (audioBlob.size < MINIMUM_CAPTURE_BYTES) {
					console.warn(`‚ö†Ô∏è Captura ${source.toUpperCase()} muito curta (${audioBlob.size} bytes); pulando transcri√ß√£o`);
				} else {
					try {
						const transcribedText = await transcribeWhisper(audioBlob, source);
						handleWhisperMessage(transcribedText, source);
					} catch (error) {
						console.error(`‚ùå Erro ao transcrever ${source}:`, error.message);
					}
				}
			}

			// Limpa chunks para pr√≥xima grava√ß√£o
			audioChunks.length = 0;

			// Reinicia MediaRecorder para continuar capturando ap√≥s a transcri√ß√£o
			if (vars.isActive() && mediaRecorder.state === 'inactive') {
				try {
					mediaRecorder.start();
					debugLogWhisper(`‚ñ∂Ô∏è MediaRecorder reiniciado para ${source.toUpperCase()}`, false);
				} catch (restartError) {
					console.error(`‚ùå Erro ao reiniciar MediaRecorder (${source}):`, restartError);
				}
			}
		};

		// Cria AudioContext 16kHz para processamento em tempo real (VAD)
		const audioContext = new (globalThis.AudioContext || globalThis.webkitAudioContext)({
			sampleRate: AUDIO_SAMPLE_RATE,
		});
		await audioContext.audioWorklet.addModule(AUDIO_WORKLET_PROCESSOR_PATH);

		// Cria MediaStreamSource e guarda via whisperState
		const mediaSource = audioContext.createMediaStreamSource(stream);

		// Inicia AudioWorklet para captura e processamento de √°udio em tempo real
		const processor = new AudioWorkletNode(audioContext, STT_AUDIO_WORKLET_PROCESSOR);
		processor.port.postMessage({ type: 'setThreshold', threshold: cfg.threshold });
		processor.port.onmessage = event => {
			// Processa mensagens do AudioWorklet (audioData e volumeUpdate separadamente)
			processIncomingAudioMessageWhisper(source, event.data, mediaRecorder, vars).catch(error_ =>
				console.error(`‚ùå Erro ao processar mensagem do worklet (${source}):`, error_),
			);
		};

		// Conecta fluxo: Source -> processor -> destination
		mediaSource.connect(processor);
		processor.connect(audioContext.destination);

		// Atualiza estado
		vars.setStream(stream);
		vars.setAudioContext(audioContext);
		vars.setSource(mediaSource);
		vars.setProcessor(processor);
		vars.setActive(true);
		vars.setStartAt(Date.now());
		vars.setMediaRecorder(mediaRecorder);

		// Inicia grava√ß√£o
		mediaRecorder.start();

		debugLogWhisper(cfg.startLog, true);
	} catch (error) {
		console.error(`‚ùå Erro ao iniciar Whisper ${source.toUpperCase()}:`, error);
		stopWhisper(source);
		throw error;
	}
}

// Processa mensagens de √°udio recebida do AudioWorklet
async function processIncomingAudioMessageWhisper(source, data, mediaRecorder, vars) {
	if (data.type === 'audioData') {
		// Processa chunk de √°udio PCM16
		onAudioChunkWhisper(source, data, vars);
	} else if (data.type === 'volumeUpdate') {
		vars.lastPercent = data.percent;

		// Processa atualiza√ß√£o de volume/VAD
		handleVolumeUpdate(source, data);

		// Detecta sil√™ncio e dispara transcri√ß√£o autom√°tica
		handleSilenceDetectionWhisper(source, data.percent, mediaRecorder);
	}
}

// Processa chunk de √°udio PCM16 do AudioWorklet
function onAudioChunkWhisper(source, data, vars) {
	const { pcm16 } = data;

	if (!pcm16 || pcm16.length === 0) return;

	// VAD: Detecta fala usando VAD Engine
	const isSpeech = vad?.detectSpeech(pcm16, vars.lastPercent, vars.vadWindow);
	updateVADState(vars, isSpeech);
}

// Trata detec√ß√£o de sil√™ncio com VAD ou fallback
function handleSilenceDetectionWhisper(source, percent, mediaRecorder) {
	const vars = whisperState[source];
	const silenceTimeout = source === INPUT ? SILENCE_TIMEOUT_INPUT : SILENCE_TIMEOUT_OUTPUT;
	const now = Date.now();

	// Decis√£o principal: VAD se dispon√≠vel, sen√£o fallback por volume
	const useVADDecision = vad?.isEnabled?.() && vars._lastIsSpeech !== undefined;
	const effectiveSpeech = useVADDecision ? !!vars._lastIsSpeech : percent > 0;

	debugLogWhisper(
		`üîç VAD ${source}: ${vars._lastIsSpeech ? 'speech' : 'silence'} - üîä volume: ${percent.toFixed(2)}%`,
		false,
	);

	if (effectiveSpeech) {
		// Se detectou fala, resetamos timer de sil√™ncio
		if (vars.inSilence) {
			if (!vars.noiseStartTime) vars.noiseStartTime = Date.now();

			const noiseDuration = vars.noiseStartTime - vars.noiseStopTime;
			vars.noiseStopTime = null;

			debugLogWhisper(`üü¢ üü¢ üü¢ ***** üîä Fala real detectada ap√≥s (${noiseDuration}ms) *****`, true);
		}

		vars.inSilence = false;
		vars.shouldFinalizeAskCurrent = false;
		vars.lastActive = now;
		vars.noiseStartTime = null;
	} else {
		// Sil√™ncio detectado ‚Üí verifica se j√° passou o timeout
		const elapsed = now - vars.lastActive;

		// Entrando em sil√™ncio est√°vel
		if (elapsed >= silenceTimeout && !vars.inSilence) {
			vars.inSilence = true;
			vars.shouldFinalizeAskCurrent = true;
			vars.noiseStopTime = Date.now();

			debugLogWhisper(`üî¥ üî¥ üî¥ ***** üîá Sil√™ncio est√°vel detectado (${elapsed}ms) *****`, true);

			// Dispara finalize apenas uma vez
			mediaRecorder.stop();
		}
	}
}

/* ================================ */
//	PROCESSAMENTO DE MENSAGENS
/* ================================ */

// Processa mensagens do Whisper (final ou parcial)
function handleWhisperMessage(result, source = INPUT) {
	handleFinalWhisperMessage(source, result);
}

// Processa mensagens finais do Whisper (transcri√ß√µes completas)
function handleFinalWhisperMessage(source, transcript) {
	debugLogWhisper(`üìù üü¢ Handle FINAL [${source.toUpperCase()}]: "${transcript}"`, true);

	const vars = whisperState[source];
	vars.lastTranscript = transcript.trim() ? transcript : vars.lastTranscript;

	if (transcript.trim()) {
		// Adiciona placeholder com transcri√ß√£o
		const placeholderId = `whisper-${source}-${Date.now()}-${Math.floor(Math.random() * 1000)}`;
		const metrics = calculateTimingMetrics(vars);

		// Adiciona transcri√ß√£o com placeholder na UI
		addTranscriptPlaceholder(vars.author, placeholderId, metrics.startStr);
		// Preenche placeholder com resultado final
		fillTranscriptPlaceholder(vars.author, transcript, placeholderId, metrics);
		// Limpa interim do UI
		clearInterim(source);
	}

	// Atualiza CURRENT question (apenas para output)
	updateCurrentQuestion(source, transcript, false);
}

/* ================================ */
//	HELPERS
/* ================================ */

// Obt√©m o modelo STT configurado
function getConfiguredSTTModel() {
	try {
		const activeProvider = globalThis.configManager?.config?.api?.activeProvider || 'openai';
		const sttModel = globalThis.configManager?.config?.api?.[activeProvider]?.selectedSTTModel;

		if (sttModel) {
			return sttModel;
		}

		console.warn(`‚ö†Ô∏è Modelo STT n√£o configurado para ${activeProvider}, usando padr√£o: whisper-1`);
		return 'whisper-1';
	} catch (error) {
		console.warn('‚ö†Ô∏è configManager n√£o dispon√≠vel, usando padr√£o: whisper-1', error);
		return 'whisper-1';
	}
}

// Atualiza volume recebido do AudioWorklet
function handleVolumeUpdate(source, data) {
	// Emite volume para UI
	if (globalThis.RendererAPI?.emitUIChange) {
		const ev = source === INPUT ? 'onInputVolumeUpdate' : 'onOutputVolumeUpdate';
		globalThis.RendererAPI.emitUIChange(ev, { percent: data.percent });
	}
}

// Adiciona transcri√ß√£o com placeholder ao UI
function addTranscriptPlaceholder(author, placeholderId, timeStr) {
	if (globalThis.RendererAPI?.emitUIChange) {
		globalThis.RendererAPI.emitUIChange('onTranscriptAdd', {
			author,
			text: '...',
			timeStr,
			elementId: 'conversation',
			placeholderId,
		});
	}
}

// Preenche placeholder com transcri√ß√£o final
function fillTranscriptPlaceholder(author, transcript, placeholderId, metrics) {
	if (globalThis.RendererAPI?.emitUIChange) {
		globalThis.RendererAPI.emitUIChange('onPlaceholderFulfill', {
			speaker: author,
			text: transcript,
			placeholderId,
			...metrics,
			showMeta: false,
		});
	}
}

// Limpa interim transcript do UI
function clearInterim(source) {
	const interimId = source === INPUT ? 'whisper-interim-input' : 'whisper-interim-output';
	if (globalThis.RendererAPI?.emitUIChange) {
		globalThis.RendererAPI.emitUIChange('onClearInterim', { id: interimId });
	}
}

// Atualiza interim transcript no UI
function updateInterim(source, transcript, author) {
	const interimId = source === INPUT ? 'whisper-interim-input' : 'whisper-interim-output';
	if (globalThis.RendererAPI?.emitUIChange) {
		globalThis.RendererAPI.emitUIChange('onUpdateInterim', {
			id: interimId,
			speaker: author,
			text: transcript,
		});
	}
}

// Atualiza CURRENT question (apenas para output)
function updateCurrentQuestion(source, transcript, isInterim = false) {
	const vars = whisperState[source];
	if (source === OUTPUT && globalThis.RendererAPI?.handleCurrentQuestion) {
		globalThis.RendererAPI.handleCurrentQuestion(vars.author, transcript, {
			isInterim,
			shouldFinalizeAskCurrent: vars.shouldFinalizeAskCurrent,
		});
		// üî• S√≥ reseta quando for mensagem FINAL (n√£o interim)
		if (!isInterim && vars.shouldFinalizeAskCurrent) vars.shouldFinalizeAskCurrent = false;
	}
}

// Calcula m√©tricas de timing para transcri√ß√£o
function calculateTimingMetrics(vars) {
	const startAt = vars.startAt?.();
	const now = Date.now();
	const elapsedMs = startAt ? now - startAt : 0;
	return {
		startStr: startAt ? new Date(startAt).toLocaleTimeString() : new Date(now).toLocaleTimeString(),
		stopStr: new Date(now).toLocaleTimeString(),
		recordingDuration: (elapsedMs / 1000).toFixed(2),
		latency: (elapsedMs / 1000).toFixed(2),
		total: (elapsedMs / 1000).toFixed(2),
	};
}

/* ================================ */
//	TROCA DE DISPOSITIVO
/* ================================ */

// Troca din√¢mica do dispositivo Whisper (input/output)
async function changeDeviceWhisper(source, UIElements) {
	debugLogWhisper(`üîÑ Trocando dispositivo Whisper (${source})...`, false);

	const vars = whisperState[source];
	const wasActive = vars.isActive();

	if (wasActive) {
		vars.setIsSwitching(true);
		stopWhisper(source);
	}

	// Aguarda um pouco para liberar recursos
	await new Promise(resolve => setTimeout(resolve, 300));

	if (wasActive) {
		try {
			await startWhisper(source, UIElements);
		} catch (error) {
			console.error(`‚ùå Erro ao reiniciar ap√≥s troca de dispositivo:`, error);
		} finally {
			vars.setIsSwitching(false);
		}
	}
}

/* ================================ */
//	WHISPER - PARAR FLUXO (STT)
/* ================================ */

// Para captura de √°udio
function stopWhisper(source) {
	const vars = whisperState[source];

	if (!vars.isActive()) {
		console.warn(`‚ö†Ô∏è Whisper ${source.toUpperCase()} n√£o est√° ativo`);
		return;
	}

	try {
		// Desconecta AudioWorklet
		if (vars.processor()) {
			vars.processor().disconnect();
			vars.setProcessor(null);
		}

		if (vars.source()) {
			vars.source().disconnect();
			vars.setSource(null);
		}

		// Fecha AudioContext
		if (vars.audioContext()) {
			if (vars.audioContext().state !== 'closed') {
				vars
					.audioContext()
					.close()
					.catch(err => console.warn(`‚ö†Ô∏è Erro ao fechar AudioContext:`, err));
			}
			vars.setAudioContext(null);
		}

		// Para grava√ß√£o
		if (vars.mediaRecorder() && vars.mediaRecorder().state !== 'inactive') {
			vars.mediaRecorder().stop();
		}

		// Limpa stream
		if (vars.stream()) {
			vars
				.stream()
				.getTracks()
				.forEach(track => track.stop());
		}

		// Reseta estado
		vars.setActive(false);
		vars.setStream(null);
		vars.setMediaRecorder(null);
		vars.setAudioChunks([]);

		debugLogWhisper(`üõë Captura Whisper ${source.toUpperCase()} parada`, true);
	} catch (error) {
		console.error(`‚ùå Erro ao parar Whisper ${source.toUpperCase()}:`, error);
	}
}

/* ================================ */
//	DEBUG LOG WHISPER
/* ================================ */

/**
 * Log de debug padronizado para stt-whisper.js
 * Por padr√£o nunca loga, se quiser mostrar √© s√≥ passar true.
 * @param {*} msg
 * @param {boolean} showLog - true para mostrar, false para ignorar
 */
function debugLogWhisper(...args) {
	const maybeFlag = args.at(-1);
	const showLog = typeof maybeFlag === 'boolean' ? maybeFlag : false;

	const nowLog = new Date();
	const timeStr =
		`${nowLog.getHours().toString().padStart(2, '0')}:` +
		`${nowLog.getMinutes().toString().padStart(2, '0')}:` +
		`${nowLog.getSeconds().toString().padStart(2, '0')}.` +
		`${nowLog.getMilliseconds().toString().padStart(3, '0')}`;

	if (showLog) {
		const cleanArgs = typeof maybeFlag === 'boolean' ? args.slice(0, -1) : args;
		// prettier-ignore
		console.log(
			`%c‚è±Ô∏è [${timeStr}] ü™≤ ‚ùØ‚ùØ‚ùØ‚ùØ Debug em stt-whisper.js:`, 
			'color: blue; font-weight: bold;', 
			...cleanArgs
		);
	}
}

/* ================================ */
//	INTERFACE P√öBLICA
/* ================================ */

/**
 * Inicia Whisper para INPUT + OUTPUT
 */
async function startAudioWhisper(UIElements) {
	try {
		// Inicializa VAD Engine (singleton)
		vad = getVADEngine();
		debugLogWhisper(`‚úÖ VAD Engine inicializado - Status: ${JSON.stringify(vad.getStatus())}`, true);

		// üî• Whisper: Inicia INPUT/OUTPUT
		if (UIElements.inputSelect?.value) await startWhisper(INPUT, UIElements);
		if (UIElements.outputSelect?.value) await startWhisper(OUTPUT, UIElements);
	} catch (error) {
		console.error('‚ùå Erro ao iniciar Whisper:', error);
		throw error;
	}
}

/**
 * Para Whisper para INPUT + OUTPUT
 */
function stopAudioWhisper() {
	stopWhisper(INPUT);
	stopWhisper(OUTPUT);
}

/**
 * Muda dispositivo para um source mantendo Whisper ativo
 */
async function switchDeviceWhisper(source, newDeviceId) {
	debugLogWhisper('In√≠cio da fun√ß√£o: "switchDeviceWhisper"');
	debugLogWhisper('Fim da fun√ß√£o: "switchDeviceWhisper"');
	return await changeDeviceWhisper(source, newDeviceId);
}

/* ================================ */
//	EXPORTS (CommonJS)
/* ================================ */

module.exports = {
	startAudioWhisper,
	stopAudioWhisper,
	switchDeviceWhisper,
};
