# Refatora√ß√£o Completa - AskMe v2.0

## üéØ Objetivo

Refatorar o `renderer.js` (2154 linhas) para separar responsabilidades e suportar m√∫ltiplos provedores de LLM (OpenAI, Gemini, Claude), mantendo uma √∫nica fun√ß√£o `askLLM()` centralizada.

## ‚úÖ O Que Foi Refatorado

### üì¶ Fase 0: Prepara√ß√£o
- **FASE 0.0**: Backup via git
- **FASE 0.1-0.2**: Verifica√ß√£o de depend√™ncias (npm install)
- **FASE 0.3**: Reorganiza√ß√£o de STT providers em pasta `stt/`
  - Movido: `stt-deepgram.js`, `stt-vosk.js`, `stt-whisper.js` ‚Üí `stt/`
  - Atualizado: imports em `renderer.js`
  - Testado: ‚úÖ app inicia normalmente

### üìö Fase 1: Arquitetura Base (6 Classes)

#### 1. `state/AppState.js` (120 linhas)
**O que faz**: Centraliza estado da aplica√ß√£o
- Propriedades: `audio`, `window`, `interview`, `metrics`
- M√©todos helper: `getCurrentQuestion()`, `resetCurrentQuestion()`, `addToHistory()`, `markAsAnswered()`
- Substitui: 15+ vari√°veis globais espalhadas

#### 2. `events/EventBus.js` (60 linhas)
**O que faz**: Sistema pub/sub para comunica√ß√£o desacoplada
- M√©todos: `on()`, `off()`, `emit()`, `clear()`
- Com: try/catch em callbacks para evitar crashes
- Substitui: 20+ enumera√ß√µes de eventos e listeners manuais

#### 3. `utils/Logger.js` (40 linhas)
**O que faz**: Logging estruturado com timestamps
- M√©todos: `debug()`, `info()`, `warn()`, `error()`
- Formato: `[ISO_TIMESTAMP] [LEVEL] message {data}`
- Substitui: `debugLogRenderer()`, `console.log()`, `console.error()`

#### 4. `strategies/STTStrategy.js` (70 linhas)
**O que faz**: Roteamento abstrato para STT providers
- M√©todos: `register()`, `start()`, `stop()`, `switchDevice()`
- Registro: deepgram, vosk, whisper-cpp-local, whisper-1
- Substitui: if/else manual para cada provider

#### 5. `llm/LLMManager.js` (55 linhas)
**O que faz**: Orquestrador de m√∫ltiplos LLM providers
- M√©todos: `register()`, `getHandler()`, `complete()`, `stream()`
- Interface: Consistente para todos os handlers
- Futuro: Suporta Gemini, Anthropic, etc.

#### 6. `llm/handlers/openai-handler.js` (75 linhas)
**O que faz**: Interface para OpenAI
- M√©todos: `complete()` (Promise), `stream()` (AsyncGenerator)
- Padr√£o: Singleton (module.exports = new OpenAIHandler())
- Usa: `ipcRenderer.invoke()` para comunicar com main

### üîÑ Fase 2: Integra√ß√£o em renderer.js

**Linhas afetadas**: ~100 linhas modificadas

#### 2.1 Imports Novos (linha 26)
```javascript
const { validateLLMRequest, handleLLMStream, handleLLMBatch } = require('./handlers/llmHandlers.js');
```

#### 2.2 Instancia√ß√£o (linhas 29-32)
```javascript
const appState = new AppState();
const eventBus = new EventBus();
const sttStrategy = new STTStrategy();
const llmManager = new LLMManager();
```

#### 2.3 Registros LLM (linhas 35-37)
```javascript
llmManager.register('openai', openaiHandler);
// Futuro: llmManager.register('gemini', ...);
// Futuro: llmManager.register('anthropic', ...);
```

#### 2.4 Listeners EventBus (linhas 40-68)
```javascript
eventBus.on('answerStreamChunk', data => {
  emitUIChange('onAnswerStreamChunk', { ... });
});
eventBus.on('llmStreamEnd', data => { ... });
eventBus.on('llmBatchEnd', data => { ... });
eventBus.on('error', error => { ... });
```

#### 2.5 Fun√ß√µes STT Refatoradas (linhas 660-679)

**Antes** (30 linhas cada):
```javascript
function startAudio() {
  if (sttModel === 'deepgram') startAudioDeepgram(...);
  else if (sttModel === 'vosk') startAudioVosk(...);
  else if (sttModel === 'whisper') startAudioWhisper(...);
  // ... muito if/else
}
```

**Depois** (9 linhas):
```javascript
function startAudio() {
  try {
    sttStrategy.start(sttModel, elements);
    Logger.info('startAudio', { model: sttModel });
  } catch (error) {
    Logger.error('Erro ao iniciar √°udio', { error: error.message });
  }
}
```

#### 2.6 `onAudioDeviceChanged` Refatorada (linhas 287-310)

**Antes**: 55 linhas com m√∫ltiplos listeners
**Depois**: 22 linhas com EventBus

```javascript
eventBus.on('audioDeviceChanged', async data => {
  sttStrategy.switchDevice(sttModel, data.type, data.deviceId);
  // ... fallback para onUIChange compatibilidade
});
```

### ‚ú® Fase 3: Refatora√ß√£o de LLM

#### 3.1 `handlers/llmHandlers.js` (140 linhas - NOVO)
**Quebra a fun√ß√£o gigante `askGpt()` em 3 partes**

**Fun√ß√£o 1: `validateLLMRequest()`**
- Valida: texto n√£o vazio
- Dedupe: evita reenviar mesma pergunta (CURRENT)
- Bloqueia: duplica√ß√£o em hist√≥rico (modo entrevista)
- Retorna: `{questionId, text, isCurrent}`

**Fun√ß√£o 2: `handleLLMStream()` (modo entrevista)**
- Obt√©m handler: `llmManager.getHandler('openai')`
- Itera: async generator do handler.stream()
- Emite: tokens via `eventBus.emit('answerStreamChunk')`
- Finaliza: `eventBus.emit('llmStreamEnd')`

**Fun√ß√£o 3: `handleLLMBatch()` (modo normal)**
- Obt√©m handler: `llmManager.getHandler('openai')`
- Aguarda: `handler.complete(messages)`
- Emite: resposta via `eventBus.emit('llmBatchEnd')`

#### 3.2 Refatora√ß√£o de `askGpt()` ‚Üí `askLLM()`

**Antes**: 230+ linhas com l√≥gica duplicada
```javascript
async function askGpt() {
  // 1. Valida√ß√£o (10 linhas)
  // 2. Se streaming: 170 linhas de listeners
  // 3. Se batch: 40 linhas de invoke
  // Total: duplica√ß√£o, listeners, estado confuso
}
```

**Depois**: 22 linhas, centralizadas
```javascript
async function askLLM() {
  try {
    const { questionId, text, isCurrent } = validateLLMRequest(...);
    const isInterviewMode = ModeController.isInterviewMode();
    
    if (isInterviewMode) {
      await handleLLMStream(appState, questionId, text, ...);
    } else {
      await handleLLMBatch(appState, questionId, text, ...);
    }
  } catch (error) {
    Logger.error('Erro em askLLM', { error: error.message });
    eventBus.emit('error', error.message);
  }
}
```

**Redu√ß√£o**: 230 ‚Üí 22 linhas (**90% mais curta!**)

#### 3.3 Refatora√ß√£o de `analyzeScreenshots()`

**Antes**: Emitia eventos direto com `emitUIChange()`
**Depois**: Usa `eventBus.emit('answerStreamChunk')` consistentemente

- Agora: Trata screenshot analysis como stream simulado
- Benef√≠cio: Reutiliza listener da eventBus
- Consist√™ncia: Mesmo fluxo de UI para GPT e screenshots

#### 3.4 Mock Interceptor (TODO)
- Arquivo: Mantido em renderer.js (linhas ~1501-1640)
- Status: Funcional para debug, marcado como TODO para futura remo√ß√£o
- Raz√£o: Ainda usado em modo debug, deixaremos refatora√ß√£o para depois

### üöÄ Fase 4: Templates para Outros LLMs

#### 4.1 `llm/handlers/gemini-handler.js` (125 linhas - TEMPLATE)
- Interface: Igual a openai-handler
- Status: Todo descrito, pronto para implementa√ß√£o
- Pasos: 1. npm install @google/generative-ai
         2. Obter API key em https://ai.google.dev/
         3. Descomementar c√≥digo

#### 4.2 `llm/handlers/anthropic-handler.js` (125 linhas - TEMPLATE)
- Interface: Igual a openai-handler
- Status: Todo descrito, pronto para implementa√ß√£o
- Pasos: 1. npm install @anthropic-ai/sdk
         2. Obter API key em https://console.anthropic.com/
         3. Descomementar c√≥digo

### üßπ Fase 5: Limpeza e Documenta√ß√£o

#### 5.1 Remover Coment√°rios `// antigo XPTO`
- Removidos: Todos coment√°rios de rastreamento antigo
- Arquivos: renderer.js, llmHandlers.js
- Resultado: C√≥digo clean, sem ru√≠do hist√≥rico

#### 5.2 Este Arquivo
- Documenta√ß√£o completa da refatora√ß√£o
- Padr√µes e conven√ß√µes aplicados
- Como estender com novos LLMs

## üìä Impacto da Refatora√ß√£o

### Redu√ß√£o de C√≥digo
- `askGpt()`: 230 linhas ‚Üí 22 linhas (-90%)
- `startAudio()`: 30 linhas ‚Üí 9 linhas (-70%)
- `stopAudio()`: 28 linhas ‚Üí 9 linhas (-68%)
- **Total**: ~300 linhas de c√≥digo duplicado removidas

### Melhoria de Arquitetura
- **Antes**: 1 arquivo monol√≠tico (2154 linhas)
- **Depois**: 7 arquivos bem definidos (6 classes + 1 handlers)
- **Resultado**: Separa√ß√£o de responsabilidades clara

### Suporte Multi-LLM
- **Antes**: Seria necess√°rio duplicar `askGpt()` por LLM
- **Depois**: Uma √∫nica `askLLM()` + handler por provedor
- **Escalabilidade**: Adicionar novo LLM = criar 1 classe (n√£o duplicar 200 linhas)

### Testabilidade
- **Antes**: Fun√ß√µes gigantes, acopladas, dif√≠ceis de testar
- **Depois**: Fun√ß√µes pequenas, interfaces claras, mock√°veis
- **Exemplo**: Testar `validateLLMRequest()` sem iniciar app

### Manutenibilidade
- **Logging**: Substituiu `debugLogRenderer()` por `Logger`
- **Estado**: Centralizado em `AppState` (era 15+ vari√°veis)
- **Eventos**: Desacoplados via `EventBus`
- **Resultado**: C√≥digo mais leg√≠vel e manuten√≠vel

## üîê Sem Mudan√ßas de Comportamento

### O Que Continua Igual
- ‚úÖ Fluxo de √°udio (STT)
- ‚úÖ Transcri√ß√£o (Deepgram, Vosk, Whisper)
- ‚úÖ Respostas do GPT (streaming e batch)
- ‚úÖ Interface do usu√°rio
- ‚úÖ Atalhos globais (Ctrl+D, Ctrl+Enter)
- ‚úÖ Overlay overlay behavior
- ‚úÖ Modo entrevista vs modo normal

### Modifica√ß√µes Internas Apenas
- Classes e fun√ß√µes foram reorganizadas
- Listeners foram movidos para eventBus
- Logging foi padronizado
- **Mas**: Comportamento final √© id√™ntico

## üöÄ Como Estender

### Adicionar um Novo LLM (ex: Gemini)

1. **Implementar handler** (baseado em gemini-handler.js template)
```javascript
// llm/handlers/gemini-handler.js
class GeminiHandler {
  async initialize(apiKey) { /* ... */ }
  async complete(messages) { /* ... */ }
  async *stream(messages) { /* ... */ }
}
module.exports = new GeminiHandler();
```

2. **Registrar em renderer.js**
```javascript
llmManager.register('gemini', require('./llm/handlers/gemini-handler.js'));
```

3. **Configurar no config-manager.js** (j√° suporta isso)
```javascript
// User selects "Gemini" in UI
config.llmProvider = 'gemini';
```

4. **Atualizar handlers/llmHandlers.js** (opcional, se mudar l√≥gica)
```javascript
// Trocar 'openai' por din√¢mico baseado em config
const currentLLM = config.llmProvider; // 'openai' | 'gemini' | ...
const handler = llmManager.getHandler(currentLLM);
```

**Resultado**: 4 linhas de c√≥digo, reutiliza toda a l√≥gica de streaming/batch! ‚úÖ

### Adicionar um Novo STT Provider

Mesmo padr√£o (STTStrategy):

1. **Implementar provider** (ex: stt/stt-azure.js)
2. **Registrar em renderer.js**
```javascript
sttStrategy.register('azure', { start, stop, switchDevice });
```
3. **Pronto!** J√° funciona com UI

## üìù Padr√µes Aplicados

### 1. **Strategy Pattern** (STTStrategy)
- Diferentes STT providers com mesma interface
- Sele√ß√£o din√¢mica sem if/else

### 2. **Observer/Pub-Sub Pattern** (EventBus)
- Comunica√ß√£o desacoplada
- Reduz acoplamento entre componentes

### 3. **Factory/Manager Pattern** (LLMManager)
- Centraliza cria√ß√£o e sele√ß√£o de handlers
- Interface uniforme para m√∫ltiplos providers

### 4. **Handler/Middleware Pattern** (llmHandlers)
- L√≥gica de neg√≥cio separada em fun√ß√µes puras
- Reutiliz√°vel independente do provider

## ‚úÖ Checklist de Verifica√ß√£o

- [x] Todas as classes cridas e testadas
- [x] Integra√ß√£o em renderer.js funcional
- [x] STT reorganizado em pasta
- [x] `askGpt()` ‚Üí `askLLM()` refatorado
- [x] `analyzeScreenshots()` usa eventBus
- [x] Templates Gemini e Anthropic criados
- [x] Coment√°rios de tracking removidos
- [x] App inicia sem erros
- [x] Streaming continua funcionando
- [x] Batch continua funcionando

## üîó Arquivos Modificados

### Novos Arquivos
- `state/AppState.js`
- `events/EventBus.js`
- `utils/Logger.js`
- `strategies/STTStrategy.js`
- `llm/LLMManager.js`
- `llm/handlers/openai-handler.js`
- `llm/handlers/gemini-handler.js` (template)
- `llm/handlers/anthropic-handler.js` (template)
- `handlers/llmHandlers.js`
- `README_REFACTORING.md` (este arquivo)

### Pastas Reorganizadas
- `stt/stt-deepgram.js` (movido de root)
- `stt/stt-vosk.js` (movido de root)
- `stt/stt-whisper.js` (movido de root)

### Modificados
- `renderer.js` (~100 linhas de mudan√ßas)

## üìö Leitura Recomendada

1. Comece por: `state/AppState.js` (estrutura de dados)
2. Depois: `events/EventBus.js` (comunica√ß√£o)
3. Depois: `llm/LLMManager.js` (orquestra√ß√£o)
4. Depois: `handlers/llmHandlers.js` (l√≥gica)
5. Por fim: `renderer.js` - veja como tudo se encaixa

## üéì Li√ß√µes Aprendidas

1. **Separa√ß√£o de Responsabilidades**: Classes pequenas = c√≥digo melhor
2. **Interfaces Consistentes**: Todos handlers t√™m `.complete()` e `.stream()`
3. **Pub/Sub vs Callbacks**: EventBus > 50 listeners manuais
4. **Testabilidade**: Fun√ß√µes puras s√£o mais f√°ceis de testar
5. **Escalabilidade**: Adicionar novo LLM √© trivial agora

---

**Data**: Janeiro 2025  
**Status**: ‚úÖ CONCLU√çDO E TESTADO  
**Pr√≥ximas Fases**: Implementar Gemini/Anthropic, testes de integra√ß√£o completos
