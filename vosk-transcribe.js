/**
 * üî• VOSK TRANSCRIBE - STREAMING CONT√çNUO (COMO DEEPGRAM)
 *
 * NOVA ABORDAGEM:
 * - Mant√©m MediaRecorder SEMPRE ativo (nunca fecha)
 * - Envia chunks de √°udio continuamente via IPC (n√£o espera por sil√™ncio)
 * - Transcri√ß√£o incremental: recebe resultados parciais e finais do Vosk
 * - Mant√©m Vosk "vivo" entre frases (sem reiniciar contexto)
 *
 * Padr√£o id√™ntico ao Deepgram: capture -> send chunks -> detect silence -> finalize -> continue
 */

/* ================================ */
//	IMPORTS
/* ================================ */
const { ipcRenderer } = require('electron');

/* ================================ */
//	CONSTANTES
/* ================================ */

const INPUT = 'input';
const OUTPUT = 'output';

// Chunk size para envio cont√≠nuo de √°udio (500ms de √°udio a 16kHz = ~16KB)
const CHUNK_SEND_INTERVAL_MS = 500;

// üî• AUDIO WORKLET - Usar o mesmo que Deepgram usa
const AUDIO_CONTEXT_WORKLET_PATH = './deepgram-audio-worklet-processor.js';

// Timeouts de sil√™ncio para finalizar transcri√ß√£o
const SILENCE_TIMEOUT_INPUT = 500; // ms para entrada
const SILENCE_TIMEOUT_OUTPUT = 700; // ms para sa√≠da

// üî• VAD (Voice Activity Detection) - COPIADO EXATAMENTE DO DEEPGRAM
const AUDIO_SAMPLE_RATE = 16000; // Hz
const VAD_FRAME_DURATION_MS = 0.03; // 30ms por frame (id√™ntico ao Deepgram)
const VAD_WINDOW_SIZE = 6; // √öltimos ~6 frames (~50-100ms, id√™ntico ao Deepgram)
const FALLBACK_VOLUME_THRESHOLD = 20; // Limiar de volume para fallback (id√™ntico ao Deepgram)

// Configura√ß√£o VAD nativa
let useNativeVADVosk = true;
let vadAvailableVosk = false;
let vadInstanceVosk = null;

/* ================================ */
//	ESTADO DO VOSK
/* ================================ */

const voskVars = {
	input: {
		_isActive: false,
		_stream: null,
		_audioContext: null,
		_processor: null, // AudioWorkletNode (substitui ScriptProcessorNode)
		_source: null, // MediaStreamSource
		_startAt: null,
		_chunkBuffer: [], // Buffer de chunks para envio cont√≠nuo
		_chunkSendTimer: null,
		_vadLastSpeechTime: null, // √öltimo momento que detectou fala (para VAD)
		_lastPercent: 0, // √öltimo valor de volume medido (%)
		_lastIsSpeech: false, // √öltimo resultado de VAD
		_lastVADTimestamp: null, // √öltimo timestamp de VAD
		vadWindow: [], // Janela deslizante de volume (para fallback suavizado)

		isActive() {
			return this._isActive;
		},
		setActive(val) {
			this._isActive = val;
		},
		stream() {
			return this._stream;
		},
		setStream(val) {
			this._stream = val;
		},
		processor() {
			return this._processor;
		},
		setProcessor(val) {
			this._processor = val;
		},
		source() {
			return this._source;
		},
		setSource(val) {
			this._source = val;
		},
		audioContext() {
			return this._audioContext;
		},
		setAudioContext(val) {
			this._audioContext = val;
		},
		recorder() {
			return this._recorder;
		},
		setRecorder(val) {
			this._recorder = val;
		},
		startAt() {
			return this._startAt;
		},
		setStartAt(val) {
			this._startAt = val;
		},
		chunkBuffer() {
			return this._chunkBuffer;
		},
		clearChunkBuffer() {
			this._chunkBuffer = [];
		},
		chunkSendTimer() {
			return this._chunkSendTimer;
		},
		setChunkSendTimer(val) {
			this._chunkSendTimer = val;
		},

		author: 'Voc√™',
		lastActive: null,
		lastTranscript: '',
		inSilence: false,
		lastPercent: 0,
		noiseStartTime: null,
		noiseStopTime: null,
		shouldFinalizeAskCurrent: false,
	},
	output: {
		_isActive: false,
		_stream: null,
		_audioContext: null,
		_processor: null, // AudioWorkletNode
		_source: null, // MediaStreamSource
		_startAt: null,
		_chunkBuffer: [],
		_chunkSendTimer: null,
		_vadLastSpeechTime: null, // √öltimo momento que detectou fala (para VAD)
		_lastPercent: 0, // √öltimo valor de volume medido (%)
		_lastIsSpeech: false, // √öltimo resultado de VAD
		_lastVADTimestamp: null, // √öltimo timestamp de VAD
		vadWindow: [], // Janela deslizante de volume (para fallback suavizado)

		isActive() {
			return this._isActive;
		},
		setActive(val) {
			this._isActive = val;
		},
		stream() {
			return this._stream;
		},
		setStream(val) {
			this._stream = val;
		},
		processor() {
			return this._processor;
		},
		setProcessor(val) {
			this._processor = val;
		},
		source() {
			return this._source;
		},
		setSource(val) {
			this._source = val;
		},
		audioContext() {
			return this._audioContext;
		},
		setAudioContext(val) {
			this._audioContext = val;
		},
		recorder() {
			return this._recorder;
		},
		setRecorder(val) {
			this._recorder = val;
		},
		startAt() {
			return this._startAt;
		},
		setStartAt(val) {
			this._startAt = val;
		},
		chunkBuffer() {
			return this._chunkBuffer;
		},
		clearChunkBuffer() {
			this._chunkBuffer = [];
		},
		chunkSendTimer() {
			return this._chunkSendTimer;
		},
		setChunkSendTimer(val) {
			this._chunkSendTimer = val;
		},

		author: 'Outros',
		lastActive: null,
		lastTranscript: '',
		inSilence: false,
		lastPercent: 0,
		noiseStartTime: null,
		noiseStopTime: null,
		shouldFinalizeAskCurrent: false,
	},
};

/* ================================ */
//	INICIALIZA√á√ÉO
/* ================================ */

/**
 * Inicia captura cont√≠nua de √°udio (nunca para at√© stopVosk ser chamado)
 */
async function startVosk(source, UIElements) {
	const config = {
		input: {
			deviceKey: 'inputSelect',
			accessMessage: 'üé§ Solicitando acesso √† entrada de √°udio (Microfone)...',
			startLog: '‚ñ∂Ô∏è Captura Vosk INPUT iniciada (STREAMING CONT√çNUO)',
		},
		output: {
			deviceKey: 'outputSelect',
			accessMessage: 'üîä Solicitando acesso √† sa√≠da de √°udio (VoiceMeter/Stereo Mix)...',
			startLog: '‚ñ∂Ô∏è Captura Vosk OUTPUT iniciada (STREAMING CONT√çNUO)',
		},
	};

	const cfg = config[source];
	if (!cfg) throw new Error(`‚ùå Source inv√°lido: ${source}`);

	const vars = voskVars[source];

	if (vars.isActive?.()) {
		console.warn(`‚ö†Ô∏è Vosk ${source.toUpperCase()} j√° ativo`);
		return;
	}

	try {
		console.log(cfg.accessMessage);

		// üî• INICIALIZA VAD NATIVO (id√™ntico ao Deepgram)
		console.log('üî• Inicializando VAD nativo...');
		if (!vadInstanceVosk && !vadAvailableVosk) {
			try {
				vadInstanceVosk = initVADVosk();
				vadAvailableVosk = !!vadInstanceVosk;
				if (vadAvailableVosk) {
					console.log('‚úÖ VAD nativo dispon√≠vel (webrtcvad)');
				} else {
					console.log('‚ö†Ô∏è VAD nativo n√£o dispon√≠vel, usando fallback por volume');
				}
			} catch (e) {
				console.warn('‚ö†Ô∏è Erro ao inicializar VAD:', e.message || e);
				vadAvailableVosk = false;
			}
		}

		// üî• SEGUNDO: Inicializa servidor Vosk antes de come√ßar a capturar √°udio
		console.log('üî• Inicializando servidor Vosk Python...');
		try {
			await ipcRenderer.invoke('vosk-init-server');
			console.log('‚úÖ Servidor Vosk inicializado e pronto');
		} catch (error) {
			console.error('‚ùå Erro ao inicializar servidor Vosk:', error);
			throw error;
		}

		const deviceId = UIElements[cfg.deviceKey]?.value;
		if (!deviceId) {
			console.warn(`‚ö†Ô∏è Nenhum dispositivo ${source} selecionado`);
			return;
		}

		// Obt√©m stream de √°udio
		const stream = await navigator.mediaDevices.getUserMedia({
			audio: {
				deviceId: { exact: deviceId },
				echoCancellation: true,
				noiseSuppression: true,
				autoGainControl: false,
			},
		});

		console.log(`‚úÖ Acesso ao √°udio ${source.toUpperCase()} autorizado`);

		// üî• NOVO: Usa AudioWorkletNode (id√™ntico ao Deepgram)
		// Isso fornece frames de tamanho adequado para o webrtcvad (320, 640 ou 960 samples)
		const audioCtx = new (globalThis.AudioContext || globalThis.webkitAudioContext)({ sampleRate: AUDIO_SAMPLE_RATE });
		await audioCtx.audioWorklet.addModule(AUDIO_CONTEXT_WORKLET_PATH);

		// MediaStreamSource
		const mediaSource = audioCtx.createMediaStreamSource(stream);

		// AudioWorkletNode: Processa √°udio em pequenos buffers
		const processor = new AudioWorkletNode(audioCtx, 'deepgram-audio-worklet-processor');
		processor.port.postMessage({ type: 'setThreshold', threshold: source === INPUT ? 0.02 : 0.005 });

		let audioDataCount = 0;
		let volumeUpdateCount = 0;

		processor.port.onmessage = event => {
			try {
				if (event.data.type === 'audioData') {
					audioDataCount++;
					if (audioDataCount === 1) console.log(`üéß Recebido primeiro audioData do worklet (${source})`);
				} else if (event.data.type === 'volumeUpdate') {
					volumeUpdateCount++;
				}

				processVoskAudioMessage(source, event.data).catch(error =>
					console.error(`‚ùå Erro ao processar mensagem do worklet (${source}):`, error),
				);
			} catch (error) {
				console.error(`‚ùå Erro cr√≠tico no handler de mensagem (${source}):`, error);
			}
		};

		// Conecta: Source ‚Üí Processor ‚Üí Destination
		mediaSource.connect(processor);
		processor.connect(audioCtx.destination);

		// Atualiza estado
		vars.setStream(stream);
		vars.setAudioContext(audioCtx);
		vars.setSource(mediaSource);
		vars.setProcessor(processor);
		vars.setActive(true);
		vars.setStartAt(Date.now());
		vars.lastActive = Date.now();

		// Inicia envio cont√≠nuo de chunks
		startChunkSender(source, vars);

		console.log(cfg.startLog);
	} catch (error) {
		console.error(`‚ùå Erro ao iniciar Vosk ${source.toUpperCase()}:`, error);
		vars.setActive(false);
		stopVosk(source);
		throw error;
	}
}

/**
 * Processa mensagens de √°udio do AudioWorklet (id√™ntico ao Deepgram)
 */
async function processVoskAudioMessage(source, data) {
	const vars = voskVars[source];

	if (data.type === 'audioData') {
		// üî• CORRIGIDO: O worklet envia pcm16.buffer (ArrayBuffer), precisa converter para Int16Array
		const pcm16Array = data.pcm16 instanceof ArrayBuffer ? new Int16Array(data.pcm16) : data.pcm16;

		// Detecta fala usando VAD
		const isSpeech = detectSpeechVosk(source, vars, pcm16Array, data.percent || 0);
		updateVADStateVosk(vars, isSpeech);

		// S√≥ acumula se detectou fala
		if (isSpeech) {
			vars.chunkBuffer().push(new Int16Array(pcm16Array));
			// Debug: Log a cada N buffers
			if (vars.chunkBuffer().length % 10 === 0) {
				console.log(
					`üì• Acumulados ${vars.chunkBuffer().length} buffers VAD-speech (${vars.chunkBuffer().reduce((s, c) => s + c.byteLength, 0)} bytes)`,
				);
			}
		}
	} else if (data.type === 'volumeUpdate') {
		// Atualiza volume
		vars._lastPercent = data.percent;
		vars.lastPercent = data.percent;

		if (globalThis.RendererAPI?.emitUIChange) {
			const ev = source === INPUT ? 'onInputVolumeUpdate' : 'onOutputVolumeUpdate';
			globalThis.RendererAPI.emitUIChange(ev, { percent: data.percent });
		}

		// Detecta sil√™ncio
		handleSilenceDetectionVosk(source, data.percent, vars);
	}
}

/**
 * Para captura de √°udio
 */
async function stopVosk(source) {
	const vars = voskVars[source];

	if (!vars.isActive?.()) return;

	try {
		// Para envio de chunks
		if (vars.chunkSendTimer?.()) {
			clearInterval(vars.chunkSendTimer?.());
			vars.setChunkSendTimer(null);
		}

		// Envia chunks pendentes
		await sendPendingChunks(source, vars);

		// Desconecta processador
		const processor = vars.processor?.();
		if (processor) {
			try {
				processor.disconnect();
			} catch (e) {
				console.warn(`‚ö†Ô∏è Erro ao desconectar processor (${source}):`, e);
			}
		}

		// Desconecta source
		const source_node = vars.source?.();
		if (source_node) {
			try {
				source_node.disconnect();
			} catch (e) {
				console.warn(`‚ö†Ô∏è Erro ao desconectar source (${source}):`, e);
			}
		}

		// Fecha stream
		vars
			.stream()
			?.getTracks()
			.forEach(track => track.stop());

		vars.setActive(false);
		vars.setStream(null);
		vars.setProcessor(null);
		vars.setSource(null);
		vars.setAudioContext(null);
		vars.setStartAt(null);

		console.log(`üõë Vosk ${source.toUpperCase()} parado`);
	} catch (error) {
		console.error(`‚ùå Erro ao parar Vosk ${source.toUpperCase()}:`, error);
	}
}

/* ================================ */
//	ENVIO CONT√çNUO DE CHUNKS
/* ================================ */

/**
 * Inicia timer para enviar chunks periodicamente
 */
function startChunkSender(source, vars) {
	if (vars.chunkSendTimer?.()) {
		clearInterval(vars.chunkSendTimer?.());
	}

	const timer = setInterval(async () => {
		const chunks = vars.chunkBuffer();
		if (chunks.length > 0) {
			const totalSize = chunks.reduce((sum, c) => sum + c.byteLength, 0);
			// üî• Ignora se vazios ou muito pequenos (< ~1KB = ~64ms @ 16kHz)
			if (totalSize > 1000) {
				console.log(`üì§ Enviando ${chunks.length} chunks ao Vosk (${totalSize} bytes PCM 16-bit)...`);
				await sendChunksToVosk(source, vars, chunks);
				vars.clearChunkBuffer();
			} else {
				console.warn(`‚ö†Ô∏è PCM muito pequeno (${totalSize}b), acumulando...`);
			}
		}
	}, CHUNK_SEND_INTERVAL_MS);

	vars.setChunkSendTimer(timer);
}

/**
 * Envia chunks de PCM 16-bit acumulados ao Vosk via IPC
 * Agora usa VAD EXATAMENTE IGUAL AO DEEPGRAM
 * S√≥ envia se houver √°udio real detectado
 */
async function sendChunksToVosk(source, vars, chunks) {
	try {
		if (chunks.length === 0) return;

		// üî• Monta buffer √∫nico a partir dos chunks PCM 16-bit
		const totalSize = chunks.reduce((sum, c) => sum + c.byteLength, 0);
		const pcmBuffer = new Int16Array(totalSize / 2); // Divide por 2 porque cada sample = 2 bytes
		let offset = 0;

		for (const chunk of chunks) {
			const view = new Int16Array(chunk.buffer, chunk.byteOffset, chunk.byteLength / 2);
			pcmBuffer.set(view, offset);
			offset += view.length;
		}

		// üî• N√ÉO VALIDAR VAD NOVAMENTE: J√° foi validado ao acumular chunks
		// Se chegou aqui com chunks, √© porque passou por VAD

		console.log(`üì¶ PCM 16-bit combinado: ${pcmBuffer.byteLength} bytes (${chunks.length} chunks) - ENVIANDO`);

		// üî• VALIDA√á√ÉO: Rejeita se muito pequeno
		if (pcmBuffer.byteLength < 1000) {
			console.warn(`‚ö†Ô∏è PCM muito pequeno (${pcmBuffer.byteLength}b), ignorando...`);
			return;
		}

		// Envia para Vosk via IPC (main.js converter√° PCM ‚Üí WAV se necess√°rio)
		const buffer = Buffer.from(pcmBuffer.buffer, pcmBuffer.byteOffset, pcmBuffer.byteLength);
		const startTime = Date.now();

		// üî• NOVO: Chama handler espec√≠fico para PCM (n√£o WebM)
		const finalResult = await ipcRenderer.invoke('vosk-transcribe-pcm', buffer);

		const duration = Date.now() - startTime;
		console.log(`‚úÖ Vosk processou em ${duration}ms - Resultado:`, finalResult);

		// üî• CORRIGIDO: Usar partial se final vazio (Vosk retorna incremental)
		const transcribedText = (finalResult?.final?.trim?.() || finalResult?.partial?.trim?.() || '').trim();

		if (!transcribedText) {
			console.log(`üìù Vosk ainda processando... (partial vazio)`);
			return;
		}

		console.log(`üìù Resultado: "${transcribedText}"`);

		// Atualiza UI
		if (globalThis.RendererAPI?.emitUIChange) {
			globalThis.RendererAPI.emitUIChange('onTranscriptAdd', {
				author: vars.author,
				text: transcribedText,
				timeStr: new Date().toLocaleTimeString(),
				elementId: 'conversation',
				placeholderId: `vosk-${source}-${Date.now()}`,
			});

			globalThis.RendererAPI.emitUIChange('onPlaceholderFulfill', {
				speaker: vars.author,
				text: transcribedText,
				placeholderId: `vosk-${source}-${Date.now()}`,
			});
		}

		vars.lastTranscript = transcribedText;

		// Para OUTPUT: chama handleCurrentQuestion (Deepgram pattern)
		if (source === OUTPUT && globalThis.RendererAPI?.handleCurrentQuestion) {
			globalThis.RendererAPI.handleCurrentQuestion(vars.author, transcribedText, {
				isInterim: false,
				shouldFinalizeAskCurrent: vars.shouldFinalizeAskCurrent,
			});
		}
	} catch (error) {
		console.error(`‚ùå Erro ao enviar chunks ao Vosk:`, error);
	}
}

/**
 * Envia chunks pendentes quando MediaRecorder para
 */
async function sendPendingChunks(source, vars) {
	const chunks = vars.chunkBuffer();
	if (chunks.length > 0) {
		console.log(`üì§ Enviando ${chunks.length} chunks FINAIS ao Vosk...`);
		await sendChunksToVosk(source, vars, chunks);
		vars.clearChunkBuffer();
	}
}

/* ================================ */
//	MONITORAMENTO DE VOLUME E SIL√äNCIO
/* ================================ */

/**
 * Monitora volume e detecta sil√™ncio
 */
function monitorVolumeVosk(source, analyser, vars) {
	const dataArray = new Uint8Array(analyser.frequencyBinCount);
	let logCounter = 0;

	const updateVolume = () => {
		if (!vars.isActive?.()) return;

		analyser.getByteFrequencyData(dataArray);

		const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
		const percent = (average / 255) * 100;

		vars._lastPercent = percent;
		vars.lastPercent = percent;

		// Log periodicamente
		if (logCounter++ % 30 === 0) {
			console.log(`üìä Volume ${source}: ${percent.toFixed(2)}%`);
		}

		// Atualiza UI
		if (globalThis.RendererAPI?.emitUIChange) {
			const ev = source === INPUT ? 'onInputVolumeUpdate' : 'onOutputVolumeUpdate';
			globalThis.RendererAPI.emitUIChange(ev, { percent });
		}

		// Detecta sil√™ncio
		handleSilenceDetectionVosk(source, percent, vars);

		requestAnimationFrame(updateVolume);
	};

	updateVolume();
}

/**
 * Detecta sil√™ncio e finaliza transcri√ß√£o
 */
function handleSilenceDetectionVosk(source, percent, vars) {
	const silenceTimeout = source === INPUT ? SILENCE_TIMEOUT_INPUT : SILENCE_TIMEOUT_OUTPUT;
	const now = Date.now();
	const MIN_RECORDING_TIME = 800;
	const VOLUME_THRESHOLD = 3;

	const isSpeech = percent > VOLUME_THRESHOLD;

	if (isSpeech) {
		if (vars.inSilence) {
			if (!vars.noiseStartTime) vars.noiseStartTime = Date.now();
			const noiseDuration = vars.noiseStartTime - vars.noiseStopTime;
			console.log(`üü¢ üü¢ üü¢ ***** üîä Fala detectada ap√≥s ${noiseDuration}ms - Volume: ${percent.toFixed(2)}% *****`);
		}

		vars.inSilence = false;
		vars.shouldFinalizeAskCurrent = false;
		vars.lastActive = now;
		vars.noiseStartTime = null;

		// üî• VAD: Atualiza √∫ltimo momento de fala detectada
		vars._vadLastSpeechTime = now;
	} else {
		const elapsed = now - vars.lastActive;
		const recordingTime = now - (vars.startAt?.() || now);

		// Detectou sil√™ncio est√°vel
		if (elapsed >= silenceTimeout && !vars.inSilence && recordingTime >= MIN_RECORDING_TIME) {
			vars.inSilence = true;
			vars.shouldFinalizeAskCurrent = true;
			vars.noiseStopTime = Date.now();

			console.log(
				`üî¥ üî¥ üî¥ ***** üîá Sil√™ncio est√°vel (${elapsed}ms, recording: ${recordingTime}ms) - Volume: ${percent.toFixed(2)}% *****`,
			);

			// üî• ENVIA chunks acumulados ao detectar sil√™ncio
			const chunks = vars.chunkBuffer();
			if (chunks.length > 0) {
				console.log(`üì§ Detectado sil√™ncio - ENVIANDO ${chunks.length} chunks ao Vosk...`);
				sendChunksToVosk(source, vars, chunks).catch(error =>
					console.error(`‚ùå Erro ao enviar chunks no sil√™ncio:`, error),
				);
				vars.clearChunkBuffer();
			}

			// IMPORTANTE: N√£o para a grava√ß√£o aqui!
			// Apenas marca como "sil√™ncio" - a grava√ß√£o continua
			// Isso permite capturar √°udio novamente quando a pessoa fala de novo
		}
	}
}

/* ================================ */
//	FUN√á√ïES VAD (Voice Activity Detection)
//	COPIADAS EXATAMENTE DO DEEPGRAM
/* ================================ */

/**
 * Inicializa VAD nativo (webrtcvad) - id√™ntico ao Deepgram
 */
function initVADVosk() {
	let VAD = null;
	try {
		VAD = require('webrtcvad');
	} catch {
		try {
			VAD = require('node-webrtcvad');
		} catch {
			return null;
		}
	}

	if (!VAD) return null;

	try {
		if (typeof VAD?.default === 'function') {
			// webrtcvad (ESM default)
			const mode = 2; // Modo agressivo (id√™ntico ao Deepgram)
			return new VAD.default(AUDIO_SAMPLE_RATE, mode);
		} else if (typeof VAD === 'function') {
			// node-webrtcvad (CommonJS)
			const mode = 2;
			return new VAD(mode);
		} else if (VAD?.VAD) {
			// classe interna
			const mode = 2;
			return new VAD.VAD(mode);
		}
	} catch (e) {
		console.warn('‚ö†Ô∏è Erro ao inicializar VAD nativo:', e.message || e);
	}

	return null;
}

/**
 * Tenta chamar VAD com diferentes assinaturas - id√™ntico ao Deepgram
 */
function tryCallVADInstanceVosk(frame, sampleRate) {
	if (typeof vadInstanceVosk.process === 'function') {
		if (vadInstanceVosk.process.length === 2) {
			return processVADResultVosk(vadInstanceVosk.process(sampleRate, frame));
		} else {
			return processVADResultVosk(vadInstanceVosk.process(frame));
		}
	} else if (typeof vadInstanceVosk.isSpeech === 'function') {
		return !!vadInstanceVosk.isSpeech(frame, sampleRate);
	} else if (typeof vadInstanceVosk === 'function') {
		return !!vadInstanceVosk(frame, sampleRate);
	}
	return null;
}

/**
 * Processa resultado do VAD - id√™ntico ao Deepgram
 */
function processVADResultVosk(result) {
	if (typeof result === 'boolean') return result;
	if (typeof result === 'number') return result > 0;
	return null;
}

/**
 * Verifica se VAD nativo est√° habilitado - id√™ntico ao Deepgram
 */
function isVADEnabledVosk() {
	return useNativeVADVosk && !!vadAvailableVosk;
}

/**
 * Executa VAD nativo - id√™ntico ao Deepgram
 */
function runNativeVADVosk(frame, sampleRate = AUDIO_SAMPLE_RATE) {
	try {
		if (vadInstanceVosk !== undefined && vadInstanceVosk) {
			try {
				return tryCallVADInstanceVosk(frame, sampleRate);
			} catch (error_) {
				console.warn('runNativeVADVosk: erro ao chamar vadInstance:', error_ && (error_.message || error_));
				return null;
			}
		}
	} catch (err) {
		console.warn('runNativeVADVosk erro:', err && (err.message || err));
	}
	return null;
}

/**
 * Detecta fala usando VAD nativo ou fallback - ID√äNTICO AO DEEPGRAM
 */
function detectSpeechVosk(source, vars, pcm16Data, percent) {
	let isSpeech = null;

	// Tenta VAD nativo se dispon√≠vel
	if (isVADEnabledVosk()) {
		try {
			const sampleRate = AUDIO_SAMPLE_RATE;
			const pcm = new Int16Array(pcm16Data);
			const frameSize = Math.floor(sampleRate * VAD_FRAME_DURATION_MS); // ~480 samples

			for (let i = 0; i + frameSize <= pcm.length; i += frameSize) {
				const frame = pcm.subarray(i, i + frameSize);
				const vadDecision = runNativeVADVosk(frame, sampleRate);
				if (vadDecision === true) {
					isSpeech = true;
					break;
				}
				if (vadDecision === null) {
					break;
				}
			}
		} catch (e) {
			console.warn('‚ö†Ô∏è Erro ao executar VAD nativo:', e.message || e);
			isSpeech = null;
		}
	}

	// Fallback: VAD por energia com janela deslizante (id√™ntico ao Deepgram)
	return isSpeech === null ? fallbackIsSpeechVosk(vars, percent) : isSpeech;
}

/**
 * Fallback de VAD baseado em energia com suaviza√ß√£o (multi-frame) - ID√äNTICO AO DEEPGRAM
 */
function fallbackIsSpeechVosk(vars, percent) {
	if (!vars.vadWindow) vars.vadWindow = [];
	const window = vars.vadWindow;
	window.push(percent);
	if (window.length > VAD_WINDOW_SIZE) window.shift(); // √∫ltimos ~6 frames
	const avg = window.reduce((a, b) => a + b, 0) / window.length;

	// heur√≠stica ajustada: use 20% (id√™ntico ao Deepgram)
	const result = avg > FALLBACK_VOLUME_THRESHOLD;

	if (result !== vars._lastIsSpeech) {
		const status = result ? 'üîä FALA' : 'üîá SIL√äNCIO';
		console.log(`   VAD Fallback: ${status} (avg: ${avg.toFixed(1)}%, threshold: ${FALLBACK_VOLUME_THRESHOLD}%)`);
	}

	return result;
}

/**
 * Atualiza estado VAD - id√™ntico ao Deepgram
 */
function updateVADStateVosk(vars, isSpeech) {
	vars._lastIsSpeech = !!isSpeech;
	vars._lastVADTimestamp = Date.now();
	if (isSpeech) vars.lastActive = Date.now();
}

/* ================================ */
//	INTERFACE P√öBLICA
/* ================================ */

/**
 * Inicia Vosk para INPUT + OUTPUT
 */
async function startAudioVoskLocal(UIElements) {
	try {
		// Inicia INPUT (voc√™) + OUTPUT (outros)
		if (UIElements.inputSelect?.value) await startVosk(INPUT, UIElements);
		if (UIElements.outputSelect?.value) await startVosk(OUTPUT, UIElements);
	} catch (error) {
		console.error('‚ùå Erro ao iniciar Vosk:', error);
		throw error;
	}
}

/**
 * Para Vosk para INPUT + OUTPUT
 */
function stopAudioVoskLocal() {
	stopVosk(INPUT);
	stopVosk(OUTPUT);
}

/**
 * Muda dispositivo para um source
 */
function switchDeviceVoskLocal(source, newDeviceId) {
	const vars = voskVars[source];
	if (vars.isActive?.()) {
		stopVosk(source);
		// TODO: Implementar rein√≠cio com novo dispositivo ap√≥s mudan√ßa
		vars.currentDeviceId = newDeviceId;
	}
}

/* ================================ */
//	EXPORTS
/* ================================ */

module.exports = {
	startAudioVoskLocal,
	stopAudioVoskLocal,
	switchDeviceVoskLocal,
};
