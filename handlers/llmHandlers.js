/**
 * llmHandlers - Handlers separados para LLM (gen√©rico)
 *
 * Quebra a fun√ß√£o gigante askGpt() em:
 * 1. validateLLMRequest() - valida√ß√£o (antigo validateAskGptRequest)
 * 2. handleLLMStream() - modo entrevista (antigo handleAskGptStream)
 * 3. handleLLMBatch() - modo normal (antigo handleAskGptBatch)
 */

const { ipcRenderer } = require('electron');
const Logger = require('../utils/Logger.js');

/**
 * Valida requisi√ß√£o de LLM
 * @param {AppState} appState - Estado da app
 * @param {string} questionId - ID da pergunta selecionada
 * @param {function} getSelectedQuestionText - Getter do texto
 * @throws {Error} Se valida√ß√£o falhar
 * @returns {Object} {questionId, text, isCurrent}
 */
function validateLLMRequest(appState, questionId, getSelectedQuestionText) {
	const CURRENT_QUESTION_ID = 'CURRENT';
	const text = getSelectedQuestionText();
	const isCurrent = questionId === CURRENT_QUESTION_ID;

	// Valida√ß√£o 1: Text n√£o vazio
	if (!text?.trim()) {
		throw new Error('Pergunta vazia - nada a enviar para LLM');
	}

	// Valida√ß√£o 2: Dedupe para CURRENT
	if (isCurrent) {
		const normalizedText = text
			.toLowerCase()
			.replaceAll('?', '')
			.replaceAll('!', '')
			.replaceAll('.', '')
			.replaceAll('\n', '')
			.trim();
		if (normalizedText === appState.interview.lastAskedQuestionNormalized) {
			throw new Error('Pergunta j√° enviada');
		}
	}

	// Valida√ß√£o 3: Modo entrevista bloqueia duplica√ß√£o no hist√≥rico
	if (!isCurrent && appState.interview.answeredQuestions.has(questionId)) {
		throw new Error('Essa pergunta j√° foi respondida');
	}

	return { questionId, text, isCurrent };
}

/**
 * Manipula resposta em modo streaming (entrevista)
 */
async function handleLLMStream(appState, questionId, text, SYSTEM_PROMPT, eventBus, llmManager, turnId = null) {
	Logger.info('Iniciando stream LLM', { questionId, textLength: text.length, turnId });

	let streamedText = '';
	appState.metrics.gptStartTime = Date.now();
	appState.interview.gptRequestedTurnId = appState.interview.interviewTurnId;
	appState.interview.gptRequestedQuestionId = questionId;

	// Obter handler LLM e invocar stream
	const currentLLM = appState.selectedProvider || globalThis.configManager?.config?.api?.activeProvider || 'openai';
	const handler = llmManager.getHandler(currentLLM);

	try {
		// Chamar stream do handler
		const streamGenerator = await handler.stream([
			{ role: 'system', content: SYSTEM_PROMPT },
			{ role: 'user', content: text },
		]);

		// Iterar tokens
		for await (const token of streamGenerator) {
			streamedText += token;
			appState.metrics.gptFirstTokenTime = appState.metrics.gptFirstTokenTime || Date.now();

			eventBus.emit('answerStreamChunk', {
				questionId,
				turnId, // üî• Incluir turnId para UI
				token,
				accum: streamedText,
			});
		}

		appState.metrics.gptEndTime = Date.now();
		appState.interview.gptAnsweredTurnId = appState.interview.interviewTurnId;

		Logger.info('Stream LLM finalizado', {
			duration: appState.metrics.gptEndTime - appState.metrics.gptStartTime,
		});

		eventBus.emit('llmStreamEnd', {
			questionId,
			streamedText,
		});
	} catch (error) {
		Logger.error('Erro em handleLLMStream', { error: error.message });
		eventBus.emit('error', error.message);
		throw error;
	}
}

/**
 * Manipula resposta em modo batch (normal)
 */
async function handleLLMBatch(appState, questionId, text, SYSTEM_PROMPT, eventBus, llmManager) {
	Logger.info('Iniciando batch LLM', { questionId, textLength: text.length });

	appState.metrics.gptStartTime = Date.now();

	// Obter handler LLM e invocar complete
	const currentLLM = appState.selectedProvider || globalThis.configManager?.config?.api?.activeProvider || 'openai';
	const handler = llmManager.getHandler(currentLLM);

	try {
		const response = await handler.complete([
			{ role: 'system', content: SYSTEM_PROMPT },
			{ role: 'user', content: text },
		]);

		appState.metrics.gptEndTime = Date.now();

		Logger.info('Batch LLM finalizado', {
			duration: appState.metrics.gptEndTime - appState.metrics.gptStartTime,
		});

		eventBus.emit('llmBatchEnd', {
			questionId,
			response,
		});
	} catch (error) {
		Logger.error('Erro em handleLLMBatch', { error: error.message });
		eventBus.emit('error', error.message);
		throw error;
	}
}

module.exports = {
	validateLLMRequest,
	handleLLMStream,
	handleLLMBatch,
};
