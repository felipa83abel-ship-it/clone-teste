/**
 * üî• VOSK TRANSCRIBE - M√ìDULO INDEPENDENTE
 *
 * Implementa√ß√£o isolada de transcri√ß√£o com Vosk (local).
 * - Captura √°udio via MediaRecorder (WebM) + IPC para servidor Vosk Python
 * - Emite eventos 'transcription' para desacoplamento (mesmo padr√£o do Deepgram)
 * - Segue padr√£o: vosk-transcribe.js emite eventos, renderer.js processa
 */

/* ================================ */
//	IMPORTS
/* ================================ */
const { ipcRenderer } = require('electron');

/* ================================ */
//	CONSTANTES
/* ================================ */

// Configura√ß√£o Geral
const INPUT = 'input';
const OUTPUT = 'output';

// Timeouts de Sil√™ncio (id√™ntico ao Deepgram)
const SILENCE_TIMEOUT_INPUT = 500; // ms para entrada (microfone)
const SILENCE_TIMEOUT_OUTPUT = 700; // ms para sa√≠da (sistema)

/* ================================ */
//	ESTADO DO VOSK
/* ================================ */

// Refer√™ncia aos UIElements (armazenada ao iniciar)
let uiElementsRef = null;

// voskVars mant√©m seu pr√≥prio estado interno (isolado por source: input/output)
const voskVars = {
	input: {
		_isActive: false,
		_stream: null,
		_audioContext: null,
		_recorder: null,
		_analyser: null,
		_startAt: null,
		_silenceTimer: null,
		_audioChunks: [],

		isActive() {
			return this._isActive;
		},
		setActive(val) {
			this._isActive = val;
		},
		stream() {
			return this._stream;
		},
		setStream(val) {
			this._stream = val;
		},
		recorder() {
			return this._recorder;
		},
		setRecorder(val) {
			this._recorder = val;
		},
		startAt() {
			return this._startAt;
		},
		setStartAt(val) {
			this._startAt = val;
		},
		audioChunks() {
			return this._audioChunks;
		},
		clearAudioChunks() {
			this._audioChunks = [];
		},

		author: 'Voc√™',
		lastActive: null, // Resetado quando inicia grava√ß√£o
		lastTranscript: '',
		inSilence: false,
		lastPercent: 0,
	},
	output: {
		_isActive: false,
		_stream: null,
		_audioContext: null,
		_recorder: null,
		_analyser: null,
		_startAt: null,
		_silenceTimer: null,
		_audioChunks: [],

		isActive() {
			return this._isActive;
		},
		setActive(val) {
			this._isActive = val;
		},
		stream() {
			return this._stream;
		},
		setStream(val) {
			this._stream = val;
		},
		recorder() {
			return this._recorder;
		},
		setRecorder(val) {
			this._recorder = val;
		},
		startAt() {
			return this._startAt;
		},
		setStartAt(val) {
			this._startAt = val;
		},
		audioChunks() {
			return this._audioChunks;
		},
		clearAudioChunks() {
			this._audioChunks = [];
		},

		author: 'Outros',
		lastActive: null, // Resetado quando inicia grava√ß√£o
		lastTranscript: '',
		inSilence: false,
		lastPercent: 0,
	},
};

/* ================================ */
//	INICIALIZA√á√ÉO (PRIVADA)
/* ================================ */

/**
 * Inicia captura de √°udio para um source (INPUT ou OUTPUT)
 * Segue mesmo padr√£o do Deepgram: getUserMedia -> MediaRecorder -> an√°lise de volume
 */
async function startVosk(source, UIElements) {
	const config = {
		input: {
			deviceKey: 'inputSelect',
			accessMessage: 'üé§ Solicitando acesso √† entrada de √°udio (Microfone)...',
			startLog: '‚ñ∂Ô∏è Captura Vosk INPUT iniciada',
		},
		output: {
			deviceKey: 'outputSelect',
			accessMessage: 'üîä Solicitando acesso √† sa√≠da de √°udio (VoiceMeter/Stereo Mix)...',
			startLog: '‚ñ∂Ô∏è Captura Vosk OUTPUT iniciada',
		},
	};

	const cfg = config[source];
	if (!cfg) {
		throw new Error(`‚ùå Source inv√°lido: ${source}. Use ${INPUT} ou ${OUTPUT}`);
	}

	const vars = voskVars[source];

	if (vars.isActive?.()) {
		console.warn(`‚ö†Ô∏è Vosk ${source.toUpperCase()} j√° ativo`);
		return;
	}

	try {
		console.log(cfg.accessMessage);

		// Obt√©m dispositivo selecionado
		const deviceId = UIElements[cfg.deviceKey]?.value;
		if (!deviceId) {
			console.warn(`‚ö†Ô∏è Nenhum dispositivo ${source} selecionado`);
			return;
		}

		// Inicia stream de √°udio
		const stream = await navigator.mediaDevices.getUserMedia({
			audio: {
				deviceId: { exact: deviceId },
				echoCancellation: true,
				noiseSuppression: true,
				autoGainControl: false,
			},
		});

		console.log(`‚úÖ Acesso ao √°udio ${source.toUpperCase()} autorizado`);

		// AudioContext para an√°lise de volume
		const audioCtx = new (globalThis.AudioContext || globalThis.webkitAudioContext)();
		const mediaSource = audioCtx.createMediaStreamSource(stream);
		const analyser = audioCtx.createAnalyser();
		analyser.fftSize = 256;
		mediaSource.connect(analyser);

		// MediaRecorder para captura de √°udio em WebM
		const recorder = new MediaRecorder(stream);

		recorder.addEventListener('dataavailable', event => {
			vars.audioChunks().push(event.data);
		});

		recorder.addEventListener('stop', async () => {
			console.log(`üõë Evento 'stop' do MediaRecorder disparado para ${source}`);
			const chunks = vars.audioChunks();
			if (chunks.length === 0) {
				console.warn(`‚ö†Ô∏è Nenhum √°udio capturado para ${source}`);
				return;
			}

			console.log(`üì¶ Chunks capturados: ${chunks.length}, Volume final: ${vars.lastPercent.toFixed(2)}%`);

			// Monta blob e envia para Vosk via IPC
			const audioBlob = new Blob(chunks, { type: 'audio/webm' });
			console.log(`üìù Blob size: ${audioBlob.size} bytes`);
			await transcribeVoskWithBlob(audioBlob, source, vars);

			// Limpa chunks para pr√≥xima grava√ß√£o
			vars.clearAudioChunks();
		});

		// Inicia grava√ß√£o
		recorder.start();

		// Atualiza estado
		vars.setStream(stream);
		vars.setRecorder(recorder);
		vars.setActive(true);
		vars.setStartAt(Date.now());
		vars.lastActive = Date.now(); // Reset lastActive ao iniciar grava√ß√£o

		// Listener para stream inativo (stream pode ficar inativo mesmo se recorder continua)
		stream.addEventListener('inactive', () => {
			console.warn(`‚ö†Ô∏è Stream ${source} ficou INATIVO! Finalizando grava√ß√£o...`);
			const rec = vars.recorder();
			if (rec && rec.state === 'recording') {
				console.log(`üî¥ For√ßando stop() do recorder (stream inativo)`);
				rec.stop();
			}
		});

		// Inicia monitoramento de volume
		monitorVolumeVosk(source, analyser, vars);

		console.log(cfg.startLog);
	} catch (error) {
		console.error(`‚ùå Erro ao iniciar Vosk ${source.toUpperCase()}:`, error);
		vars.setActive(false);
		stopVosk(source);
		throw error;
	}
}

/**
 * Para captura de √°udio para um source (INPUT ou OUTPUT)
 */
function stopVosk(source) {
	const vars = voskVars[source];

	if (!vars.isActive?.()) return;

	try {
		// Para MediaRecorder (dispara evento 'stop' que faz transcri√ß√£o)
		if (vars.recorder()?.state === 'recording') {
			vars.recorder().stop();
		}

		// Fecha stream
		vars
			.stream()
			?.getTracks()
			.forEach(track => track.stop());

		// Limpa timers
		if (vars._silenceTimer) {
			clearTimeout(vars._silenceTimer);
			vars._silenceTimer = null;
		}

		vars.setActive(false);
		vars.setStream(null);
		vars.setRecorder(null);
		vars.setStartAt(null);

		console.log(`üõë Vosk ${source.toUpperCase()} parado`);
	} catch (error) {
		console.error(`‚ùå Erro ao parar Vosk ${source.toUpperCase()}:`, error);
	}
}

/* ================================ */
//	TRANSCRI√á√ÉO
/* ================================ */

/**
 * Transcreve blob de √°udio enviando para servidor Vosk via IPC
 * Segue mesmo padr√£o do Deepgram: chama RendererAPI.emitUIChange() para atualizar UI
 */
async function transcribeVoskWithBlob(blob, source, vars) {
	try {
		console.log(`üöÄ Enviando para Vosk (${source})...`);

		const buffer = Buffer.from(await blob.arrayBuffer());
		const startTime = Date.now();

		// Envia para servidor Vosk via IPC (main.js)
		const finalResult = await ipcRenderer.invoke('vosk-transcribe', buffer);

		const endTime = Date.now();
		const duration = endTime - startTime;
		console.log(`‚úÖ Vosk conclu√≠do em ${duration}ms`);

		// Extrai texto final
		let transcribedText = '';
		if (typeof finalResult === 'string') {
			transcribedText = finalResult;
		} else if (typeof finalResult === 'object' && finalResult !== null) {
			transcribedText = finalResult.final || finalResult.result?.[0] || '';
		}

		console.log(`üìù Resultado (${transcribedText.length} chars): "${transcribedText.substring(0, 80)}..."`);

		// Calcula m√©tricas de timing (id√™ntico ao Deepgram)
		const startTime_ts = vars.startAt?.();
		const now = Date.now();
		const elapsedMs = startTime_ts ? now - startTime_ts : 0;
		const metrics = {
			startStr: startTime_ts ? new Date(startTime_ts).toLocaleTimeString() : new Date(now).toLocaleTimeString(),
			stopStr: new Date(now).toLocaleTimeString(),
			recordingDuration: (elapsedMs / 1000).toFixed(2),
			latency: (duration / 1000).toFixed(2),
			total: (elapsedMs / 1000).toFixed(2),
		};

		// Atualiza estado
		vars.lastTranscript = transcribedText;

		// Processa resultado FINAL exatamente como Deepgram faz
		if (transcribedText.trim()) {
			const placeholderId = `vosk-${source}-${Date.now()}-${Math.floor(Math.random() * 1000)}`;

			// 1Ô∏è‚É£ Adiciona placeholder com "..."
			if (globalThis.RendererAPI?.emitUIChange) {
				globalThis.RendererAPI.emitUIChange('onTranscriptAdd', {
					author: vars.author,
					text: '...',
					timeStr: metrics.startStr,
					elementId: 'conversation',
					placeholderId,
				});
			}

			// 2Ô∏è‚É£ Preenche placeholder com transcri√ß√£o final
			if (globalThis.RendererAPI?.emitUIChange) {
				globalThis.RendererAPI.emitUIChange('onPlaceholderFulfill', {
					speaker: vars.author,
					text: transcribedText,
					placeholderId,
					...metrics,
					showMeta: false,
				});
			}

			// 3Ô∏è‚É£ Atualiza CURRENT question (apenas para output)
			if (source === OUTPUT && globalThis.RendererAPI?.handleCurrentQuestion) {
				globalThis.RendererAPI.handleCurrentQuestion(vars.author, transcribedText, {
					isInterim: false,
					shouldFinalizeAskCurrent: false,
				});
			}
		}
	} catch (error) {
		console.error(`‚ùå Vosk ${source} falhou:`, error.message);
		throw new Error(`Vosk local falhou: ${error.message}. Altere o modelo em "Configura√ß√µes ‚Üí API e Modelos"`);
	}
}

/* ================================ */
//	MONITORAMENTO DE VOLUME
/* ================================ */

/**
 * Monitora volume de √°udio em tempo real
 * Semelhante ao Deepgram: analisa frequ√™ncia e emite updates de volume
 */
function monitorVolumeVosk(source, analyser, vars) {
	const dataArray = new Uint8Array(analyser.frequencyBinCount);
	let logCounter = 0; // Log a cada 30 frames (~500ms)

	function updateVolume() {
		if (!vars.isActive?.()) return;

		analyser.getByteFrequencyData(dataArray);

		// Calcula volume como percentual
		const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
		const percent = (average / 255) * 100;

		vars.lastPercent = percent;

		// Log debug periodicamente
		if (logCounter++ % 30 === 0) {
			console.log(`üìä Volume ${source}: ${percent.toFixed(2)}% (average: ${average.toFixed(2)}/255)`);
		}

		// Atualiza UI de volume
		if (globalThis.RendererAPI?.emitUIChange) {
			const ev = source === INPUT ? 'onInputVolumeUpdate' : 'onOutputVolumeUpdate';
			globalThis.RendererAPI.emitUIChange(ev, { percent });
		}

		// Detecta sil√™ncio (id√™ntico ao Deepgram)
		handleSilenceDetectionVosk(source, percent);

		requestAnimationFrame(updateVolume);
	}

	updateVolume();
}

/**
 * Detecta sil√™ncio baseado em volume
 * Id√™ntico ao Deepgram: usa threshold de volume + timeout
 */
function handleSilenceDetectionVosk(source, percent) {
	const vars = voskVars[source];
	const silenceTimeout = source === INPUT ? SILENCE_TIMEOUT_INPUT : SILENCE_TIMEOUT_OUTPUT;
	const now = Date.now();
	const MIN_RECORDING_TIME = 800; // Aumentado para 800ms
	const VOLUME_THRESHOLD = 3; // Threshold aumentado: 3% de volume

	const isSpeech = percent > VOLUME_THRESHOLD;

	if (isSpeech) {
		// Se detectou fala, resetamos estado de sil√™ncio
		if (vars.inSilence) {
			if (!vars.noiseStartTime) vars.noiseStartTime = Date.now();

			const noiseDuration = vars.noiseStartTime - vars.noiseStopTime;
			vars.noiseStopTime = null;

			console.log(
				`üü¢ üü¢ üü¢ ***** üîä Fala real detectada ap√≥s (${noiseDuration}ms) - Volume: ${percent.toFixed(2)}% *****`,
			);
		}

		vars.inSilence = false;
		vars.shouldFinalizeAskCurrent = false;
		vars.lastActive = now;
		vars.noiseStartTime = null;
	} else {
		// Sil√™ncio detectado ‚Üí verifica se j√° passou o timeout E tempo m√≠nimo de grava√ß√£o
		const elapsed = now - vars.lastActive;
		const recordingTime = now - (vars.startAt?.() || now);

		// Entrando em sil√™ncio est√°vel (apenas se teve tempo m√≠nimo de grava√ß√£o)
		if (elapsed >= silenceTimeout && !vars.inSilence && recordingTime >= MIN_RECORDING_TIME) {
			vars.inSilence = true;
			vars.shouldFinalizeAskCurrent = true;
			vars.noiseStopTime = Date.now();

			console.log(
				`üî¥ üî¥ üî¥ ***** üîá Sil√™ncio est√°vel detectado (${elapsed}ms, recording: ${recordingTime}ms) - Volume: ${percent.toFixed(2)}% *****`,
			);

			// Para a grava√ß√£o para enviar para transcri√ß√£o
			const recorder = vars.recorder();
			if (!recorder) {
				console.warn(`‚ö†Ô∏è Recorder √© null/undefined!`);
				return;
			}

			console.log(`üìã Estado do recorder: ${recorder.state} (existe: ${!!recorder})`);

			// Tenta parar independente do estado (alguns navegadores permitem stop() mesmo em inactive)
			try {
				if (recorder.state === 'recording') {
					console.log(`‚èπÔ∏è Parando grava√ß√£o de ${source} (sil√™ncio detectado) - estado: recording`);
					recorder.stop();
				} else if (recorder.state === 'paused') {
					console.log(`‚èπÔ∏è Resumindo e parando grava√ß√£o de ${source} - estado era: paused`);
					recorder.resume();
					recorder.stop();
				} else if (recorder.state === 'inactive') {
					// Stream pode ter ficado inativo, mas tentamos disparar stop mesmo assim
					// O evento 'stop' pode n√£o disparar, ent√£o simulamos manualmente
					console.warn(`‚ö†Ô∏è Recorder j√° est√° INACTIVE! Finalizando manualmente...`);
					const chunks = vars.audioChunks();
					if (chunks.length > 0) {
						// Simula o evento 'stop' manualmente
						setTimeout(async () => {
							console.log(`üì¶ Simulando evento 'stop' (chunks: ${chunks.length})`);
							const audioBlob = new Blob(chunks, { type: 'audio/webm' });
							console.log(`üìù Blob size: ${audioBlob.size} bytes`);
							await transcribeVoskWithBlob(audioBlob, source, vars);
							vars.clearAudioChunks();
						}, 0);
					}
				}
			} catch (e) {
				console.error(`‚ùå Erro ao parar recorder: ${e.message}`);
			}
		}
	}
}

/* ================================ */
//	INTERFACE P√öBLICA (exportada)
/* ================================ */

/**
 * Inicia captura de √°udio INPUT + OUTPUT (Vosk local)
 * Interface id√™ntica ao Deepgram: startAudioDeepgram(UIElements)
 */
async function startAudioVoskLocal(UIElements) {
	console.log('üî• Iniciando Vosk Local...');

	try {
		// Armazena refer√™ncia para uso posterior (device switching)
		uiElementsRef = UIElements;

		// Inicia INPUT (voc√™) + OUTPUT (outros)
		if (UIElements.inputSelect?.value) await startVosk(INPUT, UIElements);
		if (UIElements.outputSelect?.value) await startVosk(OUTPUT, UIElements);

		console.log('‚úÖ Vosk Local iniciado');
	} catch (error) {
		console.error('‚ùå Erro ao iniciar Vosk:', error);
		throw error;
	}
}

/**
 * Para captura de √°udio INPUT + OUTPUT (Vosk local)
 * Interface id√™ntica ao Deepgram: stopAudioDeepgram()
 */
function stopAudioVoskLocal() {
	console.log('üõë Parando Vosk Local...');

	try {
		stopVosk(INPUT);
		stopVosk(OUTPUT);
		console.log('‚úÖ Vosk Local parado completamente');
	} catch (error) {
		console.error('‚ùå Erro ao parar Vosk:', error);
	}
}

/**
 * Troca dispositivo de √°udio dinamicamente (input/output)
 * Interface id√™ntica ao Deepgram: switchDeviceDeepgram(source, newDeviceId)
 */
async function switchDeviceVoskLocal(source, newDeviceId) {
	console.log(`üîÑ Trocando dispositivo ${source} para ${newDeviceId}...`);

	try {
		const vars = voskVars[source];

		if (!vars.isActive?.()) {
			console.warn(`‚ö†Ô∏è Vosk ${source.toUpperCase()} n√£o est√° ativo`);
			return;
		}

		// Para captura atual
		stopVosk(source);

		// Atualiza UIElements com novo dispositivo (ou espera pelo pr√≥ximo startVosk)
		if (uiElementsRef) {
			const deviceKey = source === INPUT ? 'inputSelect' : 'outputSelect';
			if (uiElementsRef[deviceKey]) {
				uiElementsRef[deviceKey].value = newDeviceId;
			}
		}

		// Reinicia com novo dispositivo
		if (uiElementsRef) {
			await startVosk(source, uiElementsRef);
		}

		console.log(`‚úÖ Dispositivo ${source} trocado para ${newDeviceId}`);
	} catch (error) {
		console.error(`‚ùå Erro ao trocar dispositivo:`, error);
	}
}

/* ================================ */
//	EXPORTS (CommonJS)
/* ================================ */

module.exports = {
	startAudioVoskLocal,
	stopAudioVoskLocal,
	switchDeviceVoskLocal,
};
