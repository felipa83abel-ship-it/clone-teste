/* ===============================
   IMPORTS
=============================== */
const { ipcRenderer } = require('electron');
const { marked } = require('marked');
const hljs = require('highlight.js');

// üîí DESABILITADO TEMPORARIAMENTE
const DESABILITADO_TEMPORARIAMENTE = false;
const ASK_GPT_DESABILITADO_TEMPORARIAMENTE = true;

/* ===============================
   CONSTANTES
=============================== */

const YOU = 'Voc√™';
const OTHER = 'Outros';

const ENABLE_INTERVIEW_TIMING_DEBUG = true; // ‚Üê desligar depois = false
const QUESTION_IDLE_TIMEOUT = 300; // Tempo de espera para a pergunta ser considerada inativa = 300
const CURRENT_QUESTION_ID = 'CURRENT'; // ID da pergunta atual

const INPUT_SPEECH_THRESHOLD = 20; // Valor limite (threshold) para detectar fala mais cedo = 20
const INPUT_SILENCE_TIMEOUT = 100; // Tempo de espera para sil√™ncio = 100
const MIN_INPUT_AUDIO_SIZE = 1000; // Valor m√≠nimo de tamanho de √°udio para a normal = 1000
const MIN_INPUT_AUDIO_SIZE_INTERVIEW = 350; // Valor m√≠nimo de tamanho de √°udio para a entrevista = 350

const OUTPUT_SPEECH_THRESHOLD = 20; // Valor limite (threshold) para detectar fala mais cedo = 8
const OUTPUT_SILENCE_TIMEOUT = 100; // Tempo de espera para sil√™ncio = 250
const MIN_OUTPUT_AUDIO_SIZE = 1000; // Valor m√≠nimo de tamanho de √°udio para a normal = 2500
const MIN_OUTPUT_AUDIO_SIZE_INTERVIEW = 350; // Valor m√≠nimo para enviar parcial (~3-4 chunks, ~3KB)
// controla intervalo m√≠nimo entre requisi√ß√µes STT parciais (ms) - mant√©m rate-limit para n√£o sobrecarregar API
const PARTIAL_MIN_INTERVAL_MS = 3000;

const OUTPUT_ENDING_PHRASES = ['tchau', 'tchau tchau', 'obrigado', 'valeu', 'falou', 'beleza', 'ok']; // Palavras finais para detectar o fim da fala

const SYSTEM_PROMPT = `
Voc√™ √© um assistente para entrevistas t√©cnicas de Java. Responda como candidato.
Regras de resposta (priorize sempre estas):
- Seja natural e conciso: responda em no m√°ximo 1‚Äì2 frases curtas.
- Use linguagem coloquial e direta, como algu√©m explicando rapidamente verbalmente.
- Evite listas longas, exemplos extensos ou par√°grafos detalhados.
- N√£o comece com cumprimentos ou palavras de preenchimento (ex.: "Claro", "Ok").
- Quando necess√°rio, entregue um exemplo m√≠nimo de 1 linha apenas.
`;

/* ===============================
   ESTADO GLOBAL
=============================== */

let APP_CONFIG = {
	MODE_DEBUG: false,
};

// ü™ü Estado do Drag and Drop da janela
let isDraggingWindow = false;

let isRunning = false;
let audioContext;
let mockInterviewRunning = false;

let USE_LOCAL_WHISPER = false; // false = OpenAI, true = Whisper local
let transcriptionMetrics = {
	audioStartTime: null,
	whisperStartTime: null,
	whisperEndTime: null,
	gptStartTime: null,
	gptEndTime: null,
	totalTime: null,
	audioSize: 0,
};

/* üé§ INPUT (VOC√ä) */
let inputStream;
let inputAnalyser;
let inputData;
let inputRecorder;
let inputChunks = [];
let inputSpeaking = false;
let inputSilenceTimer = null;
let inputPartialChunks = [];
let inputPartialTimer = null;

/* üîä OUTPUT (OUTROS) */
let outputStream;
let outputAnalyser;
let outputData;
let outputRecorder;
let outputChunks = [];
let outputSpeaking = false;
let outputSilenceTimer = null;
let outputPartialChunks = [];
let outputPartialTimer = null;
let outputPartialText = '';

// üî• NOVO: IDs para rastrear e parar os loops de animation
let inputVolumeAnimationId = null;
let outputVolumeAnimationId = null;

/* üß† PERGUNTAS */
let currentQuestion = { text: '', lastUpdate: 0, finalized: false, lastUpdateTime: null, createdAt: null };
let questionsHistory = [];
const answeredQuestions = new Set(); // üîí Armazena respostas j√° geradas (questionId -> true)
let selectedQuestionId = null;
let interviewTurnId = 0;
let gptAnsweredTurnId = null;
let gptRequestedTurnId = null;
let lastSentQuestionText = '';
let autoCloseQuestionTimer = null;
let lastInputStartAt = null;
let lastInputStopAt = null;
let lastOutputStartAt = null;
let lastOutputStopAt = null;
let lastInputPlaceholderEl = null;
let lastOutputPlaceholderEl = null;
let lastAskedQuestionNormalized = null;
let lastPartialSttAt = null;

/* ===============================
   CALLBACKS / OBSERVERS SYSTEM
   renderer.js √© "cego" para DOM
   config-manager.js se inscreve em mudan√ßas
=============================== */

const UICallbacks = {
	onError: null, // üî• NOVO: Para mostrar erros de valida√ß√£o
	onTranscriptAdd: null,
	onCurrentQuestionUpdate: null,
	onQuestionsHistoryUpdate: null,
	onAnswerAdd: null,
	onStatusUpdate: null, // ‚Üê Adicionado: Para atualizar status na UI
	onInputVolumeUpdate: null,
	onOutputVolumeUpdate: null,
	onMockBadgeUpdate: null,
	onDOMElementsReady: null, // callback para pedir elementos ao config-manager
	onListenButtonToggle: null,
	onAnswerSelected: null,
	onClearAllSelections: null,
	onScrollToQuestion: null,
	onTranscriptionCleared: null,
	onAnswersCleared: null,
	onAnswerStreamChunk: null,
	onModeSelectUpdate: null,
	onPlaceholderFulfill: null,
	onPlaceholderUpdate: null,
};

// Fun√ß√£o para config-manager se inscrever em eventos
function onUIChange(eventName, callback) {
	if (UICallbacks.hasOwnProperty(eventName)) {
		UICallbacks[eventName] = callback;
		console.log(`üì° UI callback registrado em renderer.js: ${eventName}`);
	}
}

// Fun√ß√£o para emitir/enviar eventos para config-manager
function emitUIChange(eventName, data) {
	if (UICallbacks[eventName] && typeof UICallbacks[eventName] === 'function') {
		UICallbacks[eventName](data);
	} else {
		console.warn(`‚ö†Ô∏è DEBUG: Nenhum callback registrado para '${eventName}'`);
	}
}

/* ===============================
   ELEMENTOS UI - Solicitado por callback
   (config-manager.js fornece esses elementos)
=============================== */

let UIElements = {
	inputSelect: null,
	outputSelect: null,
	listenBtn: null,
	statusText: null,
	transcriptionBox: null, // Mantido para compatibilidade, mas pode receber 'conversation'
	currentQuestionBox: null,
	currentQuestionTextBox: null,
	questionsHistoryBox: null,
	answersHistoryBox: null,
	askBtn: null,
	inputVu: null,
	outputVu: null,
	inputVuHome: null,
	outputVuHome: null,
	mockToggle: null,
	mockBadge: null,
	interviewModeSelect: null,
	btnClose: null,
	btnToggleClick: null,
	dragHandle: null,
	darkToggle: null,
	opacitySlider: null,
};

// config-manager.js chama isso para registrar elementos
function registerUIElements(elements) {
	UIElements = { ...UIElements, ...elements };
	console.log('‚úÖ UI Elements registrados no renderer.js');
}

/* ===============================
   MODO / ORQUESTRADOR
=============================== */

const MODES = {
	NORMAL: 'NORMAL',
	INTERVIEW: 'INTERVIEW',
};

// üîÑ modo atual (default = comportamento atual)
let CURRENT_MODE = MODES.NORMAL;

// üéº controlador central de estrat√©gia
const ModeController = {
	isInterviewMode() {
		return CURRENT_MODE === MODES.INTERVIEW;
	},

	// ‚è±Ô∏è MediaRecorder.start(timeslice)
	mediaRecorderTimeslice() {
		if (!this.isInterviewMode()) return null;

		// OUTPUT pode ser mais agressivo que INPUT
		return 60; // reduzido para janelas parciais mais responsivas
	},

	// ü§ñ GPT streaming
	allowGptStreaming() {
		return this.isInterviewMode();
	},

	// üì¶ tamanho m√≠nimo de √°udio aceito
	minInputAudioSize(defaultSize) {
		return this.isInterviewMode() ? Math.min(400, defaultSize) : defaultSize;
	},
};

/* ===============================
   HELPERS PUROS
=============================== */

function finalizeQuestion(t) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "finalizeQuestion"');
	debugLogRenderer('Fim da fun√ß√£o: "finalizeQuestion"');
	return t.trim().endsWith('?') ? t.trim() : t.trim() + '?';
}

function normalizeForCompare(t) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "normalizeForCompare"');
	debugLogRenderer('Fim da fun√ß√£o: "normalizeForCompare"');
	return (t || '')
		.toLowerCase()
		.replace(/[?!.\n\r]/g, '')
		.replace(/\s+/g, ' ')
		.trim();
}

function looksLikeQuestion(t) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "looksLikeQuestion"');
	const s = t.toLowerCase().trim();

	// precisa ter ? OU come√ßar com palavra t√≠pica de pergunta
	const questionStarters = [
		'o que',
		'por que',
		'porque',
		'como',
		'qual',
		'quais',
		'quando',
		'onde',
		'fale',
		'me fale',
		'me explica',
		'me explique',
		'me diga',
		'diga',
		'voc√™',
		'explique',
		'descreva',
		'j√°',
		'tu j√°',
	];

	debugLogRenderer('Fim da fun√ß√£o: "looksLikeQuestion"');
	return s.includes('?') || questionStarters.some(q => s.startsWith(q));
}

function isGarbageSentence(t) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "isGarbageSentence"');
	const s = t.toLowerCase();
	debugLogRenderer('Fim da fun√ß√£o: "isGarbageSentence"');
	return ['obrigado', 'at√© a pr√≥xima', 'finalizando'].some(w => s.includes(w));
}

// Encurta uma resposta em markdown para at√© `maxSentences` senten√ßas.
function shortenAnswer(markdownText, maxSentences = 2) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "shortenAnswer"');
	if (!markdownText) return markdownText;

	// remove blocos de c√≥digo temporariamente para evitar cortes ruins
	const codeBlocks = [];
	const withoutCode = markdownText.replace(/```[\s\S]*?```/g, match => {
		codeBlocks.push(match);
		return `__CODEBLOCK_${codeBlocks.length - 1}__`;
	});

	// remove inline code
	const tmp = withoutCode.replace(/`([^`]*)`/g, '$1');

	// extrai senten√ßas por pontua√ß√£o final
	const parts = tmp.split(/(?<=[\.\?!])\s+/);

	const take = parts.slice(0, maxSentences).join(' ').trim();

	// restaura blocos de c√≥digo, caso existam (apendados ao final)
	let result = take;
	if (codeBlocks.length) {
		result += '\n\n' + codeBlocks.join('\n\n');
	}

	// garante pontua√ß√£o final
	if (!/[\.\?!]$/.test(result)) result = result + '.';

	debugLogRenderer('Fim da fun√ß√£o: "shortenAnswer"');
	return result;
}

function isIncompleteQuestion(t) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "isIncompleteQuestion"');
	if (!t) return false;
	const s = t.trim();
	// casos √≥bvios: cont√©m retic√™ncias (..., ‚Ä¶) ‚Äî normalmente placeholders ou cortes
	if (s.includes('...') || s.includes('‚Ä¶')) return true;

	// termina com fragmento muito curto seguido de pontua√ß√£o (ex: "O que √© a...")
	// ou termina com apenas 1-3 letras antes do fim (sinal de corte)
	if (/\b\w{1,3}[\.]{0,3}$/.test(s) && /\.\.{1,3}$/.test(s)) return true;

	// termina com palavra muito curta e sem contexto (ex: endsWith ' a' )
	if (/\b[a-z]{1,2}$/.test(s.toLowerCase())) return true;

	debugLogRenderer('Fim da fun√ß√£o: "isIncompleteQuestion"');
	return false;
}

function getNavigableQuestionIds() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "getNavigableQuestionIds"');
	const ids = [];

	// CURRENT s√≥ entra se tiver texto
	if (currentQuestion.text && currentQuestion.text.trim().length > 0) {
		ids.push(CURRENT_QUESTION_ID);
	}

	// Hist√≥rico (mais recente primeiro)
	ids.push(
		...questionsHistory
			.slice()
			.reverse()
			.map(q => q.id),
	);

	debugLogRenderer('Fim da fun√ß√£o: "getNavigableQuestionIds"');
	return ids;
}

function findAnswerByQuestionId(questionId) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "findAnswerByQuestionId"');

	if (!questionId) {
		// ID inv√°lido
		debugLogRenderer('Fim da fun√ß√£o: "findAnswerByQuestionId"');
		return false;
	}

	debugLogRenderer('Fim da fun√ß√£o: "findAnswerByQuestionId"');
	return answeredQuestions.has(questionId);
}

function promoteCurrentToHistory(text) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "promoteCurrentToHistory"');
	console.log('üìö promovendo pergunta para hist√≥rico:', text);

	// evita duplica√ß√£o no hist√≥rico: se a √∫ltima entrada √© igual (normalizada), n√£o adiciona
	const last = questionsHistory.length ? questionsHistory[questionsHistory.length - 1] : null;
	if (last && normalizeForCompare(last.text) === normalizeForCompare(text)) {
		console.log('üîï pergunta igual j√° presente no hist√≥rico ‚Äî pulando promo√ß√£o');

		// limpa CURRENT mas preserva sele√ß√£o conforme antes
		const prevSelected = selectedQuestionId;
		currentQuestion = { text: '', lastUpdate: 0, finalized: false, lastUpdateTime: null, createdAt: null };
		if (prevSelected === null || prevSelected === CURRENT_QUESTION_ID) {
			selectedQuestionId = CURRENT_QUESTION_ID;
		} else {
			selectedQuestionId = prevSelected;
		}

		renderQuestionsHistory();
		renderCurrentQuestion();
		return;
	}

	const newId = crypto.randomUUID();

	questionsHistory.push({
		id: newId,
		text,
		createdAt: currentQuestion.createdAt || Date.now(),
		lastUpdateTime: currentQuestion.lastUpdateTime || currentQuestion.createdAt || Date.now(),
	});

	// preserva sele√ß√£o do usu√°rio: se n√£o havia sele√ß√£o expl√≠cita ou estava no CURRENT,
	// mant√©m a sele√ß√£o no CURRENT para que o novo CURRENT seja principal.
	const prevSelected = selectedQuestionId;

	currentQuestion = { text: '', lastUpdate: 0, finalized: false, lastUpdateTime: null, createdAt: null };

	if (prevSelected === null || prevSelected === CURRENT_QUESTION_ID) {
		selectedQuestionId = CURRENT_QUESTION_ID;
	} else {
		// usu√°rio tinha selecionado algo no hist√≥rico ‚Äî preserva essa sele√ß√£o
		selectedQuestionId = prevSelected;
	}

	renderQuestionsHistory();
	renderCurrentQuestion();

	debugLogRenderer('Fim da fun√ß√£o: "promoteCurrentToHistory"');
}

function isQuestionReady(text) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "isQuestionReady"');
	if (!ModeController.isInterviewMode()) return true;

	const trimmed = text.trim();

	// üî• entrevistas podem ter perguntas curtas ("O que √© POO")
	if (trimmed.length < 10) return false;

	// ignora despedidas
	if (isEndingPhrase(trimmed)) return false;

	// heur√≠stica simples de pergunta
	const questionIndicators = [
		'o que',
		'por que',
		'porque',
		'como',
		'qual',
		'quais',
		'quando',
		'onde',
		'fale',
		'me fale',
		'me explica',
		'me explique',
		'me diga',
		'diga',
		'voc√™',
		'explique',
		'descreva',
		'j√°',
		'tu j√°',
	];

	const lower = trimmed.toLowerCase();

	const hasIndicator = questionIndicators.some(q => lower.includes(q));

	const hasQuestionMark = trimmed.includes('?');

	debugLogRenderer('Fim da fun√ß√£o: "isQuestionReady"'); // s√≥ dispara se houver ind√≠cio real
	return hasIndicator || hasQuestionMark;
}

function isEndingPhrase(text) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "isEndingPhrase"');
	const normalized = text.toLowerCase().trim();

	debugLogRenderer('Fim da fun√ß√£o: "isEndingPhrase"');
	return OUTPUT_ENDING_PHRASES.some(p => normalized === p);
}

/* ===============================
   TRANSCRI√á√ÉO LOCAL
=============================== */

function setTranscriptionMode(useLocal) {
	USE_LOCAL_WHISPER = useLocal;
	console.log(`üé§ Modo de transcri√ß√£o: ${useLocal ? 'WHISPER LOCAL' : 'OPENAI'}`);
}

async function transcribeAudio(blob) {
	transcriptionMetrics.audioStartTime = Date.now();
	transcriptionMetrics.audioSize = blob.size;

	const buffer = Buffer.from(await blob.arrayBuffer());
	console.log(`üé§ Transcri√ß√£o (${USE_LOCAL_WHISPER ? 'Local' : 'OpenAI'}): ${blob.size} bytes`);
	console.log(
		`‚è±Ô∏è In√≠cio: ${new Date(transcriptionMetrics.audioStartTime).toLocaleTimeString()}.${
			transcriptionMetrics.audioStartTime % 1000
		}`,
	);

	if (USE_LOCAL_WHISPER) {
		try {
			console.log(`üöÄ Enviando para Whisper local...`);
			transcriptionMetrics.whisperStartTime = Date.now();

			const result = await ipcRenderer.invoke('transcribe-local', buffer);

			transcriptionMetrics.whisperEndTime = Date.now();
			const whisperTime = transcriptionMetrics.whisperEndTime - transcriptionMetrics.whisperStartTime;

			console.log(`‚úÖ Whisper local conclu√≠do em ${whisperTime}ms`);
			console.log(`üìù Resultado (${result.length} chars): "${result.substring(0, 80)}..."`);

			// Log intermedi√°rio
			console.log(
				`üìä Whisper: ${whisperTime}ms para ${blob.size} bytes (${Math.round(blob.size / whisperTime)} bytes/ms)`,
			);

			return result;
		} catch (error) {
			console.error('‚ùå Whisper local falhou:', error.message);
			// Fallback para OpenAI
			try {
				return await ipcRenderer.invoke('transcribe-audio', buffer);
			} catch (openaiError) {
				throw new Error(`Falha na transcri√ß√£o: ${openaiError.message}`);
			}
		}
	} else {
		transcriptionMetrics.whisperStartTime = Date.now();
		const result = await ipcRenderer.invoke('transcribe-audio', buffer);
		transcriptionMetrics.whisperEndTime = Date.now();

		const whisperTime = transcriptionMetrics.whisperEndTime - transcriptionMetrics.whisperStartTime;
		console.log(`‚úÖ OpenAI conclu√≠do em ${whisperTime}ms`);

		return result;
	}
}

async function transcribeAudioPartial(blob) {
	const buffer = Buffer.from(await blob.arrayBuffer());

	if (USE_LOCAL_WHISPER) {
		try {
			return await ipcRenderer.invoke('transcribe-local-partial', buffer);
		} catch (error) {
			console.warn('‚ö†Ô∏è Whisper local parcial falhou:', error.message);
			return '';
		}
	} else {
		return await ipcRenderer.invoke('transcribe-audio-partial', buffer);
	}
}

/* ===============================
   TRANSCRI√á√ÉO VOSK (MODO ENTREVISTA)
=============================== */

let voskAccumulatedText = ''; // Acumula resultado parcial do Vosk
let voskPartialTimer = null;
let voskScriptProcessor = null; // ScriptProcessorNode para capturar PCM bruto
let voskAudioBuffer = []; // Acumula PCM entre envios

/**
 * Converte array de floats PCM para Int16Array
 */
function floatToPCM16(floatArray) {
	const pcm16 = new Int16Array(floatArray.length);
	for (let i = 0; i < floatArray.length; i++) {
		pcm16[i] = Math.max(-1, Math.min(1, floatArray[i])) * 0x7fff;
	}
	return pcm16;
}

/**
 * Inicia captura de PCM bruto do √°udio (substitui MediaRecorder para Vosk)
 * @param {MediaStreamAudioSourceNode} source - Source do √°udio da stream
 * @deprecated Usar MediaRecorder com timeslice ao inv√©s de ScriptProcessorNode
 */
function startVoskPcmCapture(source) {
	console.warn('‚ö†Ô∏è startVoskPcmCapture deprecated - use MediaRecorder timeslice instead');
}

/**
 * Para captura de PCM bruto do Vosk
 */
function stopVoskPcmCapture() {
	try {
		if (voskScriptProcessor) {
			voskScriptProcessor.disconnect();
			voskScriptProcessor.onaudioprocess = null;
			voskScriptProcessor = null;
		}
		voskAudioBuffer = [];
		console.log('‚úÖ Captura PCM para Vosk parada');
	} catch (error) {
		console.error('‚ùå Erro ao parar captura PCM:', error);
	}
}

/**
 * Transcreve chunk de blob com Vosk (modo entrevista - padr√£o Deepgram)
 * Envia blobs WebM diretamente para Vosk via IPC
 */
/**
 * üö´ DEPRECADO: Vosk n√£o funciona com chunks WebM fragmentados do MediaRecorder
 * MediaRecorder gera blobs WebM incompletos que ffmpeg/Vosk rejeitam
 * Solu√ß√£o: usar apenas Whisper para OUTPUT (funciona bem com WebM fragmentado)
 * @deprecated
 */
async function voskTranscribeChunkFromBlob(blob) {
	console.warn('‚ö†Ô∏è voskTranscribeChunkFromBlob deprecado - usar Whisper ao inv√©s');
	// Fun√ß√£o removida - ver transcribeOutput() para transcri√ß√£o final de sa√≠da
}

/**
 * Inicia captura de PCM bruto do √°udio (substitui MediaRecorder para Vosk)
 * @param {MediaStreamAudioSourceNode} source - Source do √°udio da stream
 * @deprecated Usar MediaRecorder com timeslice ao inv√©s de ScriptProcessorNode
 */
function startVoskPcmCapture(source) {
	console.warn('‚ö†Ô∏è startVoskPcmCapture deprecated - usar MediaRecorder com timeslice ao inv√©s de ScriptProcessorNode');
	// Fun√ß√£o deprecada mantida para compatibilidade reversa
}

/* ===============================
   DISPOSITIVOS / CONTROLE DE √ÅUDIO
=============================== */

async function startAudio() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "startAudio"');

	// Se houver dispositivo de entrada selecionado, inicia a captura de √°udio
	if (UIElements.inputSelect?.value) await startInput();
	// Se houver dispositivo de sa√≠da selecionado, inicia a captura de √°udio
	if (UIElements.outputSelect?.value) await startOutput();

	debugLogRenderer('Fim da fun√ß√£o: "startAudio"');
}

async function stopAudio() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "stopAudio"');

	if (currentQuestion.text) closeCurrentQuestionForced();

	inputRecorder?.state === 'recording' && inputRecorder.stop();
	outputRecorder?.state === 'recording' && outputRecorder.stop();

	// üÜï VOSK: Reset do estado
	if (ModeController.isInterviewMode()) {
		voskAccumulatedText = '';
		if (voskPartialTimer) {
			clearTimeout(voskPartialTimer);
			voskPartialTimer = null;
		}
	}

	stopInputMonitor();
	stopOutputMonitor();

	debugLogRenderer('Fim da fun√ß√£o: "stopAudio"');
}

async function restartAudioPipeline() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "restartAudioPipeline"');

	stopAudio();

	debugLogRenderer('Fim da fun√ß√£o: "restartAudioPipeline"');
}

/* ===============================
   AUDIO - VOLUME MONITORING
=============================== */

// Inicia apenas monitoramento de volume (sem gravar)
async function startInputVolumeMonitoring() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "startInputVolumeMonitoring"');

	if (APP_CONFIG.MODE_DEBUG) {
		console.log('üé§ Monitoramento de volume entrada (modo teste)...');
		return;
	}

	if (!UIElements.inputSelect?.value) {
		console.log('‚ö†Ô∏è Nenhum dispositivo input selecionado');
		return;
	}

	if (!audioContext) {
		audioContext = new AudioContext();
	}

	// üî• NOVO: Se j√° tem stream ativa, n√£o faz nada
	if (inputStream && inputAnalyser) {
		console.log('‚ÑπÔ∏è Monitoramento de volume de entrada j√° ativo');
		return;
	}

	try {
		// Verificar se isRunning √© false antes de iniciar o stream
		if (!isRunning) {
			console.log('üîÑ Iniciando stream de √°udio (input)...');

			inputStream = await navigator.mediaDevices.getUserMedia({
				audio: { deviceId: { exact: UIElements.inputSelect.value } },
			});

			const source = audioContext.createMediaStreamSource(inputStream);

			inputAnalyser = audioContext.createAnalyser();
			inputAnalyser.fftSize = 256;
			inputData = new Uint8Array(inputAnalyser.frequencyBinCount);
			source.connect(inputAnalyser);

			console.log('‚úÖ Monitoramento de volume de entrada iniciado com sucesso');
			updateInputVolume(); // üî• Inicia o loop de atualiza√ß√£o
		}
	} catch (error) {
		console.error('‚ùå Erro ao iniciar monitoramento de volume de entrada:', error);
		inputStream = null;
		inputAnalyser = null;
	}

	debugLogRenderer('Fim da fun√ß√£o: "startInputVolumeMonitoring"');
}

// Inicia apenas monitoramento de volume para output (sem gravar)
async function startOutputVolumeMonitoring() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "startOutputVolumeMonitoring"');

	// Se o modo de debug estiver ativo, retorna
	if (APP_CONFIG.MODE_DEBUG) {
		console.log('üîä Monitoramento de volume sa√≠da (modo teste)...');
		return;
	}

	// Se n√£o houver dispositivo de sa√≠da selecionado, retorna
	if (!UIElements.outputSelect?.value) {
		console.log('‚ö†Ô∏è Nenhum dispositivo output selecionado');
		return;
	}

	// Se n√£o houver contexto de √°udio, cria um novo
	if (!audioContext) {
		audioContext = new AudioContext();
	}

	// Se j√° houver stream e analisador de frequ√™ncia ativos, retorna
	if (outputStream && outputAnalyser) {
		console.log('‚ÑπÔ∏è Monitoramento de volume de sa√≠da j√° ativo');
		return;
	}

	try {
		// Se isRunning for false, inicia o stream de √°udio (output)
		if (!isRunning) {
			console.log('üîÑ Iniciando stream de √°udio (output)...');

			// Cria a stream de √°udio (outputStream)
			await createOutputStream();

			// Inicia o loop de atualiza√ß√£o do volume de sa√≠da
			updateOutputVolume();
		}

		debugLogRenderer('Fim da fun√ß√£o: "startOutputVolumeMonitoring"');
	} catch (error) {
		console.error('‚ùå Erro ao iniciar monitoramento de volume de sa√≠da:', error);

		// Limpa a stream e o analisador de frequ√™ncia (outputStream e outputAnalyser)
		outputStream = null;
		outputAnalyser = null;
	}
}

function stopInputVolumeMonitoring() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "stopInputVolumeMonitoring"');

	// Se isRunning true, n√£o para o monitoramento
	if (isRunning) {
		console.log('‚ÑπÔ∏è Monitoramento de volume de entrada em execu√ß√£o, isRunning = true ‚Äî pulando parada');

		debugLogRenderer('Fim da fun√ß√£o: "stopInputVolumeMonitoring"');
		return;
	}

	// 1. Para o loop de anima√ß√£o
	if (inputVolumeAnimationId) {
		cancelAnimationFrame(inputVolumeAnimationId);
		inputVolumeAnimationId = null;
	}

	// 2. Para as tracks de √°udio para economizar energia/recurso
	if (inputStream) {
		inputStream.getTracks().forEach(track => track.stop());
		inputStream = null;
	}

	inputAnalyser = null;
	inputData = null;

	// 3. Zera a UI
	emitUIChange('onInputVolumeUpdate', { percent: 0 });

	console.log('üõë Monitoramento de volume de entrada parado');

	debugLogRenderer('Fim da fun√ß√£o: "stopInputVolumeMonitoring"');
}

function stopOutputVolumeMonitoring() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "stopOutputVolumeMonitoring"');

	// Se isRunning true, n√£o para o monitoramento
	if (isRunning) {
		console.log('‚ÑπÔ∏è Monitoramento de volume de sa√≠da em execu√ß√£o, isRunning = true ‚Äî pulando parada');

		debugLogRenderer('Fim da fun√ß√£o: "stopOutputVolumeMonitoring"');
		return;
	}

	// 1. Para o loop de anima√ß√£o
	if (outputVolumeAnimationId) {
		cancelAnimationFrame(outputVolumeAnimationId);
		outputVolumeAnimationId = null;
	}

	// 2.Para as tracks de √°udio para economizar energia/recurso
	if (outputStream) {
		outputStream.getTracks().forEach(track => track.stop());
		outputStream = null;
	}

	outputAnalyser = null;
	outputData = null;

	// 3. Zera a UI
	emitUIChange('onOutputVolumeUpdate', { percent: 0 });

	console.log('üõë Monitoramento de volume de sa√≠da parado');

	debugLogRenderer('Fim da fun√ß√£o: "stopOutputVolumeMonitoring"');
}

/* ===============================
   AUDIO - INPUT (VOC√ä)
=============================== */

async function startInput() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "startInput"');

	if (APP_CONFIG.MODE_DEBUG) {
		const text = 'Iniciando monitoramento de entrada de √°udio (modo teste)...';
		addTranscript(YOU, text);
		return;
	}

	if (!UIElements.inputSelect?.value) return;

	if (!audioContext) {
		audioContext = new AudioContext();
	}

	// CR√çTICO: Evita recriar recorder E stream se j√° existem
	if (inputRecorder && inputRecorder.state !== 'inactive') {
		console.log('‚ÑπÔ∏è inputRecorder j√° existe e est√° ativo, pulando reconfigura√ß√£o');
		return;
	}

	// Se j√° existe stream mas precisa reconfigurar, limpa primeiro
	if (inputStream) {
		console.log('üßπ Limpando stream de entrada anterior antes de recriar');
		inputStream.getTracks().forEach(t => t.stop());
		inputStream = null;
	}

	try {
		inputStream = await navigator.mediaDevices.getUserMedia({
			audio: { deviceId: { exact: UIElements.inputSelect.value } },
		});

		const source = audioContext.createMediaStreamSource(inputStream);

		inputAnalyser = audioContext.createAnalyser();
		inputAnalyser.fftSize = 256;
		inputData = new Uint8Array(inputAnalyser.frequencyBinCount);
		source.connect(inputAnalyser);

		// recorder SEMPRE existe
		inputRecorder = new MediaRecorder(inputStream, {
			mimeType: 'audio/webm;codecs=opus',
		});

		inputRecorder.ondataavailable = e => {
			console.log('üî• input.ondataavailable - chunk tamanho:', e.data?.size || e.data?.byteLength || 'n/a');

			inputChunks.push(e.data);

			// MODO ENTREVISTA ‚Äì permite transcri√ß√£o incremental
			if (ModeController.isInterviewMode()) {
				console.log('üß© handlePartialInputChunk chamado (input)');
				handlePartialInputChunk(e.data);
			}
		};

		inputRecorder.onstop = () => {
			console.log('‚èπÔ∏è inputRecorder.onstop chamado');

			// marca o momento exato em que a grava√ß√£o parou
			lastInputStopAt = Date.now();
			const recordingDuration = lastInputStopAt - lastInputStartAt;
			console.log('‚è±Ô∏è Parada:', new Date(lastInputStopAt).toLocaleTimeString());
			console.log('‚è±Ô∏è Dura√ß√£o da grava√ß√£o:', recordingDuration, 'ms');

			// Cancela qualquer timer pendente de transcri√ß√£o parcial
			// Isso evita que handlePartialInputChunk processe chunks ap√≥s onstop
			if (inputPartialTimer) {
				clearTimeout(inputPartialTimer);
				inputPartialTimer = null;
				console.log('‚è±Ô∏è Cancelado timer de transcri√ß√£o parcial (inputPartialTimer)');
			}

			// Limpa chunks parciais acumulados para evitar duplica√ß√£o
			inputPartialChunks = [];
			console.log('üóëÔ∏è Limpos chunks parciais acumulados (inputPartialChunks)');

			// adiciona placeholder visual para indicar que estamos aguardando a transcri√ß√£o
			// usa startAt se dispon√≠vel para mostrar o hor√°rio inicial enquanto aguarda
			const timeForPlaceholder = lastInputStartAt || lastInputStopAt;
			lastInputPlaceholderEl = addTranscript(YOU, '...', timeForPlaceholder);
			if (lastInputPlaceholderEl) {
				lastInputPlaceholderEl.dataset.stopAt = lastInputStopAt;
				if (lastInputStartAt) lastInputPlaceholderEl.dataset.startAt = lastInputStartAt;
			}

			transcribeInput();
		};

		// Inicia loop de volume apenas se n√£o estiver rodando
		if (!inputVolumeAnimationId) {
			updateInputVolume();
		}

		console.log('‚úÖ startInput: Configurado com sucesso');
	} catch (error) {
		console.error('‚ùå Erro em startInput:', error);
		inputStream = null;
		inputRecorder = null;
		throw error;
	}

	debugLogRenderer('Fim da fun√ß√£o: "startInput"');
}

function updateInputVolume() {
	//debugLogRenderer('In√≠cio da fun√ß√£o: "updateInputVolume"');

	// CR√çTICO: Verifica se deve continuar ANTES de fazer qualquer processamento
	if (!inputAnalyser || !inputData) {
		console.log('‚ö†Ô∏è updateInputVolume: analyser ou data n√£o dispon√≠vel, parando loop');
		if (inputVolumeAnimationId) {
			cancelAnimationFrame(inputVolumeAnimationId);
			inputVolumeAnimationId = null;
		}
		emitUIChange('onInputVolumeUpdate', { percent: 0 });
		return;
	}

	try {
		inputAnalyser.getByteFrequencyData(inputData);
		const avg = inputData.reduce((a, b) => a + b, 0) / inputData.length;
		const percent = Math.min(100, Math.round((avg / 80) * 100));

		// Emite evento em vez de atualizar DOM diretamente
		emitUIChange('onInputVolumeUpdate', { percent });

		if (avg > INPUT_SPEECH_THRESHOLD && inputRecorder && isRunning) {
			if (!inputSpeaking) {
				inputSpeaking = true;
				inputChunks = [];

				const slice = ModeController.mediaRecorderTimeslice();
				lastInputStartAt = Date.now();
				console.log(
					'üéôÔ∏è iniciando grava√ß√£o de entrada (inputRecorder.start) - startAt',
					new Date(lastInputStartAt).toLocaleTimeString(),
				);
				slice ? inputRecorder.start(slice) : inputRecorder.start();
			}
			if (inputSilenceTimer) {
				clearTimeout(inputSilenceTimer);
				inputSilenceTimer = null;
			}
		} else if (inputSpeaking && !inputSilenceTimer && inputRecorder) {
			inputSilenceTimer = setTimeout(() => {
				inputSpeaking = false;
				inputSilenceTimer = null;
				console.log('‚èπÔ∏è parando grava√ß√£o de entrada por sil√™ncio (inputRecorder.stop)');
				if (inputRecorder && inputRecorder.state === 'recording') {
					inputRecorder.stop();
				}
			}, INPUT_SILENCE_TIMEOUT);
		}
	} catch (error) {
		console.error('‚ùå Erro em updateInputVolume:', error);
		if (inputVolumeAnimationId) {
			cancelAnimationFrame(inputVolumeAnimationId);
			inputVolumeAnimationId = null;
		}
		emitUIChange('onInputVolumeUpdate', { percent: 0 });
		return;
	}

	// Continua o loop apenas se tudo estiver OK
	inputVolumeAnimationId = requestAnimationFrame(updateInputVolume);

	//debugLogRenderer('Fim da fun√ß√£o: "updateInputVolume"');
}

function stopInputMonitor() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "stopInputMonitor"');

	// 1. Para o loop de animation PRIMEIRO
	if (inputVolumeAnimationId) {
		cancelAnimationFrame(inputVolumeAnimationId);
		inputVolumeAnimationId = null;
		console.log('‚úÖ Loop de anima√ß√£o de entrada cancelado');
	}

	// 2. Para o recorder se estiver gravando
	if (inputRecorder) {
		if (inputRecorder.state === 'recording') {
			console.log('‚èπÔ∏è Parando recorder de entrada...');
			inputRecorder.stop();
		}
		inputRecorder = null;
	}

	// 3. Fecha a stream
	if (inputStream) {
		inputStream.getTracks().forEach(t => {
			t.stop();
			console.log('‚úÖ Track de entrada parada:', t.label);
		});
		inputStream = null;
	}

	// 4. Limpa analyser e dados
	inputAnalyser = null;
	inputData = null;

	// 5. Reseta estado
	inputSpeaking = false;
	if (inputSilenceTimer) {
		clearTimeout(inputSilenceTimer);
		inputSilenceTimer = null;
	}

	// 6. Atualiza UI
	emitUIChange('onInputVolumeUpdate', { percent: 0 });

	debugLogRenderer('Fim da fun√ß√£o: "stopInputMonitor"');
	return Promise.resolve();
}

/* ===============================
   AUDIO - OUTPUT (OUTROS) - VIA VOICEMEETER
=============================== */

async function createOutputStream() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "createOutputStream"');

	// Cria a stream de √°udio (outputStream)
	outputStream = await navigator.mediaDevices.getUserMedia({
		audio: { deviceId: { exact: UIElements.outputSelect.value } },
	});

	// Cria o source de √°udio (source)
	const source = audioContext.createMediaStreamSource(outputStream);

	// Cria o analisador de frequ√™ncia (outputAnalyser)
	outputAnalyser = audioContext.createAnalyser();
	// Define o tamanho do FFT (fftSize) como 256
	outputAnalyser.fftSize = 256;
	// Cria os dados (outputData)
	outputData = new Uint8Array(outputAnalyser.frequencyBinCount);
	// Conecta o source ao analisador de frequ√™ncia
	source.connect(outputAnalyser);

	debugLogRenderer('Fim da fun√ß√£o: "createOutputStream"');

	return source;
}

async function startOutput() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "startOutput"');

	// Se o modo de debug estiver ativo, retorna
	if (APP_CONFIG.MODE_DEBUG) {
		const text = 'Iniciando monitoramento de sa√≠da de √°udio (modo teste)...';
		addTranscript(OTHER, text);
		return;
	}

	// Se n√£o houver dispositivo de sa√≠da selecionado, retorna
	if (!UIElements.outputSelect?.value) {
		console.log('‚ö†Ô∏è Nenhum dispositivo output selecionado');
		return;
	}

	// Se n√£o houver contexto de √°udio, cria um novo
	if (!audioContext) {
		audioContext = new AudioContext();
	}

	// Se j√° houver outputRecorder e ele estiver ativo, retorna
	if (outputRecorder && outputRecorder.state !== 'inactive') {
		console.log('‚ÑπÔ∏è outputRecorder j√° existe e est√° ativo, pulando reconfigura√ß√£o');
		return;
	}

	// Se j√° houver outputStream, limpa primeiro
	if (outputStream) {
		console.log('üßπ Limpando stream de sa√≠da anterior antes de recriar');
		outputStream.getTracks().forEach(t => t.stop());
		outputStream = null;
	}

	try {
		console.log('üîÑ startOutput: Configurando monitoramento de sa√≠da de √°udio...');

		// Cria a stream de √°udio (outputStream)
		await createOutputStream();

		// Cria o recorder (outputRecorder), recorder SEMPRE existe
		outputRecorder = new MediaRecorder(outputStream, {
			mimeType: 'audio/webm;codecs=opus',
		});

		// Define o callback para quando houver dados dispon√≠veis no outputRecorder, acionado ao chamar outputRecorder.start()
		outputRecorder.ondataavailable = e => {
			console.log(
				'üî• outputRecorder.ondataavailable chamado - chunk tamanho:',
				e.data?.size || e.data?.byteLength || 'n/a',
			);

			// Adiciona o chunk (peda√ßos de dados) ao array de chunks de sa√≠da
			outputChunks.push(e.data);

			// MODO ENTREVISTA ‚Äì permite transcri√ß√£o incremental
			if (ModeController.isInterviewMode()) {
				console.log('üß© handlePartialOutputChunk chamado (output)');
				handlePartialOutputChunk(e.data);
			}
		};

		// Define o callback para quando o outputRecorder for parado, acionado ao chamar outputRecorder.stop()
		outputRecorder.onstop = () => {
			console.log('‚èπÔ∏è outputRecorder.onstop chamado');

			// Marca o momento exato em que a grava√ß√£o parou
			lastOutputStopAt = Date.now();
			const recordingDuration = lastOutputStopAt - lastOutputStartAt;
			console.log('‚è±Ô∏è Parada: ' + new Date(lastOutputStopAt).toLocaleTimeString());
			console.log('‚è±Ô∏è Dura√ß√£o da grava√ß√£o:', recordingDuration, 'ms');

			// Cancela qualquer timer pendente de transcri√ß√£o parcial
			// Isso evita que transcribeOutputPartial processe chunks ap√≥s onstop
			if (outputPartialTimer) {
				clearTimeout(outputPartialTimer);
				outputPartialTimer = null;
				console.log('‚è±Ô∏è Cancelado timer de transcri√ß√£o parcial (outputPartialTimer)');
			}

			// Limpa chunks parciais acumulados para evitar duplica√ß√£o
			outputPartialChunks = [];
			console.log('üóëÔ∏è Limpos chunks parciais acumulados (outputPartialChunks)');

			// Fluxo padr√£o (Whisper): Adiciona placeholder visual para indicar que estamos aguardando a transcri√ß√£o
			const timeForPlaceholder = lastOutputStartAt || lastOutputStopAt;
			lastOutputPlaceholderEl = addTranscript(OTHER, '...', timeForPlaceholder);

			// Se o placeholder foi criado, define os atributos de startAt e stopAt
			if (lastOutputPlaceholderEl) {
				lastOutputPlaceholderEl.dataset.stopAt = lastOutputStopAt;
				if (lastOutputStartAt) lastOutputPlaceholderEl.dataset.startAt = lastOutputStartAt;
			}

			// Inicia a transcri√ß√£o do √°udio de sa√≠da (Whisper)
			transcribeOutput();
		};

		// Inicia o loop de atualiza√ß√£o do volume de sa√≠da, se n√£o estiver rodando
		if (!outputVolumeAnimationId) {
			updateOutputVolume();
		}

		console.log('‚úÖ startOutput: Monitoramento de sa√≠da de √°udio configurado com sucesso');
	} catch (error) {
		console.error('‚ùå Erro em startOutput:', error);

		outputStream = null;
		outputRecorder = null;
		throw error;
	}

	debugLogRenderer('Fim da fun√ß√£o: "startOutput"');
}

function updateOutputVolume() {
	//debugLogRenderer('In√≠cio da fun√ß√£o: "updateOutputVolume"');

	// Cr√≠tico: Verifica se o analisador de frequ√™ncia (outputAnalyser) e os dados (outputData)
	// est√£o dispon√≠veis antes de continuar o loop de anima√ß√£o
	if (!outputAnalyser || !outputData) {
		console.log('‚ö†Ô∏è updateOutputVolume: outputAnalyser ou outputData n√£o dispon√≠vel, parando loop de anima√ß√£o');

		// Se o loop de anima√ß√£o (outputVolumeAnimationId) estiver definido, limpa o loop de anima√ß√£o
		if (outputVolumeAnimationId) {
			// Para o loop de anima√ß√£o
			cancelAnimationFrame(outputVolumeAnimationId);
			// Limpa o loop de anima√ß√£o
			outputVolumeAnimationId = null;
		}

		// Emite o evento 'onOutputVolumeUpdate' para atualizar o volume de sa√≠da
		emitUIChange('onOutputVolumeUpdate', { percent: 0 });

		return;
	}

	try {
		// Obt√©m os dados do analisador de frequ√™ncia (outputAnalyser)
		outputAnalyser.getByteFrequencyData(outputData);
		// Calcula o volume m√©dio (avg) dos dados do analisador de frequ√™ncia (outputData)
		const avg = outputData.reduce((a, b) => a + b, 0) / outputData.length;
		// Calcula o percentual de volume (percent) dos dados do analisador de frequ√™ncia (outputData)
		const percent = Math.min(100, Math.round((avg / 60) * 100));

		// Emite o evento 'onOutputVolumeUpdate' para atualizar o volume de sa√≠da
		emitUIChange('onOutputVolumeUpdate', { percent });

		// Se o volume m√©dio (avg) estiver acima do limite (OUTPUT_SPEECH_THRESHOLD)
		// e o recorder (outputRecorder) estiver rodando e o isRunning for true, inicia a grava√ß√£o de sa√≠da
		if (avg > OUTPUT_SPEECH_THRESHOLD && outputRecorder && isRunning) {
			// Se o outputSpeaking for false, inicia a grava√ß√£o de sa√≠da
			if (!outputSpeaking) {
				// Define o estado de outputSpeaking como true
				outputSpeaking = true;
				// Limpa o array de chunks de sa√≠da
				outputChunks = [];

				// Define o momento exato em que a grava√ß√£o de sa√≠da foi iniciada
				lastOutputStartAt = Date.now();

				console.log('üéôÔ∏è In√≠cio: ' + new Date(lastOutputStartAt).toLocaleTimeString());

				// Usar o mesmo timeslice que INPUT para manter consist√™ncia
				const slice = ModeController.mediaRecorderTimeslice();
				slice ? outputRecorder.start(slice) : outputRecorder.start();
			}
			if (outputSilenceTimer) {
				clearTimeout(outputSilenceTimer);
				outputSilenceTimer = null;
			}
		} else if (outputSpeaking && !outputSilenceTimer && outputRecorder) {
			// Define o timer de sil√™ncio (outputSilenceTimer)
			outputSilenceTimer = setTimeout(() => {
				// Define o estado de outputSpeaking como false
				outputSpeaking = false;
				// Limpa o timer de sil√™ncio (outputSilenceTimer)
				outputSilenceTimer = null;

				console.log('‚èπÔ∏è parando grava√ß√£o de sa√≠da por sil√™ncio (outputRecorder.stop)');

				// Se o recorder (outputRecorder) estiver rodando, para a grava√ß√£o de sa√≠da
				if (outputRecorder && outputRecorder.state === 'recording') {
					// Para a grava√ß√£o de sa√≠da
					outputRecorder.stop();
				}
			}, OUTPUT_SILENCE_TIMEOUT); // Tempo de espera para sil√™ncio
		}
	} catch (error) {
		console.error('‚ùå Erro em updateOutputVolume:', error);
		// Se o loop de anima√ß√£o (outputVolumeAnimationId) estiver definido, limpa o loop de anima√ß√£o
		if (outputVolumeAnimationId) {
			// Para o loop de anima√ß√£o
			cancelAnimationFrame(outputVolumeAnimationId);
			// Limpa o loop de anima√ß√£o
			outputVolumeAnimationId = null;
		}
		// Emite o evento 'onOutputVolumeUpdate' para atualizar o volume de sa√≠da
		emitUIChange('onOutputVolumeUpdate', { percent: 0 });
		return;
	}

	// Continua o loop de anima√ß√£o apenas se tudo estiver OK
	outputVolumeAnimationId = requestAnimationFrame(updateOutputVolume);

	//debugLogRenderer('Fim da fun√ß√£o: "updateOutputVolume"');
}

function stopOutputMonitor() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "stopOutputMonitor"');

	// 1. Para o loop de animation PRIMEIRO
	if (outputVolumeAnimationId) {
		cancelAnimationFrame(outputVolumeAnimationId);
		outputVolumeAnimationId = null;
		console.log('‚úÖ Loop de anima√ß√£o de sa√≠da cancelado');
	}

	// 2. Para o recorder se estiver gravando
	if (outputRecorder) {
		if (outputRecorder.state === 'recording') {
			console.log('‚èπÔ∏è Parando recorder de sa√≠da...');
			outputRecorder.stop();
		}
		outputRecorder = null;
	}

	// 3. Fecha a stream
	if (outputStream) {
		outputStream.getTracks().forEach(t => {
			t.stop();
			console.log('‚úÖ Track de sa√≠da parada:', t.label);
		});
		outputStream = null;
	}

	// 4. Limpa analyser e dados
	outputAnalyser = null;
	outputData = null;

	// 5. Reseta estado
	outputSpeaking = false;
	if (outputSilenceTimer) {
		clearTimeout(outputSilenceTimer);
		outputSilenceTimer = null;
	}

	// 6. Atualiza UI
	emitUIChange('onOutputVolumeUpdate', { percent: 0 });

	debugLogRenderer('Fim da fun√ß√£o: "stopOutputMonitor"');
	return Promise.resolve();
}

/* ===============================
   MODO ENTREVISTA - TRANSCRI√á√ÉO PARCIAL
=============================== */

async function handlePartialInputChunk(blobChunk) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "handlePartialInputChunk"');
	if (!ModeController.isInterviewMode()) return;

	// ignora ru√≠do
	if (blobChunk.size < 200) return;

	inputPartialChunks.push(blobChunk);

	if (inputPartialTimer) clearTimeout(inputPartialTimer);

	inputPartialTimer = setTimeout(async () => {
		if (!inputPartialChunks.length) return;

		const blob = new Blob(inputPartialChunks, { type: 'audio/webm' });
		inputPartialChunks = [];

		try {
			const buffer = Buffer.from(await blob.arrayBuffer());
			const partialText = (await transcribeAudioPartial(blob))?.trim();

			if (partialText && !isGarbageSentence(partialText)) {
				addTranscript(YOU, partialText);
				handleSpeech(YOU, partialText);
			}
		} catch (err) {
			console.warn('‚ö†Ô∏è erro na transcri√ß√£o parcial (INPUT)', err);
		}
	}, 180); // janela curta (reduzida de 250 -> 180)

	debugLogRenderer('Fim da fun√ß√£o:  "handlePartialInputChunk"');
}

async function handlePartialOutputChunk(blobChunk) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "handlePartialOutputChunk"');
	if (!ModeController.isInterviewMode()) return;

	// ignora ru√≠do
	if (blobChunk.size < 200) return;

	outputPartialChunks.push(blobChunk);

	if (outputPartialTimer) clearTimeout(outputPartialTimer);

	outputPartialTimer = setTimeout(async () => {
		if (!outputPartialChunks.length) return;

		const blob = new Blob(outputPartialChunks, { type: 'audio/webm' });
		outputPartialChunks = [];

		try {
			const buffer = Buffer.from(await blob.arrayBuffer());
			const partialText = (await transcribeAudioPartial(blob))?.trim();

			if (partialText && !isGarbageSentence(partialText)) {
				addTranscript(OTHER, partialText);
				// N√ÉO chamar handleSpeech aqui - evita consolida√ß√£o nas parciais
				// consolida√ß√£o s√≥ acontece em transcribeOutput() para o √°udio final
			}
		} catch (err) {
			console.warn('‚ö†Ô∏è erro na transcri√ß√£o parcial (OUTPUT)', err);
		}
	}, 180); // janela curta (reduzida de 250 -> 180)

	debugLogRenderer('Fim da fun√ß√£o:  "handlePartialOutputChunk"');
}

function transcribeOutputPartial(blobChunk) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "transcribeOutputPartial"');

	// Se n√£o estiver no modo entrevista, retorna
	if (!ModeController.isInterviewMode()) {
		console.log('‚ÑπÔ∏è transcribeOutputPartial: retornando, modo entrevista n√£o ativo');

		debugLogRenderer('Fim da fun√ß√£o: "transcribeOutputPartial"');
		return;
	}

	// Desabilitado temporariamente (teste)
	if (DESABILITADO_TEMPORARIAMENTE) {
		debugLogRenderer('Fim da fun√ß√£o: "transcribeOutputPartial" üîí DESABILITADO TEMPORARIAMENTE');
		return;
	}

	// MODO ENTREVISTA ‚Äì permite transcri√ß√£o incremental

	// Ignora ru√≠do, evita blobs pequenos demais
	if (blobChunk.size < MIN_OUTPUT_AUDIO_SIZE_INTERVIEW) {
		console.log('‚ö†Ô∏è Ignorando blobChunk pequeno demais para transcri√ß√£o parcial (OUTPUT) - size:', blobChunk.size);

		debugLogRenderer('Fim da fun√ß√£o: "transcribeOutputPartial"');
		return;
	}

	// Adiciona o chunk ao array de chunks parciais de sa√≠da
	outputPartialChunks.push(blobChunk);
	console.log('üì¶ Chunk acumulado:', blobChunk.size, 'bytes | Total chunks:', outputPartialChunks.length);

	// Reinicia o timer para processar o chunk parcial ap√≥s um curto per√≠odo
	if (outputPartialTimer) clearTimeout(outputPartialTimer);

	// calcula delay respeitando um intervalo m√≠nimo entre requisi√ß√µes STT parciais
	const now = Date.now();
	const elapsedSinceLast = typeof lastPartialSttAt === 'number' ? now - lastPartialSttAt : Infinity;
	let intendedDelay = 120; // janela base para agrupar chunks
	if (elapsedSinceLast < PARTIAL_MIN_INTERVAL_MS) {
		intendedDelay = PARTIAL_MIN_INTERVAL_MS - elapsedSinceLast + 50; // pequeno buffer extra
		console.log('‚è±Ô∏è Ajustando delay parcial para respeitar rate-limit (ms):', intendedDelay);
	}

	// Define um timer para processar o chunk parcial ap√≥s X(ms)
	// Timeout curto (300ms) para agrupar ~5-8 chunks e enviar r√°pido para STT
	outputPartialTimer = setTimeout(async () => {
		// Se n√£o houver chunks parciais de sa√≠da, retorna
		if (!outputPartialChunks.length) {
			console.log('‚ö†Ô∏è Nenhum chunk parcial para processar');
			return;
		}

		// Cria um blob a partir dos chunks parciais de sa√≠da
		const blob = new Blob(outputPartialChunks, { type: 'audio/webm' });

		// Loga o tamanho total do blob parcial
		const totalSize = outputPartialChunks.reduce((acc, chunk) => acc + chunk.size, 0);
		console.log('üéµ Processando blob parcial:', totalSize, 'bytes de', outputPartialChunks.length, 'chunks');

		// Limpa o array de chunks parciais de sa√≠da ap√≥s criar blob
		outputPartialChunks = [];

		try {
			// Envia para transcri√ß√£o o blob parcial de sa√≠da
			const partialText = await transcribeAudioPartial(blob);
			// marca √∫ltimo envio parcial
			lastPartialSttAt = Date.now();
			console.log('üìù transcribeOutputPartial: Transcri√ß√£o recebida: ', partialText);

			// Ignora transcri√ß√£o vazia
			if (!partialText || partialText.trim().length === 0) {
				console.log('‚ö†Ô∏è Transcri√ß√£o vazia - ignorando');
				return;
			}

			// Ignora senten√ßas garbage
			if (isGarbageSentence(partialText)) {
				console.log('üóëÔ∏è Senten√ßa descartada (garbage):', partialText);
				return;
			}

			// acumula texto parcial
			outputPartialText += ' ' + partialText;
			outputPartialText = outputPartialText.trim();
			console.log('üìã Texto acumulado:', outputPartialText);

			// Atualiza UI com transcri√ß√£o parcial imediatamente (usa placeholder incremental)
			try {
				// cria placeholder se ainda n√£o existe (usa startAt se dispon√≠vel)
				if (!lastOutputPlaceholderEl) {
					const placeholderTime = lastOutputStartAt || Date.now();
					lastOutputPlaceholderEl = addTranscript(OTHER, '...', placeholderTime);
					if (lastOutputPlaceholderEl && lastOutputPlaceholderEl.dataset) {
						lastOutputPlaceholderEl.dataset.startAt = placeholderTime;
						// marca um stop provis√≥rio para o UI mostrar intervalo din√¢mico
						lastOutputPlaceholderEl.dataset.stopAt = Date.now();
					}
				} else if (lastOutputPlaceholderEl && lastOutputPlaceholderEl.dataset) {
					// atualiza stop provis√≥rio a cada parcial
					lastOutputPlaceholderEl.dataset.stopAt = Date.now();
				}

				// solicita ao config-manager atualiza√ß√£o parcial do placeholder (inclui m√©tricas provis√≥rias)
				emitUIChange('onPlaceholderUpdate', {
					speaker: OTHER,
					text: outputPartialText,
					timeStr: new Date(lastOutputStartAt || Date.now()).toLocaleTimeString(),
					startStr: new Date(lastOutputStartAt || Date.now()).toLocaleTimeString(),
					stopStr: new Date().toLocaleTimeString(),
					recordingDuration: Date.now() - (lastOutputStartAt || Date.now()),
					latency: 0,
					total: Date.now() - (lastOutputStartAt || Date.now()),
					provisional: true,
				});

				// atualiza currentQuestion para refletir texto parcial
				if (
					!currentQuestion.text ||
					normalizeForCompare(currentQuestion.text) !== normalizeForCompare(outputPartialText)
				) {
					currentQuestion.text = outputPartialText;
					currentQuestion.lastUpdate = Date.now();
					currentQuestion.lastUpdateTime = Date.now();
					currentQuestion.finalized = false;
					selectedQuestionId = CURRENT_QUESTION_ID;
					renderCurrentQuestion();
				}
			} catch (err) {
				console.warn('‚ö†Ô∏è falha ao atualizar UI com transcri√ß√£o parcial:', err);
			}

			// verifica se a pergunta est√° "pronta" (heur√≠stica)
			if (isQuestionReady(outputPartialText)) {
				console.log('‚ùì Pergunta detectada (parcial):', outputPartialText);

				// limpa texto parcial acumulado
				const newText = outputPartialText.trim();

				// verifica se o novo texto √© igual ao texto atual da pergunta, se sim, ignora
				if (newText === currentQuestion.text) {
					// üü° No modo entrevista, se a pergunta ainda N√ÉO foi fechada,
					// permitimos seguir para fechamento e chamada do GPT
					if (!currentQuestion.finalized) {
						console.log('üü° Pergunta repetida, mas v√°lida no modo entrevista ‚Äî permitindo fechamento');
					} else {
						console.log('üîï Ignorando nova transcri√ß√£o igual √† currentQuestion');
						return;
					}
				}

				// se currentQuestion ainda n√£o tinha texto, marca como um novo turno
				if (!currentQuestion.text) {
					currentQuestion.createdAt = Date.now();
					interviewTurnId++; // novo turno detectado
					console.log('üÜï Novo turno iniciado:', interviewTurnId);
				}

				// atualiza a pergunta atual com o novo texto parcial
				currentQuestion.text = newText;
				// atualiza timestamp de √∫ltima modifica√ß√£o
				currentQuestion.lastUpdate = Date.now();
				currentQuestion.lastUpdateTime = Date.now();
				// marca como n√£o finalizada
				currentQuestion.finalized = false;

				// atualiza UI
				selectedQuestionId = CURRENT_QUESTION_ID;
				renderCurrentQuestion();

				console.log('üß† currentQuestion (parcial):', currentQuestion.text);
				console.log('üéØ interviewTurnId:', interviewTurnId);
				console.log('ü§ñ gptAnsweredTurnId:', gptAnsweredTurnId);

				// reseta o timer de auto fechamento
				if (autoCloseQuestionTimer) {
					clearTimeout(autoCloseQuestionTimer);
				}

				// ‚è±Ô∏è agenda timer para auto fechamento da pergunta ap√≥s per√≠odo ocioso
				autoCloseQuestionTimer = setTimeout(() => {
					console.log('‚è±Ô∏è Auto close question disparado (timeout)');

					if (
						ModeController.isInterviewMode() &&
						currentQuestion.text &&
						!currentQuestion.finalized &&
						gptAnsweredTurnId !== interviewTurnId
					) {
						// fecha a pergunta atual automaticamente
						closeCurrentQuestion();
					}
				}, QUESTION_IDLE_TIMEOUT);

				console.log('‚è≤Ô∏è Timer de auto-fechamento agendado para', QUESTION_IDLE_TIMEOUT, 'ms');
			} else {
				console.log('‚è≥ Aguardando mais texto para formar pergunta completa');
			}
		} catch (err) {
			console.error('‚ùå Erro na transcri√ß√£o parcial (OUTPUT):', err);
		}
	}, 300); // Janela de 300ms para m√°xima responsividade - envia ~5-8 chunks a cada 3s (rate-limit)

	debugLogRenderer('Fim da fun√ß√£o: "transcribeOutputPartial"');
}

/* ===============================
   MODO NORMAL - TRANSCRI√á√ÉO
=============================== */

async function transcribeInput() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "transcribeInput"');
	if (!inputChunks.length) return;

	const blob = new Blob(inputChunks, { type: 'audio/webm' });
	console.log('üîÅ transcrever entrada - blob.size:', blob.size); // diagn√≥stico

	// ignora ru√≠do / respira√ß√£o
	const minSize = ModeController.isInterviewMode() ? MIN_INPUT_AUDIO_SIZE_INTERVIEW : MIN_INPUT_AUDIO_SIZE;

	if (blob.size < minSize) return;

	inputChunks = [];

	// medir tempo de convers√£o blob -> buffer
	const tBlobToBuffer = Date.now();
	const buffer = Buffer.from(await blob.arrayBuffer());
	console.log('timing: bufferConv', Date.now() - tBlobToBuffer, 'ms, size', buffer.length);

	// medir tempo IPC + STT (roundtrip)
	const tSend = Date.now();
	const text = (await transcribeAudio(blob))?.trim();
	console.log('timing: ipc_stt_roundtrip', Date.now() - tSend, 'ms');
	if (!text || isGarbageSentence(text)) return;

	// Se existia um placeholder (timestamp do stop), calcula m√©tricas e emite evento para atualizar
	if (lastInputPlaceholderEl && lastInputPlaceholderEl.dataset) {
		const stop = lastInputPlaceholderEl.dataset.stopAt
			? Number(lastInputPlaceholderEl.dataset.stopAt)
			: lastInputStopAt;
		const start = lastInputPlaceholderEl.dataset.startAt
			? Number(lastInputPlaceholderEl.dataset.startAt)
			: lastInputStartAt || stop;
		const now = Date.now();
		const recordingDuration = stop - start;
		const latency = now - stop;
		const total = now - start;
		const startStr = new Date(start).toLocaleTimeString();
		const stopStr = new Date(stop).toLocaleTimeString();
		const displayStr = new Date(now).toLocaleTimeString();

		// Log detalhado de timing
		console.log('‚è±Ô∏è TIMING COMPLETO:');
		console.log(`  ‚úÖ In√≠cio: ${startStr}`);
		console.log(`  ‚èπÔ∏è Parada: ${stopStr}`);
		console.log(`  üì∫ Exibi√ß√£o: ${displayStr}`);
		console.log(`  üìä Dura√ß√£o grava√ß√£o: ${recordingDuration}ms | Lat√™ncia: ${latency}ms | Total: ${total}ms`);

		// Emite para config-manager atualizar o placeholder com texto final e m√©tricas
		emitUIChange('onPlaceholderFulfill', {
			speaker: YOU,
			text,
			stopStr,
			startStr,
			recordingDuration,
			latency,
			total,
		});

		lastInputPlaceholderEl = null;
		lastInputStopAt = null;
		lastInputStartAt = null;
	} else {
		addTranscript(YOU, text);
	}

	handleSpeech(YOU, text);

	debugLogRenderer('Fim da fun√ß√£o: "transcribeInput"');
}

async function transcribeOutput() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "transcribeOutput"');

	// Desabilitado temporariamente (teste)
	if (DESABILITADO_TEMPORARIAMENTE) {
		debugLogRenderer('Fim da fun√ß√£o: "transcribeOutput" üîí DESABILITADO TEMPORARIAMENTE');
		return;
	}

	// Se n√£o houver chunks de sa√≠da, retorna
	if (!outputChunks.length) {
		console.log('‚ö†Ô∏è transcribeOutput: nenhum chunk de sa√≠da dispon√≠vel');

		debugLogRenderer('Fim da fun√ß√£o: "transcribeOutput"');
		return;
	}

	// Cria um blob a partir dos chunks de sa√≠da
	const blob = new Blob(outputChunks, { type: 'audio/webm' });
	console.log('üéµ transcribeOutput: blob.size =', blob.size, 'bytes | chunks =', outputChunks.length);

	// Valida tamanho m√≠nimo dependendo do modo (evita ru√≠do / respira√ß√£o)
	const minSize = ModeController.isInterviewMode() ? MIN_OUTPUT_AUDIO_SIZE_INTERVIEW : MIN_OUTPUT_AUDIO_SIZE;
	if (blob.size < minSize) {
		console.log('‚ö†Ô∏è transcribeOutput: Blob muito pequeno (', blob.size, '/', minSize, ') - ignorando');

		debugLogRenderer('Fim da fun√ß√£o: "transcribeOutput"');
		return;
	}

	// Limpa o array de chunks de sa√≠da
	outputChunks = [];

	try {
		// Envia para transcri√ß√£o o blob de sa√≠da
		const text = await transcribeAudio(blob);
		console.log('üìù transcribeOutput: Transcri√ß√£o recebida: ', text);

		// Ignora transcri√ß√£o vazia
		if (!text || text.trim().length === 0) {
			console.log('‚ö†Ô∏è transcribeOutput: Transcri√ß√£o vazia - ignorando');
			return;
		}

		// Ignora senten√ßas garbage
		if (isGarbageSentence(text)) {
			console.log('üóëÔ∏è transcribeOutput: Senten√ßa descartada (garbage):', text);
			return;
		}

		// Se existia um placeholder (timestamp do stop), atualiza esse placeholder com o texto final e lat√™ncia
		if (lastOutputPlaceholderEl && lastOutputPlaceholderEl.dataset) {
			console.log('üîÑ Atualizando placeholder com transcri√ß√£o final...');

			// obt√©m os timestamps de stop do dataset do placeholder, ou usa os valores globais
			const stop = lastOutputPlaceholderEl.dataset.stopAt
				? Number(lastOutputPlaceholderEl.dataset.stopAt)
				: lastOutputStopAt;

			// obt√©m os timestamps de start do dataset do placeholder, ou usa os valores globais
			const start = lastOutputPlaceholderEl.dataset.startAt
				? Number(lastOutputPlaceholderEl.dataset.startAt)
				: lastOutputStartAt || stop;

			// calcula m√©tricas
			const now = Date.now();
			const recordingDuration = stop - start;
			const latency = now - stop;
			const total = now - start;
			const startStr = new Date(start).toLocaleTimeString();
			const stopStr = new Date(stop).toLocaleTimeString();
			const displayStr = new Date(now).toLocaleTimeString();

			// Log detalhado de timing
			console.log('‚è±Ô∏è TIMING COMPLETO (Output):');
			console.log(`  ‚úÖ In√≠cio: ${startStr}`);
			console.log(`  ‚èπÔ∏è Parada: ${stopStr}`);
			console.log(`  üì∫ Exibi√ß√£o: ${displayStr}`);
			console.log(`  üìä Dura√ß√£o grava√ß√£o: ${recordingDuration}ms | Lat√™ncia: ${latency}ms | Total: ${total}ms`);

			// Emite atualiza√ß√£o de UI ao placeholder com texto final e m√©tricas
			emitUIChange('onPlaceholderFulfill', {
				speaker: OTHER,
				text,
				stopStr,
				startStr,
				recordingDuration,
				latency,
				total,
			});

			// reseta vari√°veis de placeholder
			lastOutputPlaceholderEl = null;
			lastOutputStopAt = null;
			lastOutputStartAt = null;

			// processa a fala transcrita (consolida√ß√£o de perguntas)
			// Usa Date.now() para pegar o tempo exato que chegou no renderer
			handleSpeech(OTHER, text);
		} else {
			addTranscript(OTHER, text);

			// Sem placeholder - cria placeholder e emite fulfill para garantir m√©tricas
			console.log('‚ûï Nenhum placeholder existente - criando e preenchendo com m√©tricas');
			// obt√©m timestamps de fallback
			const stop = lastOutputStopAt || Date.now();
			const start = lastOutputStartAt || stop;
			const now = Date.now();
			const recordingDuration = stop - start;
			const latency = now - stop;
			const total = now - start;
			const startStr = new Date(start).toLocaleTimeString();
			const stopStr = new Date(stop).toLocaleTimeString();
			const displayStr = new Date(now).toLocaleTimeString();

			// Log detalhado de timing
			console.log('‚è±Ô∏è TIMING COMPLETO (Output):');
			console.log(`  ‚úÖ In√≠cio: ${startStr}`);
			console.log(`  ‚èπÔ∏è Parada: ${stopStr}`);
			console.log(`  üì∫ Exibi√ß√£o: ${displayStr}`);
			console.log(`  üìä Dura√ß√£o grava√ß√£o: ${recordingDuration}ms | Lat√™ncia: ${latency}ms | Total: ${total}ms`);

			// cria um placeholder vis√≠vel antes de preencher (garante consist√™ncia com fluxo parcial)
			const placeholderEl = addTranscript(OTHER, '...', start);

			if (placeholderEl && placeholderEl.dataset) {
				placeholderEl.dataset.startAt = start;
				placeholderEl.dataset.stopAt = stop;
			}

			// Emite atualiza√ß√£o final para preencher o placeholder com texto e m√©tricas
			emitUIChange('onPlaceholderFulfill', {
				speaker: OTHER,
				text,
				stopStr,
				startStr,
				recordingDuration,
				latency,
				total,
			});

			// reseta vari√°veis de placeholder
			lastOutputPlaceholderEl = null;
			lastOutputStopAt = null;
			lastOutputStartAt = null;

			// processa a fala transcrita (consolida√ß√£o de perguntas)
			// Usa Date.now() para pegar o tempo exato que chegou no renderer
			handleSpeech(OTHER, text);
		}

		// MODO ENTREVISTA: Se a transcri√ß√£o final indicar claramente uma pergunta, fechar e enviar ao GPT imediatamente
		// if (ModeController.isInterviewMode() && isQuestionReady(text)) {
		// 	console.log('üîî transcribeOutput: Transcri√ß√£o final forma pergunta v√°lida');
		// 	console.log('   ‚Üí Fechando pergunta e chamando GPT agora');

		// 	// limpa estado parcial e cancela o temporizador autom√°tico para evitar duplicatas
		// 	outputPartialText = '';

		// 	// cancela o temporizador autom√°tico para evitar duplicatas
		// 	if (autoCloseQuestionTimer) {
		// 		clearTimeout(autoCloseQuestionTimer);
		// 		autoCloseQuestionTimer = null;
		// 		console.log('   ‚Üí Timer autom√°tico cancelado');
		// 	}

		// 	// fecha a pergunta atual imediatamente
		// 	closeCurrentQuestion();
		// }
	} catch (err) {
		console.warn('‚ö†Ô∏è erro na transcri√ß√£o (OUTPUT)', err);
	}

	debugLogRenderer('Fim da fun√ß√£o: "transcribeOutput"');
}

/* ===============================
   CONSOLIDA√á√ÉO DE PERGUNTAS
=============================== */

function handleSpeech(author, text) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "handleSpeech"');
	const cleaned = text.replace(/√ä+|hum|ahn/gi, '').trim();
	console.log('üîä handleSpeech', { author, raw: text, cleaned });
	if (cleaned.length < 3) return;

	// Usa o tempo exato que chegou no renderer (Date.now)
	const now = Date.now();

	if (author === OTHER) {
		// üëâ Se j√° existe uma pergunta finalizada,
		//    significa que uma NOVA pergunta come√ßou
		if (currentQuestion.finalized) {
			console.log(
				'‚ÑπÔ∏è Quest√£o anterior finalizada ‚Äî promovendo para a hist√≥ria e continuando a processar o novo discurso.',
			);
			promoteCurrentToHistory(currentQuestion.text);
		}

		// üß† Detecta in√≠cio de NOVA pergunta e fecha a anterior
		if (
			currentQuestion.text &&
			looksLikeQuestion(cleaned) &&
			now - currentQuestion.lastUpdate > 500 &&
			!currentQuestion.finalized
		) {
			closeCurrentQuestion();
		}

		if (!currentQuestion.text) {
			// evita criar novo turno se a transcri√ß√£o final for igual √† √∫ltima pergunta j√° enviada
			if (lastSentQuestionText && cleaned.trim() === lastSentQuestionText) {
				console.log('üîï transcri√ß√£o igual √† √∫ltima pergunta enviada ‚Äî ignorando novo turno');
				return;
			}
			currentQuestion.createdAt = Date.now();
			currentQuestion.lastUpdateTime = Date.now();
			interviewTurnId++; // üî• novo turno
		}

		// evita duplica√ß√£o quando a mesma frase parcial/final chega novamente
		if (currentQuestion.text && normalizeForCompare(currentQuestion.text) === normalizeForCompare(cleaned)) {
			console.log('üîÅ speech igual ao currentQuestion ‚Äî ignorando concatena√ß√£o');
		} else {
			currentQuestion.text += (currentQuestion.text ? ' ' : '') + cleaned;
			currentQuestion.lastUpdateTime = now;
		}
		currentQuestion.lastUpdate = now;

		// üü¶ CURRENT vira sele√ß√£o padr√£o ao receber fala
		if (!selectedQuestionId) {
			selectedQuestionId = CURRENT_QUESTION_ID;
			clearAllSelections();
		}

		renderCurrentQuestion();
	}

	debugLogRenderer('Fim da fun√ß√£o: "handleSpeech"');
}

/* ===============================
   FECHAMENTO DE PERGUNTAS
=============================== */

function closeCurrentQuestion() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "closeCurrentQuestion"');

	// üîí GUARDA ABSOLUTA:
	// Se a pergunta j√° foi finalizada, N√ÉO fa√ßa nada.
	if (currentQuestion.finalized) {
		console.log('‚õî closeCurrentQuestion ignorado ‚Äî pergunta j√° finalizada');
		return;
	}

	// Garante que lastUpdateTime seja definido quando se tenta fechar
	if (!currentQuestion.lastUpdateTime && currentQuestion.text) {
		currentQuestion.lastUpdateTime = Date.now();
	}

	console.log('üö™ closeCurrentQuestion called', {
		interviewTurnId,
		gptAnsweredTurnId,
		currentQuestionText: currentQuestion.text,
	});

	// trata perguntas incompletas
	if (isIncompleteQuestion(currentQuestion.text)) {
		console.log('‚ö†Ô∏è pergunta incompleta detectada ‚Äî promovendo ao hist√≥rico como incompleta:', currentQuestion.text);

		const newId = crypto.randomUUID();
		questionsHistory.push({
			id: newId,
			text: currentQuestion.text,
			createdAt: currentQuestion.createdAt || Date.now(),
			lastUpdateTime: currentQuestion.lastUpdateTime || currentQuestion.createdAt || Date.now(),
			incomplete: true,
		});

		selectedQuestionId = newId;

		currentQuestion.text = '';
		currentQuestion.lastUpdateTime = null;
		currentQuestion.createdAt = null;
		currentQuestion.finalized = false;

		renderQuestionsHistory();
		renderCurrentQuestion();
		return;
	}

	if (!looksLikeQuestion(currentQuestion.text)) {
		// ‚ö†Ô∏è No modo entrevista, N√ÉO abortar o fechamento
		if (ModeController.isInterviewMode()) {
			console.log('‚ö†Ô∏è looksLikeQuestion=false, mas modo entrevista ativo ‚Äî for√ßando fechamento');

			currentQuestion.text = finalizeQuestion(currentQuestion.text);
			currentQuestion.lastUpdateTime = Date.now();
			currentQuestion.finalized = true;

			// garante sele√ß√£o l√≥gica
			selectedQuestionId = CURRENT_QUESTION_ID;

			// chama GPT automaticamente se ainda n√£o respondeu este turno
			if (gptRequestedTurnId !== interviewTurnId && gptAnsweredTurnId !== interviewTurnId) {
				console.log('‚û°Ô∏è closeCurrentQuestion (fallback) chamou askGpt', {
					interviewTurnId,
					gptRequestedTurnId,
					gptAnsweredTurnId,
				});

				console.error('closeCurrentQuestion: askGpt() 2017; üîí COMENTADA at√© transcri√ß√£o em tempo real funcionar');
				// askGpt(); // üîí COMENTADA at√© transcri√ß√£o em tempo real funcionar
			}

			return;
		}

		// modo normal mant√©m comportamento atual
		currentQuestion.text = '';
		currentQuestion.lastUpdateTime = null;
		currentQuestion.createdAt = null;
		currentQuestion.finalized = false;
		renderCurrentQuestion();
		return;
	}

	// ‚úÖ consolida a pergunta
	currentQuestion.text = finalizeQuestion(currentQuestion.text);
	currentQuestion.lastUpdateTime = Date.now();
	currentQuestion.finalized = true;

	// ‚ö†Ô∏è NUNCA renderizar aqui no modo entrevista
	if (!ModeController.isInterviewMode()) {
		renderCurrentQuestion();
	}

	// üî• COMPORTAMENTO POR MODO
	if (ModeController.isInterviewMode()) {
		if (gptRequestedTurnId !== interviewTurnId && gptAnsweredTurnId !== interviewTurnId) {
			selectedQuestionId = CURRENT_QUESTION_ID;

			console.log('‚û°Ô∏è closeCurrentQuestion chamou askGpt (vou enviar para o GPT)', {
				interviewTurnId,
				gptRequestedTurnId,
				gptAnsweredTurnId,
			});

			console.error('closeCurrentQuestion: askGpt() 2054; üîí COMENTADA at√© transcri√ß√£o em tempo real funcionar');

			// askGpt(); // üîí COMENTADA at√© transcri√ß√£o em tempo real funcionar
		}
	} else {
		console.log('üîµ modo NORMAL ‚Äî promovendo CURRENT para hist√≥rico sem chamar GPT');

		promoteCurrentToHistory(currentQuestion.text);

		currentQuestion.text = '';
		currentQuestion.lastUpdateTime = null;
		currentQuestion.createdAt = null;
		currentQuestion.finalized = false;

		renderCurrentQuestion();
	}

	debugLogRenderer('Fim da fun√ß√£o: "closeCurrentQuestion"');
}

function closeCurrentQuestionForced() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "closeCurrentQuestionForced"');

	// log temporario para testar a aplica√ß√£o s√≥ remover depois
	console.log('üö™ Fechando pergunta:', currentQuestion.text);

	resetInterviewTurnState();

	if (!currentQuestion.text) return;

	questionsHistory.push({
		id: crypto.randomUUID(),
		text: finalizeQuestion(currentQuestion.text),
		createdAt: currentQuestion.createdAt || Date.now(),
	});

	currentQuestion.text = '';
	selectedQuestionId = null; // üëà libera sele√ß√£o
	renderQuestionsHistory();
	renderCurrentQuestion();

	debugLogRenderer('Fim da fun√ß√£o: "closeCurrentQuestionForced"');
}

function resetInterviewTurnState() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "resetInterviewTurnState"');

	outputPartialText = '';
	outputPartialChunks = [];

	// limpa fingerprint de pergunta enviada para evitar bloqueios indevidos
	lastAskedQuestionNormalized = null;

	debugLogRenderer('Fim da fun√ß√£o: "resetInterviewTurnState"');
}

/* ===============================
   VALIDA√á√ÉO DE API KEY
=============================== */

// üî• Verifica o Status da API
async function checkApiKeyStatus() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "checkApiKeyStatus"');
	try {
		const status = await ipcRenderer.invoke('GET_OPENAI_API_STATUS');
		console.log('üîë Status da API key:', status);

		debugLogRenderer('Fim da fun√ß√£o: "checkApiKeyStatus"');
		return status;
	} catch (error) {
		console.warn('‚ö†Ô∏è N√£o foi poss√≠vel verificar status da API:', error);
		return { initialized: false, hasKey: false };
	}
}

/* ===============================
   GPT
=============================== */
async function askGpt() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "askGpt"');

	// Desabilitado temporariamente (teste)
	if (DESABILITADO_TEMPORARIAMENTE) {
		debugLogRenderer('Fim da fun√ß√£o: "askGpt" üîí DESABILITADO TEMPORARIAMENTE');
		return;
	}

	const text = getSelectedQuestionText();

	if (!text || text.trim().length < 5) {
		updateStatusMessage('‚ö†Ô∏è Pergunta vazia ou incompleta');
		return;
	}

	const isCurrent = selectedQuestionId === CURRENT_QUESTION_ID;
	const normalizedText = normalizeForCompare(text);

	// Evita reenvio da mesma pergunta atual ao GPT (dedupe)
	if (isCurrent && normalizedText && lastAskedQuestionNormalized === normalizedText) {
		updateStatusMessage('‚õî Pergunta j√° enviada');
		console.log('‚õî askGpt: mesma pergunta j√° enviada, pulando');
		return;
	}
	const questionId = isCurrent ? CURRENT_QUESTION_ID : selectedQuestionId;

	// üõ°Ô∏è MODO ENTREVISTA ‚Äî bloqueia duplica√ß√£o APENAS para hist√≥rico
	if (ModeController.isInterviewMode() && !isCurrent) {
		const existingAnswer = findAnswerByQuestionId(questionId);
		if (existingAnswer) {
			emitUIChange('onAnswerAdd', {
				questionId,
				action: 'showExisting',
			});
			updateStatusMessage('üìå Essa pergunta j√° foi respondida');
			return;
		}
	}

	// limpa destaque
	emitUIChange('onAnswerAdd', {
		questionId,
		action: 'clearActive',
	});

	// log temporario para testar a aplica√ß√£o s√≥ remover depois
	console.log('ü§ñ askGpt chamado | questionId:', selectedQuestionId);
	console.log('üß™ GPT RECEBERIA:', text);

	console.log('üßæ askGpt diagn√≥stico', {
		textLength: text.length,
		selectedQuestionId,
		isInterviewMode: ModeController.isInterviewMode(),
		interviewTurnId,
		gptAnsweredTurnId,
	});

	// marca que este turno teve uma requisi√ß√£o ao GPT (apenas para CURRENT)
	if (isCurrent) {
		gptRequestedTurnId = interviewTurnId;
		lastAskedQuestionNormalized = normalizedText;
		console.log('‚ÑπÔ∏è gptRequestedTurnId definido para turno', gptRequestedTurnId);
		lastSentQuestionText = text.trim();
		console.log('‚ÑπÔ∏è lastSentQuestionText definido:', lastSentQuestionText);
	}

	// Inicia medi√ß√£o do GPT
	transcriptionMetrics.gptStartTime = Date.now();

	// üß™ DEBUG
	if (APP_CONFIG.MODE_DEBUG) {
		updateStatusMessage('üß™ Pergunta enviada ao GPT (modo teste)');

		const mock = getMockGptAnswer(text);
		renderGptAnswer(null, mock);

		if (isCurrent && gptRequestedTurnId === interviewTurnId) {
			promoteCurrentToHistory(text);
			resetInterviewTurnState();
		}

		// marca como respondido nesse turno (mock)
		gptAnsweredTurnId = interviewTurnId;
		gptRequestedTurnId = null;

		// Finaliza medi√ß√µes
		transcriptionMetrics.gptEndTime = Date.now();
		transcriptionMetrics.totalTime = Date.now() - transcriptionMetrics.audioStartTime;

		// Log m√©tricas
		logTranscriptionMetrics();

		return;
	}

	// üß† MODO ENTREVISTA ‚Äî STREAMING
	if (ModeController.isInterviewMode()) {
		const gptStartAt = ENABLE_INTERVIEW_TIMING_DEBUG ? Date.now() : null;
		let streamedText = '';

		console.log('‚è≥ enviando para o GPT via stream...');
		ipcRenderer
			.invoke('ask-gpt-stream', [
				{ role: 'system', content: SYSTEM_PROMPT },
				{ role: 'user', content: text },
			])
			.catch(err => {
				console.error('‚ùå Erro ao chamar ask-gpt-stream:', err);
				updateStatusMessage('‚ùå Erro ao enviar para GPT');
			});

		const onChunk = (_, token) => {
			streamedText += token;
			emitUIChange('onAnswerStreamChunk', {
				questionId,
				token,
				accum: streamedText,
			});
			console.log('üü¢ GPT_STREAM_CHUNK recebido (token parcial)', token);
		};

		const onEnd = () => {
			console.log('‚úÖ GPT_STREAM_END recebido (stream finalizado)');
			ipcRenderer.removeListener('GPT_STREAM_CHUNK', onChunk);
			ipcRenderer.removeListener('GPT_STREAM_END', onEnd);

			// Finaliza medi√ß√µes
			transcriptionMetrics.gptEndTime = Date.now();
			transcriptionMetrics.totalTime = Date.now() - transcriptionMetrics.audioStartTime;

			// Log m√©tricas
			logTranscriptionMetrics();

			let finalText = streamedText;
			if (ENABLE_INTERVIEW_TIMING_DEBUG && gptStartAt) {
				const endAt = Date.now();
				const elapsed = endAt - gptStartAt;

				const startTime = new Date(gptStartAt).toLocaleTimeString();
				const endTime = new Date(endAt).toLocaleTimeString();

				finalText +=
					`\n\n‚è±Ô∏è GPT iniciou: ${startTime}` + `\n‚è±Ô∏è GPT finalizou: ${endTime}` + `\n‚è±Ô∏è Resposta em ${elapsed}ms`;
			}

			// garante que o turno foi realmente fechado
			const wasRequestedForThisTurn = gptRequestedTurnId === interviewTurnId;

			gptAnsweredTurnId = interviewTurnId;
			gptRequestedTurnId = null;

			// üîí FECHAMENTO AT√îMICO DO CICLO
			if (isCurrent && wasRequestedForThisTurn) {
				const finalHtml = marked.parse(finalText);

				// 1Ô∏è‚É£ promove a pergunta primeiro (gera ID definitivo)
				promoteCurrentToHistory(text);

				// 2Ô∏è‚É£ pega a pergunta rec√©m-promovida
				const promotedQuestion = questionsHistory[questionsHistory.length - 1];

				if (promotedQuestion) {
					// 3Ô∏è‚É£ cria a resposta j√° com o ID CORRETO
					renderGptAnswer(promotedQuestion.id, finalHtml);

					// 4Ô∏è‚É£ marca como respondida
					promotedQuestion.answered = true;
					renderQuestionsHistory();

					console.log('‚úÖ Pergunta respondida com ID definitivo:', promotedQuestion.id);
				} else {
					console.warn('‚ö†Ô∏è pergunta promovida n√£o encontrada');
				}

				resetInterviewTurnState();
			} else if (questionId !== CURRENT_QUESTION_ID) {
				const finalHtml = marked.parse(finalText);
				renderGptAnswer(questionId, finalHtml);

				// marca a pergunta como respondida no hist√≥rico (streaming path)
				try {
					const q = questionsHistory.find(x => x.id === questionId);
					if (q) {
						q.answered = true;
						renderQuestionsHistory();
					}
				} catch (err) {
					console.warn('‚ö†Ô∏è falha ao marcar pergunta como respondida (stream):', err);
				}
			}
		};

		ipcRenderer.on('GPT_STREAM_CHUNK', onChunk);
		ipcRenderer.once('GPT_STREAM_END', onEnd);
		return;
	}

	// üîµ MODO NORMAL ‚Äî BATCH
	console.log('‚è≥ enviando para o GPT (batch)...');
	const res = await ipcRenderer.invoke('ask-gpt', [
		{ role: 'system', content: SYSTEM_PROMPT },
		{ role: 'user', content: text },
	]);

	console.log('‚úÖ resposta do GPT recebida (batch)');

	// Finaliza medi√ß√µes
	transcriptionMetrics.gptEndTime = Date.now();
	transcriptionMetrics.totalTime = Date.now() - transcriptionMetrics.audioStartTime;

	// Log m√©tricas
	logTranscriptionMetrics();

	renderGptAnswer(questionId, res);

	const wasRequestedForThisTurn = gptRequestedTurnId === interviewTurnId;

	console.log(
		'‚ÑπÔ∏è gptRequestedTurnId antes do batch:',
		gptRequestedTurnId,
		'wasRequestedForThisTurn:',
		wasRequestedForThisTurn,
	);

	// üîí FECHAMENTO AT√îMICO DO CICLO
	if (isCurrent && wasRequestedForThisTurn) {
		promoteCurrentToHistory(text);
		// ap√≥s promover para o hist√≥rico, a pergunta j√° est√° no hist√≥rico e resposta vinculada
		try {
			// Encontra a √∫ltima pergunta adicionada (que acabamos de promover)
			const q = questionsHistory[questionsHistory.length - 1];
			if (q) {
				q.answered = true;
				renderQuestionsHistory();
			}
		} catch (err) {
			console.warn('‚ö†Ô∏è falha ao marcar pergunta como respondida (batch):', err);
		}
	}

	// marca que o GPT respondeu esse turno (batch)
	gptAnsweredTurnId = interviewTurnId;
	gptRequestedTurnId = null;

	debugLogRenderer('Fim da fun√ß√£o: "askGpt"');
}

/* ===============================
   UI (RENDER / SELE√á√ÉO / SCROLL)
=============================== */

function addTranscript(author, text, time) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "addTranscript"');
	let timeStr;
	if (time) {
		if (typeof time === 'number') timeStr = new Date(time).toLocaleTimeString();
		else if (time instanceof Date) timeStr = time.toLocaleTimeString();
		else timeStr = String(time);
	} else {
		timeStr = new Date().toLocaleTimeString();
	}

	// üî• Apenas EMITE o evento com os dados
	// config-manager.js √© respons√°vel por adicionar ao DOM
	const transcriptData = {
		author,
		text,
		timeStr,
		elementId: 'conversation',
	};

	emitUIChange('onTranscriptAdd', transcriptData);

	// Retorna um objeto proxy que simula um elemento DOM para compatibilidade
	// Usado quando a transcri√ß√£o √© um placeholder que ser√° atualizado depois
	const placeholderProxy = {
		dataset: {
			startAt: typeof time === 'number' ? time : Date.now(),
			stopAt: null,
		},
		// Permite que c√≥digo posterior trate como elemento DOM
		classList: {
			add: () => {},
			remove: () => {},
			contains: () => false,
			toggle: () => false,
		},
	};

	debugLogRenderer('Fim da fun√ß√£o: "addTranscript"');
	return placeholderProxy;
}

function renderCurrentQuestion() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "renderCurrentQuestion"');

	// Desabilitado temporariamente (teste)
	if (DESABILITADO_TEMPORARIAMENTE) {
		debugLogRenderer('Fim da fun√ß√£o: "renderCurrentQuestion" üîí DESABILITADO TEMPORARIAMENTE');
		return;
	}

	if (!currentQuestion.text) {
		emitUIChange('onCurrentQuestionUpdate', { text: '', isSelected: false });
		return;
	}

	let label = currentQuestion.text;

	if (ENABLE_INTERVIEW_TIMING_DEBUG && currentQuestion.lastUpdateTime) {
		const time = new Date(currentQuestion.lastUpdateTime).toLocaleTimeString();
		label = `‚è±Ô∏è ${time} ‚Äî ${label}`;
	}

	// üî• Apenas EMITE dados - config-manager aplica ao DOM
	const questionData = {
		text: label,
		isSelected: selectedQuestionId === CURRENT_QUESTION_ID,
		rawText: currentQuestion.text,
		createdAt: currentQuestion.createdAt,
		lastUpdateTime: currentQuestion.lastUpdateTime,
	};

	console.log(`üì§ renderCurrentQuestion: emitindo onCurrentQuestionUpdate`, {
		label,
		isSelected: selectedQuestionId === CURRENT_QUESTION_ID,
	});
	emitUIChange('onCurrentQuestionUpdate', questionData);

	debugLogRenderer('Fim da fun√ß√£o: "renderCurrentQuestion"');
}

function renderQuestionsHistory() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "renderQuestionsHistory"');

	// Desabilitado temporariamente (teste)
	if (DESABILITADO_TEMPORARIAMENTE) {
		debugLogRenderer('Fim da fun√ß√£o: "renderCurrentQuestion" üîí DESABILITADO TEMPORARIAMENTE');
		return;
	}

	// üî• Gera dados estruturados - config-manager renderiza no DOM
	const historyData = [...questionsHistory].reverse().map(q => {
		let label = q.text;
		if (ENABLE_INTERVIEW_TIMING_DEBUG && q.lastUpdateTime) {
			const time = new Date(q.lastUpdateTime).toLocaleTimeString();
			label = `‚è±Ô∏è ${time} ‚Äî ${label}`;
		}

		return {
			id: q.id,
			text: label,
			isIncomplete: q.incomplete,
			isAnswered: q.answered,
			isSelected: q.id === selectedQuestionId,
		};
	});

	emitUIChange('onQuestionsHistoryUpdate', historyData);

	scrollToSelectedQuestion();

	debugLogRenderer('Fim da fun√ß√£o: "renderQuestionsHistory"');
}

function clearAllSelections() {
	// Emite evento para o controller limpar as sele√ß√µes visuais
	emitUIChange('onClearAllSelections', {});
}

function scrollToSelectedQuestion() {
	emitUIChange('onScrollToQuestion', {
		questionId: selectedQuestionId,
	});
}

function getSelectedQuestionText() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "getSelectedQuestionText"');
	// 1Ô∏è‚É£ Se existe sele√ß√£o expl√≠cita
	if (selectedQuestionId === CURRENT_QUESTION_ID) {
		return currentQuestion.text;
	}

	if (selectedQuestionId) {
		const q = questionsHistory.find(q => q.id === selectedQuestionId);
		if (q?.text) return q.text;
	}

	// 2Ô∏è‚É£ Fallback: CURRENT (se tiver texto)
	if (currentQuestion.text && currentQuestion.text.trim().length > 0) {
		return currentQuestion.text;
	}

	debugLogRenderer('Fim da fun√ß√£o: "getSelectedQuestionText"');
	return '';
}

function renderGptAnswer(questionId, markdownText) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "renderGptAnswer"');

	// üî• Renderiza markdown e retorna HTML - config-manager aplica ao DOM
	const short = shortenAnswer(markdownText, 2);
	const html = marked.parse(short);

	// Encontra texto da pergunta no hist√≥rico ou na pergunta atual
	let questionText = '';
	if (questionId === CURRENT_QUESTION_ID) {
		questionText = currentQuestion?.text || '';
	} else {
		const q = questionsHistory.find(x => x.id === questionId);
		questionText = q?.text || '';
	}

	// üîí Marca pergunta como respondida na primeira resposta
	if (questionId) {
		answeredQuestions.add(questionId);
		console.log('‚úÖ Pergunta marcada como respondida:', questionId);
	}

	const answerData = {
		questionText,
		questionId,
		html,
	};

	emitUIChange('onAnswerAdd', answerData);

	// marca a pergunta como respondida no hist√≥rico (se vinculada)
	try {
		if (questionId && questionId !== CURRENT_QUESTION_ID) {
			const q = questionsHistory.find(x => x.id === questionId);
			if (q) {
				q.answered = true;
				renderQuestionsHistory();
			}
		}
	} catch (err) {
		console.warn('‚ö†Ô∏è falha ao marcar pergunta como respondida:', err);
	}

	debugLogRenderer('Fim da fun√ß√£o: "renderGptAnswer"');
}

function resetInterviewState() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "resetInterviewState"');
	currentQuestion = { text: '', lastUpdate: 0, finalized: false, lastUpdateTime: null, createdAt: null };
	questionsHistory = [];
	selectedQuestionId = null;

	// Emit eventos para limpar UI
	emitUIChange('onTranscriptionCleared', {});
	emitUIChange('onAnswersCleared', {});

	clearAllSelections();
	renderQuestionsHistory();
	renderCurrentQuestion();

	debugLogRenderer('Fim da fun√ß√£o: "resetInterviewState"');
}

// üî• NOVO: Verifica se existe um modelo de IA ativo e retorna o nome do modelo
function hasActiveModel() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "hasActiveModel"');
	if (!window.configManager) {
		console.warn('‚ö†Ô∏è ConfigManager n√£o inicializado ainda');
		return { active: false, model: null };
	}

	const config = window.configManager.config;
	if (!config || !config.api) {
		console.warn('‚ö†Ô∏è Config ou api n√£o dispon√≠vel');
		return { active: false, model: null };
	}

	// Verifica se algum modelo est√° ativo e retorna o nome
	const providers = ['openai', 'google', 'openrouter', 'custom'];
	for (const provider of providers) {
		if (config.api[provider] && config.api[provider].enabled === true) {
			console.log(`‚úÖ Modelo ativo encontrado: ${provider}`);
			return { active: true, model: provider };
		}
	}

	console.warn('‚ö†Ô∏è Nenhum modelo ativo encontrado');

	debugLogRenderer('Fim da fun√ß√£o: "hasActiveModel"');
	return { active: false, model: null };
}

async function listenToggleBtn() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "listenToggleBtn"');

	// üî• VALIDA√á√ÉO 1: Modelo de IA ativo
	const { active: hasModel, model: activeModel } = hasActiveModel();
	console.log(`üìä DEBUG: hasModel = ${hasModel}, activeModel = ${activeModel}`);

	if (!isRunning && !hasModel) {
		const errorMsg = 'Ative um modelo de IA antes de come√ßar a ouvir';
		console.warn(`‚ö†Ô∏è ${errorMsg}`);
		console.log('üì° DEBUG: Emitindo onError:', errorMsg);
		emitUIChange('onError', errorMsg);
		return;
	}

	// üî• VALIDA√á√ÉO 2: Dispositivo de √°udio de SA√çDA (obrigat√≥rio para ouvir a reuni√£o)
	const hasOutputDevice = UIElements.outputSelect?.value;
	console.log(`üìä DEBUG: hasOutputDevice = ${hasOutputDevice}`);

	if (!isRunning && !hasOutputDevice) {
		const errorMsg = 'Selecione um dispositivo de √°udio (output) para ouvir a reuni√£o';
		console.warn(`‚ö†Ô∏è ${errorMsg}`);
		console.log('üì° DEBUG: Emitindo onError:', errorMsg);
		emitUIChange('onError', errorMsg);
		return;
	}

	// Inverte o estado de isRunning
	isRunning = !isRunning;
	const buttonText = isRunning ? 'Parar a Escuta... (Ctrl+d)' : 'Come√ßar a Ouvir... (Ctrl+d)';
	const statusMsg = isRunning ? 'Status: ouvindo...' : 'Status: parado';

	// Emite o evento 'onListenButtonToggle' para atualizar o bot√£o de escuta
	emitUIChange('onListenButtonToggle', {
		isRunning,
		buttonText,
	});

	// Atualiza o status da escuta na tela
	updateStatusMessage(statusMsg);

	console.log(`üé§ Listen toggle: ${isRunning ? 'INICIANDO' : 'PARANDO'} (modelo: ${activeModel})`);

	// Inicia ou para a captura de √°udio
	await (isRunning ? startAudio() : stopAudio());

	debugLogRenderer('Fim da fun√ß√£o: "listenToggleBtn"');
}

function handleQuestionClick(questionId) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "handleQuestionClick"');
	selectedQuestionId = questionId;
	clearAllSelections();
	renderQuestionsHistory();
	renderCurrentQuestion();

	// ‚ö†Ô∏è CURRENT nunca bloqueia resposta
	if (questionId !== CURRENT_QUESTION_ID) {
		const existingAnswer = findAnswerByQuestionId(questionId);

		if (existingAnswer) {
			emitUIChange('onAnswerSelected', {
				answerId: questionId,
				shouldScroll: true,
			});

			updateStatusMessage('üìå Essa pergunta j√° foi respondida');
			return;
		}
	}

	// Se for uma pergunta do hist√≥rico marcada como incompleta, n√£o enviar automaticamente ao GPT
	if (questionId !== CURRENT_QUESTION_ID) {
		const q = questionsHistory.find(q => q.id === questionId);
		if (q && q.incomplete) {
			updateStatusMessage('‚ö†Ô∏è Pergunta incompleta ‚Äî pressione o bot√£o de responder para enviar ao GPT');
			console.log('‚ÑπÔ∏è pergunta incompleta selecionada ‚Äî aguarda envio manual:', q.text);
			return;
		}
	}

	if (
		ModeController.isInterviewMode() &&
		selectedQuestionId === CURRENT_QUESTION_ID &&
		gptAnsweredTurnId === interviewTurnId
	) {
		updateStatusMessage('‚õî GPT j√° respondeu esse turno');
		console.log('‚õî GPT j√° respondeu esse turno');
		return;
	}

	// ‚ùì Ainda n√£o respondida ‚Üí chama GPT (click ou atalho)
	console.error('closeCurrentQuestion: askGpt() 2714; üîí COMENTADA at√© transcri√ß√£o em tempo real funcionar');
	// askGpt(); // üîí COMENTADA at√© transcri√ß√£o em tempo real funcionar

	debugLogRenderer('Fim da fun√ß√£o: "handleQuestionClick"');
}

function applyOpacity(value) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "applyOpacity"');
	const appOpacity = parseFloat(value);

	// aplica opacidade no conte√∫do geral
	document.documentElement.style.setProperty('--app-opacity', appOpacity.toFixed(2));

	// topBar nunca abaixo de 0.75
	const topbarOpacity = Math.max(appOpacity, 0.75);
	document.documentElement.style.setProperty('--app-opacity-75', topbarOpacity.toFixed(2));

	localStorage.setItem('overlayOpacity', appOpacity);

	// logs tempor√°rios para debug
	console.log('üéöÔ∏è Opacity change | app:', value, '| topBar:', topbarOpacity);

	debugLogRenderer('Fim da fun√ß√£o: "applyOpacity"');
}

// üî• Novo: atualizar status sem tocar em DOM
function updateStatusMessage(message) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "updateStatusMessage"');
	emitUIChange('onStatusUpdate', { message });
	debugLogRenderer('Fim da fun√ß√£o: "updateStatusMessage"');
}

/* ===============================
   MOCK / DEBUG
=============================== */

function startMockInterview() {
	if (mockInterviewRunning) return;
	mockInterviewRunning = true;

	// üî• Emite atualiza√ß√£o do mock badge
	emitUIChange('onMockBadgeUpdate', { visible: true });

	const mockQuestions = [
		'O que √© JVM e para que serve',
		'Qual a diferen√ßa entre JDK e JRE',
		'Explique o que √© Garbage Collector',
		'Como funciona o equals e hashCode',
		'O que √© imutabilidade em Java',
		'Explique o que √© POO',
		'Quais s√£o os pilares da POO',
		'Qual a diferen√ßa entre Spring e Spring Boot',
	];

	let index = 0;

	function sendNext() {
		if (!APP_CONFIG.MODE_DEBUG || index >= mockQuestions.length) {
			mockInterviewRunning = false;
			return;
		}

		const text = mockQuestions[index];
		addTranscript(OTHER, text); // üëà simula fala real
		handleSpeech(OTHER, text); // üëà consolida pergunta

		index++;

		// simula sil√™ncio ‚Üí fechamento da pergunta
		setTimeout(() => {
			closeCurrentQuestion();
		}, QUESTION_IDLE_TIMEOUT);

		// pr√≥xima pergunta depois de um tempo
		setTimeout(sendNext, 6000);
	}

	sendNext();
}

function getMockGptAnswer(question) {
	return `
### ‚úîÔ∏è Resposta simulada

Voc√™ perguntou:

> ${question}

Essa √© uma resposta mock apenas para validar:
- fluxo
- scroll
- sele√ß√£o
- ritmo de uso

`;
}

/* ===============================
   BOOT
=============================== */

marked.setOptions({
	highlight: function (code, lang) {
		if (lang && hljs.getLanguage(lang)) {
			return hljs.highlight(code, { language: lang }).value;
		}
		return hljs.highlightAuto(code).value;
	},
	breaks: true,
});

// Exporta fun√ß√µes p√∫blicas que o controller pode chamar
const RendererAPI = {
	// √Åudio - Grava√ß√£o
	startInput,
	stopInput: stopInputMonitor,
	startOutput,
	stopOutput: stopOutputMonitor,
	restartAudioPipeline,

	// √Åudio - Monitoramento de volume
	startInputVolumeMonitoring,
	startOutputVolumeMonitoring,
	stopInputVolumeMonitoring,
	stopOutputVolumeMonitoring,

	// Entrevista
	listenToggleBtn,
	askGpt,
	resetInterviewState,
	startMockInterview,

	// Modo
	changeMode: mode => {
		CURRENT_MODE = mode;
	},
	getMode: () => CURRENT_MODE,

	// Questions
	handleQuestionClick,
	closeCurrentQuestion,

	// UI
	applyOpacity,
	updateMockBadge: show => {
		emitUIChange('onMockBadgeUpdate', { visible: show });
	},
	setMockToggle: checked => {
		if (UIElements.mockToggle) {
			UIElements.mockToggle.checked = checked;
		}
		APP_CONFIG.MODE_DEBUG = checked;
	},
	setModeSelect: mode => {
		emitUIChange('onModeSelectUpdate', { mode });
	},

	// Drag
	initDragHandle: (dragHandle, documentElement) => {
		if (!dragHandle) return;
		const doc = documentElement || document; // fallback para document global
		dragHandle.addEventListener('pointerdown', async event => {
			console.log('ü™ü Drag iniciado (pointerdown)');
			isDraggingWindow = true;
			dragHandle.classList.add('drag-active');

			const _pid = event.pointerId;
			try {
				dragHandle.setPointerCapture && dragHandle.setPointerCapture(_pid);
			} catch (err) {
				console.warn('setPointerCapture falhou:', err);
			}

			setTimeout(() => ipcRenderer.send('START_WINDOW_DRAG'), 40);

			const startBounds = (await ipcRenderer.invoke('GET_WINDOW_BOUNDS')) || {
				x: 0,
				y: 0,
			};
			const startCursor = { x: event.screenX, y: event.screenY };
			let lastAnimation = 0;

			function onPointerMove(ev) {
				const now = performance.now();
				if (now - lastAnimation < 16) return;
				lastAnimation = now;

				const dx = ev.screenX - startCursor.x;
				const dy = ev.screenY - startCursor.y;

				ipcRenderer.send('MOVE_WINDOW_TO', {
					x: startBounds.x + dx,
					y: startBounds.y + dy,
				});
			}

			function onPointerUp(ev) {
				try {
					dragHandle.removeEventListener('pointermove', onPointerMove);
					dragHandle.removeEventListener('pointerup', onPointerUp);
				} catch (err) {}

				if (dragHandle.classList.contains('drag-active')) {
					dragHandle.classList.remove('drag-active');
				}

				try {
					dragHandle.releasePointerCapture && dragHandle.releasePointerCapture(_pid);
				} catch (err) {}

				isDraggingWindow = false;
			}

			dragHandle.addEventListener('pointermove', onPointerMove);
			dragHandle.addEventListener('pointerup', onPointerUp, { once: true });
			event.stopPropagation();
		});

		doc.addEventListener('pointerup', () => {
			if (!dragHandle.classList.contains('drag-active')) return;
			console.log('ü™ü Drag finalizado (pointerup)');
			dragHandle.classList.remove('drag-active');
			isDraggingWindow = false;
		});

		dragHandle.addEventListener('pointercancel', () => {
			if (dragHandle.classList.contains('drag-active')) {
				dragHandle.classList.remove('drag-active');
				isDraggingWindow = false;
			}
		});
	},

	// Click-through
	setClickThrough: enabled => {
		ipcRenderer.send('SET_CLICK_THROUGH', enabled);
	},
	updateClickThroughButton: (enabled, btnToggle) => {
		if (!btnToggle) return;
		btnToggle.style.opacity = enabled ? '0.5' : '1';
		btnToggle.title = enabled
			? 'Click-through ATIVO (clique para desativar)'
			: 'Click-through INATIVO (clique para ativar)';
		console.log('üé® Bot√£o atualizado - opacity:', btnToggle.style.opacity);
	},

	// UI Registration
	registerUIElements: elements => {
		registerUIElements(elements);
	},
	onUIChange: (eventName, callback) => {
		onUIChange(eventName, callback);
	},

	// API Key
	setAppConfig: config => {
		APP_CONFIG = config;
	},
	getAppConfig: () => APP_CONFIG,

	// Keyboard shortcuts
	registerKeyboardShortcuts: () => {
		window.addEventListener(
			'keydown',
			e => {
				if (e.ctrlKey && e.shiftKey && (e.code === 'ArrowUp' || e.code === 'ArrowDown')) {
					e.preventDefault();
					e.stopPropagation();

					const all = getNavigableQuestionIds();
					if (all.length === 0) return;

					let index = all.indexOf(selectedQuestionId);
					if (index === -1) {
						index = e.key === 'ArrowUp' ? all.length - 1 : 0;
					} else {
						index += e.key === 'ArrowUp' ? -1 : 1;
						index = Math.max(0, Math.min(index, all.length - 1));
					}

					selectedQuestionId = all[index];
					clearAllSelections();
					renderQuestionsHistory();
					renderCurrentQuestion();

					if (APP_CONFIG.MODE_DEBUG) {
						const msg =
							e.key === 'ArrowUp' ? 'üß™ Ctrl+ArrowUp detectado (teste)' : 'üß™ Ctrl+ArrowDown detectado (teste)';
						updateStatusMessage(msg);
						console.log('üìå Atalho Selecionou:', selectedQuestionId);
						return;
					}
				}
			},
			true,
		);
	},

	// IPC Listeners
	onApiKeyUpdated: callback => {
		ipcRenderer.on('API_KEY_UPDATED', callback);
	},
	onToggleAudio: callback => {
		ipcRenderer.on('CMD_TOGGLE_AUDIO', callback);
	},
	onAskGpt: callback => {
		ipcRenderer.on('CMD_ASK_GPT', callback);
	},
	onGptStreamChunk: callback => {
		ipcRenderer.on('GPT_STREAM_CHUNK', callback);
	},
	onGptStreamEnd: callback => {
		ipcRenderer.on('GPT_STREAM_END', callback);
	},
	sendRendererError: error => {
		try {
			console.error('RENDERER ERROR', error.error || error.message || error);
			ipcRenderer.send('RENDERER_ERROR', {
				message: String(error.message || error),
				stack: error.error?.stack || null,
			});
		} catch (err) {
			console.error('Falha ao enviar RENDERER_ERROR', err);
		}
	},

	///////////////////////////////////
	// FUN√á√ïES PARA WHISPER LOCAL
	///////////////////////////////////
	setTranscriptionMode: useLocal => {
		setTranscriptionMode(useLocal);
	},

	getTranscriptionMode: () => USE_LOCAL_WHISPER,
};

if (typeof module !== 'undefined' && module.exports) {
	module.exports = RendererAPI;
}

// üî• Expor globalmente para que config-manager possa acessar
if (typeof window !== 'undefined') {
	window.RendererAPI = RendererAPI;
}

// Fun√ß√£o de log debug estilizado
function debugLogRenderer(msg) {
	console.log('%cü™≤ ‚ùØ‚ùØ‚ùØ‚ùØ Debug: ' + msg + ' em renderer.js', 'color: brown; font-weight: bold;');
}

/* ===============================
   FUN√á√ÉO PARA LOGAR M√âTRICAS
=============================== */

function logTranscriptionMetrics() {
	if (!transcriptionMetrics.audioStartTime) return;

	const whisperTime = transcriptionMetrics.whisperEndTime - transcriptionMetrics.whisperStartTime;
	const gptTime = transcriptionMetrics.gptEndTime - transcriptionMetrics.gptStartTime;
	const totalTime = transcriptionMetrics.totalTime;

	console.log(`üìä ================================`);
	console.log(`üìä M√âTRICAS DE TEMPO DETALHADAS:`);
	console.log(`üìä ================================`);
	console.log(`üìä TAMANHO √ÅUDIO: ${transcriptionMetrics.audioSize} bytes`);
	console.log(`üìä WHISPER: ${whisperTime}ms (${Math.round(transcriptionMetrics.audioSize / whisperTime)} bytes/ms)`);
	console.log(`üìä GPT: ${gptTime}ms`);
	console.log(`üìä TOTAL: ${totalTime}ms`);
	console.log(`üìä WHISPER % DO TOTAL: ${Math.round((whisperTime / totalTime) * 100)}%`);
	console.log(`üìä GPT % DO TOTAL: ${Math.round((gptTime / totalTime) * 100)}%`);
	console.log(`üìä ================================`);

	// Reset para pr√≥xima medi√ß√£o
	transcriptionMetrics = {
		audioStartTime: null,
		whisperStartTime: null,
		whisperEndTime: null,
		gptStartTime: null,
		gptEndTime: null,
		totalTime: null,
		audioSize: 0,
	};
}

//console.log('üöÄ Entrou no renderer.js');
