/* ================================ */
//	IMPORTS
/* ================================ */
const { ipcRenderer } = require('electron');
const { marked } = require('marked');
const hljs = require('highlight.js');
const { startAudioDeepgram, stopAudioDeepgram, switchDeviceDeepgram } = require('./stt-deepgram.js');
const { startAudioVosk, stopAudioVosk, switchDeviceVosk } = require('./stt-vosk.js');
const { startAudioWhisper, stopAudioWhisper, switchDeviceWhisper } = require('./stt-whisper.js');

// ğŸ”¥ Sistema de eventos para mÃ³dulos de transcriÃ§Ã£o (desacoplamento)
window.transcriptionEvents = new EventTarget();

/* =============================== */
//	ğŸ” PROTEÃ‡ÃƒO CONTRA CAPTURA DE TELA EXTERNA (Desabilita/limita APIs usadas por Zoom, Teams, Meet, OBS, Discord, Snipping Tool, etc.)
/* =============================== */
(function protectAgainstScreenCapture() {
	// âœ… Desabilita getDisplayMedia (usado por Zoom, Meet, Teams para capturar)
	if (navigator && navigator.mediaDevices && navigator.mediaDevices.getDisplayMedia) {
		const originalGetDisplayMedia = navigator.mediaDevices.getDisplayMedia.bind(navigator.mediaDevices);
		navigator.mediaDevices.getDisplayMedia = async function (...args) {
			console.warn('ğŸ” BLOQUEADO: Tentativa de usar getDisplayMedia (captura de tela externa)');
			throw new Error('Screen capture not available in this window');
		};
	}

	// âœ… Desabilita captureStream (usado para captura de janela)
	if (window.HTMLCanvasElement && window.HTMLCanvasElement.prototype.captureStream) {
		Object.defineProperty(window.HTMLCanvasElement.prototype, 'captureStream', {
			value: function () {
				console.warn('ğŸ” BLOQUEADO: Tentativa de usar Canvas.captureStream()');
				throw new Error('Capture stream not available');
			},
			writable: false,
			configurable: false,
		});
	}

	// âœ… Intercepta getUserMedia para avisar sobre tentativas de captura de Ã¡udio (pode ser usado em combo com vÃ­deo)
	if (navigator && navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
		const originalGetUserMedia = navigator.mediaDevices.getUserMedia.bind(navigator.mediaDevices);
		navigator.mediaDevices.getUserMedia = async function (constraints) {
			if (constraints && constraints.video) {
				console.warn('ğŸ” AVISO: Tentativa de usar getUserMedia com vÃ­deo detectada');
				// Ainda permite Ã¡udio, mas bloqueia vÃ­deo para captura
				if (constraints.video) {
					delete constraints.video;
				}
			}
			return originalGetUserMedia(constraints);
		};
	}

	console.log('âœ… ProteÃ§Ã£o contra captura externa ativada');
})();

/* =============================== */
//	CONSTANTES
/* =============================== */

const YOU = 'VocÃª';
const OTHER = 'Outros';

const ENABLE_INTERVIEW_TIMING_DEBUG_METRICS = true; // â† desligar depois se nÃ£o quiser mostrar time = false
const QUESTION_IDLE_TIMEOUT = 300; // Tempo de espera para a pergunta ser considerada inativa = 300
const CURRENT_QUESTION_ID = 'CURRENT'; // ID da pergunta atual

const INPUT_SPEECH_THRESHOLD = 20; // Valor limite (threshold) para detectar fala mais cedo = 20
const INPUT_SILENCE_TIMEOUT = 100; // Tempo de espera para silÃªncio = 100
const MIN_INPUT_AUDIO_SIZE = 1000; // Valor mÃ­nimo de tamanho de Ã¡udio para a normal = 1000
const MIN_INPUT_AUDIO_SIZE_INTERVIEW = 350; // Valor mÃ­nimo de tamanho de Ã¡udio para a entrevista = 350

const OUTPUT_SPEECH_THRESHOLD = 20; // Valor limite (threshold) para detectar fala mais cedo = 8
const OUTPUT_SILENCE_TIMEOUT = 100; // ğŸ”¥ OTIMIZADO: detecta fim de fala MAIS rÃ¡pido = 80ms para latÃªncia menor
const MIN_OUTPUT_AUDIO_SIZE = 1000; // Valor mÃ­nimo de tamanho de Ã¡udio para a normal = 2500
const MIN_OUTPUT_AUDIO_SIZE_INTERVIEW = 350; // Valor mÃ­nimo para enviar parcial (~3-4 chunks, ~3KB)
// controla intervalo mÃ­nimo entre requisiÃ§Ãµes STT parciais (ms) - mantÃ©m rate-limit para nÃ£o sobrecarregar API
const PARTIAL_MIN_INTERVAL_MS = 800; // ğŸ”¥ OTIMIZADO: transcriÃ§Ã£o parcial a cada 800ms (era 3000ms)

const OUTPUT_ENDING_PHRASES = ['tchau', 'tchau tchau', 'obrigado', 'valeu', 'falou', 'beleza', 'ok']; // Palavras finais para detectar o fim da fala

const SYSTEM_PROMPT = `
VocÃª Ã© um assistente para entrevistas tÃ©cnicas de Java. Responda como candidato.
Regras de resposta (priorize sempre estas):
- Seja natural e conciso: responda em no mÃ¡ximo 1â€“2 frases curtas.
- Use linguagem coloquial e direta, como alguÃ©m explicando rapidamente verbalmente.
- Evite listas longas, exemplos extensos ou parÃ¡grafos detalhados.
- NÃ£o comece com cumprimentos ou palavras de preenchimento (ex.: "Claro", "Ok").
- Quando necessÃ¡rio, entregue um exemplo mÃ­nimo de 1 linha apenas.
`;

/* =============================== */
//	TRANSCRIÃ‡ÃƒO VOSK (MODO ENTREVISTA)
/* =============================== */

let voskAccumulatedText = ''; // Acumula resultado parcial do Vosk
let voskPartialTimer = null;
let voskScriptProcessor = null; // ScriptProcessorNode para capturar PCM bruto
let voskAudioBuffer = []; // Acumula PCM entre envios

/* =============================== */
//SCREENSHOT CAPTURE - ESTADO E CONTROLE
/* =============================== */

let capturedScreenshots = []; // Array de { filepath, filename, timestamp }
let isCapturing = false;
let isAnalyzing = false;

/* =============================== */
//	ESTADO GLOBAL
/* =============================== */

let APP_CONFIG = {
	MODE_DEBUG: false, // â† alterado via config-manager.js (true = modo mock)
};

// ğŸªŸ Estado do Drag and Drop da janela
let isDraggingWindow = false;

let isRunning = false;
let audioContext;
// let mockInterviewRunning = false;

// ğŸ”¥ MODIFICADO: STT model vem da config agora (removido USE_LOCAL_WHISPER)
let transcriptionMetrics = {
	audioStartTime: null,
	gptStartTime: null,
	gptFirstTokenTime: null,
	gptEndTime: null,
	totalTime: null,
	audioSize: 0,
};

/* ğŸ¤ INPUT (VOCÃŠ) */
let inputStream;
let inputAnalyser;
let inputData;
let inputRecorder;
let inputChunks = [];
let inputSpeaking = false;
let inputSilenceTimer = null;
let inputPartialChunks = [];
let inputPartialTimer = null;

/* ğŸ”Š OUTPUT (OUTROS) */
let outputStream;
let outputAnalyser;
let outputData;
let outputRecorder;
let outputChunks = [];
let outputSpeaking = false;
let outputSilenceTimer = null;
let outputPartialChunks = [];
let outputPartialTimer = null;
let outputPartialText = '';

// ğŸ”¥ NOVO: IDs para rastrear e parar os loops de animation
let inputVolumeAnimationId = null;
let outputVolumeAnimationId = null;

/* ğŸ§  PERGUNTAS */
let currentQuestion = {
	text: '',
	lastUpdate: 0,
	finalized: false,
	lastUpdateTime: null,
	createdAt: null,
	finalText: '',
	interimText: '',
};
let questionsHistory = [];
const answeredQuestions = new Set(); // ğŸ”’ Armazena respostas jÃ¡ geradas (questionId -> true)
let selectedQuestionId = null;
let interviewTurnId = 0;
let gptAnsweredTurnId = null;
let gptRequestedTurnId = null;
let gptRequestedQuestionId = null; // ğŸ”¥ [IMPORTANTE] Rastreia QUAL pergunta foi realmente solicitada ao GPT
let lastSentQuestionText = '';
let autoCloseQuestionTimer = null;
let currentQuestionSilenceTimer = null; // ğŸ”¥ Timer para detectar fim de fala no CURRENT
// Timestamp para debounce de finalizaÃ§Ã£o (ms)
let lastFinalizeRequestAt = 0;
let lastInputStartAt = null;
let lastInputStopAt = null;
let lastOutputStartAt = null;
let lastOutputStopAt = null;
let lastInputPlaceholderEl = null;
let lastOutputPlaceholderEl = null;
let lastAskedQuestionNormalized = null;
let lastPartialSttAt = null;
let lastOutputPlaceholderId = null; // ğŸ”¥ ID Ãºnico para rastrear qual placeholder atualizar

// ğŸ”¥ VariÃ¡veis temporÃ¡rias para transcriÃ§Ã£o atual (imunes a race conditions)
// Armazenam os timestamps capturados NO MOMENTO de onstop() para uso exclusivo por transcribeOutput()
let pendingOutputStartAt = null;
let pendingOutputStopAt = null;

/* =============================== */
//	CALLBACKS / OBSERVERS SYSTEM
// (renderer.js DEVE ser "cego" para DOM e config-manager.js se inscreve em mudanÃ§as)
/* =============================== */

const UICallbacks = {
	onError: null, // ğŸ”¥ NOVO: Para mostrar erros de validaÃ§Ã£o
	onTranscriptAdd: null,
	onCurrentQuestionUpdate: null,
	onQuestionsHistoryUpdate: null,
	// onAnswerAdd: null,
	onStatusUpdate: null, // â† Adicionado: Para atualizar status na UI
	onInputVolumeUpdate: null,
	onOutputVolumeUpdate: null,
	onMockBadgeUpdate: null,
	onDOMElementsReady: null, // callback para pedir elementos ao config-manager
	onListenButtonToggle: null,
	onAnswerSelected: null,
	onClearAllSelections: null,
	onScrollToQuestion: null,
	onTranscriptionCleared: null,
	onAnswersCleared: null,
	onAnswerStreamChunk: null,
	onAnswerIdUpdate: null,
	onModeSelectUpdate: null,
	onAnswerStreamEnd: null,
	onPlaceholderFulfill: null,
	onPlaceholderUpdate: null,
	onUpdateInterim: null,
	onClearInterim: null,
	onScreenshotBadgeUpdate: null,
	onAudioDeviceChanged: null,
};

// FunÃ§Ã£o para config-manager se inscrever em eventos
function onUIChange(eventName, callback) {
	if (UICallbacks.hasOwnProperty(eventName)) {
		UICallbacks[eventName] = callback;
		console.log(`ğŸ“¡ UI callback registrado em renderer.js: ${eventName}`);
	}
}

// FunÃ§Ã£o para emitir/enviar eventos para config-manager
function emitUIChange(eventName, data) {
	if (UICallbacks[eventName] && typeof UICallbacks[eventName] === 'function') {
		UICallbacks[eventName](data);
	} else {
		console.warn(`âš ï¸ DEBUG: Nenhum callback registrado para '${eventName}'`);
	}
}

/* =============================== */
//	ELEMENTOS UI - Solicitado por callback (config-manager.js fornece esses elementos)
/* =============================== */

let UIElements = {
	inputSelect: null,
	outputSelect: null,
	listenBtn: null,
	statusText: null,
	transcriptionBox: null, // Mantido para compatibilidade, mas pode receber 'conversation'
	currentQuestionBox: null,
	currentQuestionTextBox: null,
	questionsHistoryBox: null,
	answersHistoryBox: null,
	askBtn: null,
	inputVu: null,
	outputVu: null,
	inputVuHome: null,
	outputVuHome: null,
	mockToggle: null,
	mockBadge: null,
	interviewModeSelect: null,
	btnClose: null,
	btnToggleClick: null,
	dragHandle: null,
	darkToggle: null,
	opacitySlider: null,
};

// config-manager.js chama isso para registrar elementos
function registerUIElements(elements) {
	UIElements = { ...UIElements, ...elements };
	console.log('âœ… UI Elements registrados no renderer.js');
}

/* =============================== */
//	MODO / ORQUESTRADOR
/* =============================== */

const MODES = {
	NORMAL: 'NORMAL',
	INTERVIEW: 'INTERVIEW',
};

// ğŸ”„ modo atual (default = comportamento atual)
let CURRENT_MODE = MODES.NORMAL;

// ğŸ¼ controlador central de estratÃ©gia
const ModeController = {
	isInterviewMode() {
		return CURRENT_MODE === MODES.INTERVIEW;
	},

	// â±ï¸ MediaRecorder.start(timeslice)
	mediaRecorderTimeslice() {
		if (!this.isInterviewMode()) return null;

		// OUTPUT pode ser mais agressivo que INPUT
		return 60; // reduzido para janelas parciais mais responsivas
	},

	// ğŸ¤– GPT streaming
	allowGptStreaming() {
		return this.isInterviewMode();
	},

	// ğŸ“¦ tamanho mÃ­nimo de Ã¡udio aceito
	minInputAudioSize(defaultSize) {
		return this.isInterviewMode() ? Math.min(400, defaultSize) : defaultSize;
	},
};

/* =============================== */
//	EVENTOS DE CONFIGURAÃ‡ÃƒO / UI
/* =============================== */

// Escuta evento de mudanÃ§a de dispositivo emitido pelo config-manager
onUIChange('onAudioDeviceChanged', async data => {
	try {
		if (!isRunning) return; // sÃ³ trocar se app estiver em execuÃ§Ã£o
		if (!data || !data.type || !data.deviceId) return; // dados invÃ¡lidos

		// ğŸ”¥ ORQUESTRADOR: Roteia por modelo STT
		const sttModel = getConfiguredSTTModel();
		if (sttModel === 'deepgram') {
			if (typeof switchDeviceDeepgram === 'function') await switchDeviceDeepgram(data.type, data.deviceId);
		} else if (sttModel === 'vosk') {
			if (typeof switchDeviceVosk === 'function') await switchDeviceVosk(data.type, data.deviceId);
		} else if (sttModel === 'whisper-cpp-local' || sttModel === 'whisper-1') {
			if (typeof switchDeviceWhisper === 'function') await switchDeviceWhisper(UIElements);
		}
	} catch (err) {
		console.warn('Erro ao processar onAudioDeviceChanged:', err);
	}
});

/* =============================== */
//	AUDIO - VOLUME MONITORING
/* =============================== */

// Inicia apenas monitoramento de volume (sem gravar)
async function startInputVolumeMonitoring() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "startInputVolumeMonitoring"');

	if (APP_CONFIG.MODE_DEBUG) {
		console.log('ğŸ¤ Monitoramento de volume entrada (modo teste)...');
		return;
	}

	if (!UIElements.inputSelect?.value) {
		console.log('âš ï¸ Nenhum dispositivo input selecionado');
		return;
	}

	if (!audioContext) {
		audioContext = new AudioContext();
	}

	// ğŸ”¥ NOVO: Se jÃ¡ tem stream ativa, nÃ£o faz nada
	if (inputStream && inputAnalyser) {
		console.log('â„¹ï¸ Monitoramento de volume de entrada jÃ¡ ativo');
		return;
	}

	try {
		// Verificar se isRunning Ã© false antes de iniciar o stream
		if (!isRunning) {
			console.log('ğŸ”„ Iniciando stream de Ã¡udio (input)...');

			inputStream = await navigator.mediaDevices.getUserMedia({
				audio: { deviceId: { exact: UIElements.inputSelect.value } },
			});

			const source = audioContext.createMediaStreamSource(inputStream);

			inputAnalyser = audioContext.createAnalyser();
			inputAnalyser.fftSize = 256;
			inputData = new Uint8Array(inputAnalyser.frequencyBinCount);
			source.connect(inputAnalyser);

			console.log('âœ… Monitoramento de volume de entrada iniciado com sucesso');
			updateInputVolume(); // ğŸ”¥ Inicia o loop de atualizaÃ§Ã£o
		}
	} catch (error) {
		console.error('âŒ Erro ao iniciar monitoramento de volume de entrada:', error);
		inputStream = null;
		inputAnalyser = null;
	}

	debugLogRenderer('Fim da funÃ§Ã£o: "startInputVolumeMonitoring"');
}

// Inicia apenas monitoramento de volume para output (sem gravar)
async function startOutputVolumeMonitoring() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "startOutputVolumeMonitoring"');

	// Se o modo de debug estiver ativo, retorna
	if (APP_CONFIG.MODE_DEBUG) {
		console.log('ğŸ”Š Monitoramento de volume saÃ­da (modo teste)...');
		return;
	}

	// Se nÃ£o houver dispositivo de saÃ­da selecionado, retorna
	if (!UIElements.outputSelect?.value) {
		console.log('âš ï¸ Nenhum dispositivo output selecionado');
		return;
	}

	// Se nÃ£o houver contexto de Ã¡udio, cria um novo
	if (!audioContext) {
		audioContext = new AudioContext();
	}

	// Se jÃ¡ houver stream e analisador de frequÃªncia ativos, retorna
	if (outputStream && outputAnalyser) {
		console.log('â„¹ï¸ Monitoramento de volume de saÃ­da jÃ¡ ativo');
		return;
	}

	try {
		// Se isRunning for false, inicia o stream de Ã¡udio (output)
		if (!isRunning) {
			console.log('ğŸ”„ Iniciando stream de Ã¡udio (output)...');

			// Cria a stream de Ã¡udio (outputStream)
			await createOutputStream();
		}

		debugLogRenderer('Fim da funÃ§Ã£o: "startOutputVolumeMonitoring"');
	} catch (error) {
		console.error('âŒ Erro ao iniciar monitoramento de volume de saÃ­da:', error);

		// Limpa a stream e o analisador de frequÃªncia (outputStream e outputAnalyser)
		outputStream = null;
		outputAnalyser = null;
	}
}

function stopInputVolumeMonitoring() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "stopInputVolumeMonitoring"');

	// Se isRunning true, nÃ£o para o monitoramento
	if (isRunning) {
		console.log('â„¹ï¸ Monitoramento de volume de entrada em execuÃ§Ã£o, isRunning = true â€” pulando parada');

		debugLogRenderer('Fim da funÃ§Ã£o: "stopInputVolumeMonitoring"');
		return;
	}

	// 1. Para o loop de animaÃ§Ã£o
	if (inputVolumeAnimationId) {
		cancelAnimationFrame(inputVolumeAnimationId);
		inputVolumeAnimationId = null;
	}

	// 2. Para as tracks de Ã¡udio para economizar energia/recurso
	if (inputStream) {
		inputStream.getTracks().forEach(track => track.stop());
		inputStream = null;
	}

	inputAnalyser = null;
	inputData = null;

	// 3. Zera a UI
	emitUIChange('onInputVolumeUpdate', { percent: 0 });

	console.log('ğŸ›‘ Monitoramento de volume de entrada parado');

	debugLogRenderer('Fim da funÃ§Ã£o: "stopInputVolumeMonitoring"');
}

function stopOutputVolumeMonitoring() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "stopOutputVolumeMonitoring"');

	// Se isRunning true, nÃ£o para o monitoramento
	if (isRunning) {
		console.log('â„¹ï¸ Monitoramento de volume de saÃ­da em execuÃ§Ã£o, isRunning = true â€” pulando parada');

		debugLogRenderer('Fim da funÃ§Ã£o: "stopOutputVolumeMonitoring"');
		return;
	}

	// 1. Para o loop de animaÃ§Ã£o
	if (outputVolumeAnimationId) {
		cancelAnimationFrame(outputVolumeAnimationId);
		outputVolumeAnimationId = null;
	}

	// 2.Para as tracks de Ã¡udio para economizar energia/recurso
	if (outputStream) {
		outputStream.getTracks().forEach(track => track.stop());
		outputStream = null;
	}

	outputAnalyser = null;
	outputData = null;

	// 3. Zera a UI
	emitUIChange('onOutputVolumeUpdate', { percent: 0 });

	console.log('ğŸ›‘ Monitoramento de volume de saÃ­da parado');

	debugLogRenderer('Fim da funÃ§Ã£o: "stopOutputVolumeMonitoring"');
}

async function createOutputStream() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "createOutputStream"');

	// Cria a stream de Ã¡udio (outputStream)
	outputStream = await navigator.mediaDevices.getUserMedia({
		audio: { deviceId: { exact: UIElements.outputSelect.value } },
	});

	// Cria o source de Ã¡udio (source)
	const source = audioContext.createMediaStreamSource(outputStream);

	// Cria o analisador de frequÃªncia (outputAnalyser)
	outputAnalyser = audioContext.createAnalyser();
	// Define o tamanho do FFT (fftSize) como 256
	outputAnalyser.fftSize = 256;
	// Cria os dados (outputData)
	outputData = new Uint8Array(outputAnalyser.frequencyBinCount);
	// Conecta o source ao analisador de frequÃªncia
	source.connect(outputAnalyser);

	debugLogRenderer('Fim da funÃ§Ã£o: "createOutputStream"');

	return source;
}

/* =============================== */
//	HELPERS PUROS (novo)
/* =============================== */

// ObtÃ©m o modelo STT configurado via config-manager
function getConfiguredSTTModel() {
	try {
		if (!window.configManager || !window.configManager.config) {
			console.warn('âš ï¸ configManager nÃ£o disponÃ­vel no escopo global');
			return 'error'; // fallback
		}

		const config = window.configManager.config;
		const activeProvider = config.api?.activeProvider;
		const sttModel = config.api?.[activeProvider]?.selectedSTTModel;

		if (!sttModel) {
			console.warn(`âš ï¸ Modelo STT nÃ£o configurado para ${activeProvider}`);
			return 'error'; // fallback
		}

		return sttModel;
	} catch (err) {
		console.error('âŒ Erro ao obter modelo STT da config:', err);
		return 'error'; // fallback
	}
}

function finalizeQuestion(t) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "finalizeQuestion"');
	debugLogRenderer('Fim da funÃ§Ã£o: "finalizeQuestion"');
	return t.trim().endsWith('?') ? t.trim() : t.trim() + '?';
}

// Reseta o estado da pergunta atual (CURRENT)
function resetCurrentQuestion() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "resetCurrentQuestion"');

	currentQuestion = {
		text: '',
		lastUpdate: 0,
		finalized: false,
		lastUpdateTime: null,
		createdAt: null,
		finalText: '',
		interimText: '',
	};

	// ğŸ”¥ Limpar timer de silÃªncio
	if (currentQuestionSilenceTimer) {
		clearTimeout(currentQuestionSilenceTimer);
		currentQuestionSilenceTimer = null;
	}

	debugLogRenderer('Fim da funÃ§Ã£o: "resetCurrentQuestion"');
}

// Renderiza o histÃ³rico de perguntas
function renderQuestionsHistory() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "renderQuestionsHistory"');

	// ğŸ”¥ Gera dados estruturados - config-manager renderiza no DOM
	const historyData = [...questionsHistory].reverse().map(q => {
		let label = q.text;
		if (ENABLE_INTERVIEW_TIMING_DEBUG_METRICS && q.lastUpdateTime) {
			const time = new Date(q.lastUpdateTime).toLocaleTimeString();
			label = `â±ï¸ ${time} â€” ${label}`;
		}

		return {
			id: q.id,
			text: label,
			isIncomplete: q.incomplete,
			isAnswered: q.answered,
			isSelected: q.id === selectedQuestionId,
		};
	});

	emitUIChange('onQuestionsHistoryUpdate', historyData);

	scrollToSelectedQuestion();

	debugLogRenderer('Fim da funÃ§Ã£o: "renderQuestionsHistory"');
}

// Retorna o texto da pergunta selecionada (CURRENT ou do histÃ³rico)
function getSelectedQuestionText() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "getSelectedQuestionText"');
	debugLogRenderer('Fim da funÃ§Ã£o: "getSelectedQuestionText"');

	// 1ï¸âƒ£ Se existe seleÃ§Ã£o explÃ­cita
	if (selectedQuestionId === CURRENT_QUESTION_ID) {
		return currentQuestion.text;
	}

	if (selectedQuestionId) {
		const q = questionsHistory.find(q => q.id === selectedQuestionId);
		if (q?.text) return q.text;
	}

	// 2ï¸âƒ£ Fallback: CURRENT (se tiver texto)
	if (currentQuestion.text && currentQuestion.text.trim().length > 0) {
		return currentQuestion.text;
	}

	return '';
}

// Normaliza texto para comparaÃ§Ã£o (lowercase, remove pontuaÃ§Ã£o, espaÃ§os extras)
function normalizeForCompare(t) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "normalizeForCompare"');
	debugLogRenderer('Fim da funÃ§Ã£o: "normalizeForCompare"');
	return (t || '')
		.toLowerCase()
		.replace(/[?!.\n\r]/g, '')
		.replace(/\s+/g, ' ')
		.trim();
}

// Atualiza a mensagem de status na UI
function updateStatusMessage(message) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "updateStatusMessage"');
	emitUIChange('onStatusUpdate', { message });
	debugLogRenderer('Fim da funÃ§Ã£o: "updateStatusMessage"');
}

// Verifica se uma pergunta jÃ¡ foi respondida (pelo ID)
function findAnswerByQuestionId(questionId) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "findAnswerByQuestionId"');

	if (!questionId) {
		// ID invÃ¡lido
		debugLogRenderer('Fim da funÃ§Ã£o: "findAnswerByQuestionId"');
		return false;
	}

	debugLogRenderer('Fim da funÃ§Ã£o: "findAnswerByQuestionId"');
	return answeredQuestions.has(questionId);
}

function promoteCurrentToHistory(text) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "promoteCurrentToHistory"');

	debugLogRenderer('ğŸ“š promovendo pergunta para histÃ³rico:', text, false);

	// evita duplicaÃ§Ã£o no histÃ³rico: se a Ãºltima entrada Ã© igual (normalizada), nÃ£o adiciona
	const last = questionsHistory.length ? questionsHistory[questionsHistory.length - 1] : null;
	if (last && normalizeForCompare(last.text) === normalizeForCompare(text)) {
		debugLogRenderer('ğŸ”• pergunta igual jÃ¡ presente no histÃ³rico â€” pulando promoÃ§Ã£o', false);

		// limpa CURRENT mas preserva seleÃ§Ã£o conforme antes
		const prevSelected = selectedQuestionId;
		currentQuestion = {
			text: '',
			lastUpdate: 0,
			finalized: false,
			lastUpdateTime: null,
			createdAt: null,
			finalText: '',
			interimText: '',
		};

		// ğŸ”¥ Limpar timer de silÃªncio
		if (currentQuestionSilenceTimer) {
			clearTimeout(currentQuestionSilenceTimer);
			currentQuestionSilenceTimer = null;
		}

		if (prevSelected === null || prevSelected === CURRENT_QUESTION_ID) {
			selectedQuestionId = CURRENT_QUESTION_ID;
		} else {
			selectedQuestionId = prevSelected;
		}

		renderQuestionsHistory();
		renderCurrentQuestion();
		return;
	}

	const newId = String(questionsHistory.length + 1);

	questionsHistory.push({
		id: newId,
		text,
		createdAt: currentQuestion.createdAt || Date.now(),
		lastUpdateTime: currentQuestion.lastUpdateTime || currentQuestion.createdAt || Date.now(),
	});

	// ğŸ”¥ [IMPORTANTE] Migrar resposta de CURRENT para o novo ID no history
	if (answeredQuestions.has(CURRENT_QUESTION_ID)) {
		answeredQuestions.delete(CURRENT_QUESTION_ID);
		answeredQuestions.add(newId);
		debugLogRenderer('ğŸ”„ [IMPORTANTE] Migrada resposta de CURRENT para newId:', newId, false);
	}

	// ğŸ”¥ [CRÃTICO] Atualizar o ID do bloco de resposta no DOM se ele foi criado com CURRENT
	debugLogRenderer(
		'ğŸ”„ [IMPORTANTE] Emitindo onAnswerIdUpdate para atualizar bloco de resposta: CURRENT â†’ ',
		newId,
		false,
	);
	emitUIChange('onAnswerIdUpdate', {
		oldId: CURRENT_QUESTION_ID,
		newId: newId,
	});

	// ğŸ”¥ [IMPORTANTE] Se uma pergunta CURRENT foi solicitada ao GPT,
	// atualizar o rastreamento para apontar para o novo ID promovido
	if (gptRequestedQuestionId === CURRENT_QUESTION_ID) {
		gptRequestedQuestionId = newId;
		debugLogRenderer('ğŸ”„ [IMPORTANTE] gptRequestedQuestionId atualizado de CURRENT para newId:', newId, false);
	}

	// preserva seleÃ§Ã£o do usuÃ¡rio: se nÃ£o havia seleÃ§Ã£o explÃ­cita ou estava no CURRENT,
	// mantÃ©m a seleÃ§Ã£o no CURRENT para que o novo CURRENT seja principal.
	const prevSelected = selectedQuestionId;

	// ğŸ”¥ RESET COMPLETO: Limpar timer de silÃªncio antes de resetar
	if (currentQuestionSilenceTimer) {
		debugLogRenderer('ğŸ”¥ Limpando timer de silÃªncio durante promoÃ§Ã£o', false);
		clearTimeout(currentQuestionSilenceTimer);
		currentQuestionSilenceTimer = null;
	}

	resetCurrentQuestion();

	if (prevSelected === null || prevSelected === CURRENT_QUESTION_ID) {
		selectedQuestionId = CURRENT_QUESTION_ID;
	} else {
		// usuÃ¡rio tinha selecionado algo no histÃ³rico â€” preserva essa seleÃ§Ã£o
		selectedQuestionId = prevSelected;
	}

	renderQuestionsHistory();
	renderCurrentQuestion();

	debugLogRenderer('Fim da funÃ§Ã£o: "promoteCurrentToHistory"');
}

/* =============================== */
//	DISPOSITIVOS / CONTROLE DE ÃUDIO
/* =============================== */

async function startAudio() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "startAudio"');

	// ğŸ”¥ [NOVO ORQUESTRADOR] Detecta modelo STT e roteia
	const sttModel = getConfiguredSTTModel();
	console.log(`ğŸ¤ startAudio: Modelo STT = ${sttModel}`);

	try {
		// ğŸ”¥ ROTEAMENTO: Por modelo STT
		if (sttModel === 'deepgram') {
			await startAudioDeepgram(UIElements);
		} else if (sttModel === 'vosk') {
			await startAudioVosk(UIElements);
		} else if (sttModel === 'whisper-cpp-local' || sttModel === 'whisper-1') {
			const logLabel = sttModel === 'whisper-cpp-local' ? 'local' : 'API OpenAI';
			console.log(`ğŸ¤ Roteando para Whisper (${logLabel})`);
			await startAudioWhisper(UIElements);
		} else {
			// Modelo nÃ£o suportado
			console.error('âŒ Erro ao obter modelo STT configurado');
			return;
		}
	} catch (error) {
		console.error('âŒ Erro em startAudio:', error);
		throw error;
	}

	debugLogRenderer('Fim da funÃ§Ã£o: "startAudio"');
}

/* =============================== */
//	UI (RENDER / SELEÃ‡ÃƒO / SCROLL)
/* =============================== */

// FunÃ§Ã£o principal para o botÃ£o de iniciar/parar escuta (ComeÃ§ar a Ouvir... (Ctrl+d))
async function listenToggleBtn() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "listenToggleBtn"');

	if (!isRunning) {
		console.log('ğŸ¤ listenToggleBtn: Tentando INICIAR escuta...');

		// ğŸ”¥ VALIDAÃ‡ÃƒO 1: Modelo de IA ativo
		const { active: hasModel, model: activeModel } = hasActiveModel();
		debugLogRenderer(`ğŸ“Š DEBUG: hasModel = ${hasModel}, activeModel = ${activeModel}`, false);

		if (!hasModel) {
			const errorMsg = 'Ative um modelo de IA antes de comeÃ§ar a ouvir';
			console.warn(`âš ï¸ ${errorMsg}`);
			emitUIChange('onError', errorMsg);
			return;
		}

		// ğŸ”¥ VALIDAÃ‡ÃƒO 2: Dispositivo de Ã¡udio de SAÃDA (obrigatÃ³rio para ouvir a reuniÃ£o)
		const hasOutputDevice = UIElements.outputSelect?.value;
		debugLogRenderer(`ğŸ“Š DEBUG: hasOutputDevice = ${hasOutputDevice}`, false);

		if (!hasOutputDevice) {
			const errorMsg = 'Selecione um dispositivo de Ã¡udio (output) para ouvir a reuniÃ£o';
			console.warn(`âš ï¸ ${errorMsg}`);
			console.log('ğŸ“¡ DEBUG: Emitindo onError:', errorMsg);
			emitUIChange('onError', errorMsg);
			return;
		}
	}

	// Inverte o estado de isRunning
	isRunning = !isRunning;
	const buttonText = isRunning ? 'Parar a Escuta... (Ctrl+d)' : 'ComeÃ§ar a Ouvir... (Ctrl+d)';
	const statusMsg = isRunning ? 'Status: ouvindo...' : 'Status: parado';

	// Emite o evento 'onListenButtonToggle' para atualizar o botÃ£o de escuta
	emitUIChange('onListenButtonToggle', {
		isRunning,
		buttonText,
	});

	// Atualiza o status da escuta na tela
	updateStatusMessage(statusMsg);

	await (isRunning ? startAudio() : stopAudio());

	debugLogRenderer('Fim da funÃ§Ã£o: "listenToggleBtn"');
}

// Verifica se hÃ¡ um modelo de IA ativo na configuraÃ§Ã£o e retorna o status e o nome do modelo
function hasActiveModel() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "hasActiveModel"');
	if (!window.configManager) {
		console.warn('âš ï¸ ConfigManager nÃ£o inicializado ainda');
		return { active: false, model: null };
	}

	const config = window.configManager.config;
	if (!config || !config.api) {
		console.warn('âš ï¸ Config ou api nÃ£o disponÃ­vel');
		return { active: false, model: null };
	}

	// Verifica se algum modelo estÃ¡ ativo e retorna o nome
	const providers = ['openai', 'google', 'openrouter', 'custom'];
	for (const provider of providers) {
		if (config.api[provider] && config.api[provider].enabled === true) {
			console.log(`âœ… Modelo ativo encontrado: ${provider}`);
			return { active: true, model: provider };
		}
	}

	console.warn('âš ï¸ Nenhum modelo ativo encontrado');

	debugLogRenderer('Fim da funÃ§Ã£o: "hasActiveModel"');
	return { active: false, model: null };
}

// Manipula o clique em uma pergunta do histÃ³rico ou CURRENT
function handleQuestionClick(questionId) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "handleQuestionClick"');
	selectedQuestionId = questionId;
	clearAllSelections();
	renderQuestionsHistory();
	renderCurrentQuestion();

	// âš ï¸ CURRENT nunca bloqueia resposta
	if (questionId !== CURRENT_QUESTION_ID) {
		const existingAnswer = findAnswerByQuestionId(questionId);

		if (existingAnswer) {
			emitUIChange('onAnswerSelected', {
				questionId: questionId,
				shouldScroll: true,
			});

			updateStatusMessage('ğŸ“Œ Essa pergunta jÃ¡ foi respondida');
			return;
		}
	}

	// Se for uma pergunta do histÃ³rico marcada como incompleta, nÃ£o enviar automaticamente ao GPT
	if (questionId !== CURRENT_QUESTION_ID) {
		const q = questionsHistory.find(q => q.id === questionId);
		if (q && q.incomplete) {
			updateStatusMessage('âš ï¸ Pergunta incompleta â€” pressione o botÃ£o de responder para enviar ao GPT');
			console.log('â„¹ï¸ pergunta incompleta selecionada â€” aguarda envio manual:', q.text);
			return;
		}
	}

	if (
		ModeController.isInterviewMode() &&
		selectedQuestionId === CURRENT_QUESTION_ID &&
		gptAnsweredTurnId === interviewTurnId
	) {
		updateStatusMessage('â›” GPT jÃ¡ respondeu esse turno');
		console.log('â›” GPT jÃ¡ respondeu esse turno');
		return;
	}

	// â“ Ainda nÃ£o respondida â†’ chama GPT (click ou atalho)
	//console.error('closeCurrentQuestion: askGpt() 2978; ğŸ”’ COMENTADA atÃ© transcriÃ§Ã£o em tempo real funcionar');
	askGpt(); // ğŸ”’ COMENTADA atÃ© transcriÃ§Ã£o em tempo real funcionar

	debugLogRenderer('Fim da funÃ§Ã£o: "handleQuestionClick"');
}

// Aplica opacidade na interface
function applyOpacity(value) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "applyOpacity"');
	const appOpacity = parseFloat(value);

	// aplica opacidade no conteÃºdo geral
	document.documentElement.style.setProperty('--app-opacity', appOpacity.toFixed(2));

	// topBar nunca abaixo de 0.75
	const topbarOpacity = Math.max(appOpacity, 0.75);
	document.documentElement.style.setProperty('--app-opacity-75', topbarOpacity.toFixed(2));

	localStorage.setItem('overlayOpacity', appOpacity);

	// logs temporÃ¡rios para debug
	console.log('ğŸšï¸ Opacity change | app:', value, '| topBar:', topbarOpacity);

	debugLogRenderer('Fim da funÃ§Ã£o: "applyOpacity"');
}

// ConfiguraÃ§Ã£o do Marked.js para renderizaÃ§Ã£o de Markdown
marked.setOptions({
	html: true, // ğŸ”¥ Permite renderizaÃ§Ã£o de HTML (nÃ£o escapa entidades)
	breaks: true,
	gfm: true, // GitHub Flavored Markdown
	highlight: function (code, lang) {
		if (lang && hljs.getLanguage(lang)) {
			return hljs.highlight(code, { language: lang }).value;
		}
		return hljs.highlightAuto(code).value;
	},
});

// Limpa todas as seleÃ§Ãµes visuais de perguntas
function clearAllSelections() {
	// Emite evento para o controller limpar as seleÃ§Ãµes visuais
	emitUIChange('onClearAllSelections', {});
}

// Renderiza a pergunta atual (CURRENT)
function renderCurrentQuestion() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "renderCurrentQuestion"');

	// Se nÃ£o hÃ¡ texto, emite vazio
	if (!currentQuestion.text) {
		emitUIChange('onCurrentQuestionUpdate', { text: '', isSelected: false });
		return;
	}

	let label = currentQuestion.text;

	// Adiciona timestamp se modo debug mÃ©tricas ativo
	if (ENABLE_INTERVIEW_TIMING_DEBUG_METRICS && currentQuestion.lastUpdateTime) {
		const time = new Date(currentQuestion.lastUpdateTime).toLocaleTimeString();
		label = `â±ï¸ ${time} â€” ${label}`;
	}

	// ğŸ”¥ Gera dados estruturados - config-manager renderiza no DOM
	const questionData = {
		text: label,
		isSelected: selectedQuestionId === CURRENT_QUESTION_ID,
		rawText: currentQuestion.text,
		createdAt: currentQuestion.createdAt,
		lastUpdateTime: currentQuestion.lastUpdateTime,
	};

	// Emite evento para o config-manager renderizar no DOM
	emitUIChange('onCurrentQuestionUpdate', questionData);

	debugLogRenderer('Fim da funÃ§Ã£o: "renderCurrentQuestion"');
}

// Rola a lista de perguntas para a pergunta selecionada
function scrollToSelectedQuestion() {
	emitUIChange('onScrollToQuestion', {
		questionId: selectedQuestionId,
	});
}

/* =============================== */
//	CONSOLIDAÃ‡ÃƒO DE PERGUNTAS
/* =============================== */

// Fluxo para consolidar transcriÃ§Ãµes no CURRENT. Concatena transcriÃ§Ã£o interims e finais.
function handleCurrentQuestion(author, text, options = {}) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "handleCurrentQuestion"');

	const cleaned = text.replace(/ÃŠ+|hum|ahn/gi, '').trim();

	// Usa o tempo exato que chegou no renderer (Date.now)
	const now = Date.now();

	// Apenas consolida falas no CURRENT do OTHER
	if (author === OTHER) {
		// Se nÃ£o existe texto ainda, marca tempo de criaÃ§Ã£o e incrementa turno
		if (!currentQuestion.text) {
			currentQuestion.createdAt = now;
			interviewTurnId++;
		}

		currentQuestion.lastUpdateTime = now;
		currentQuestion.lastUpdate = now;

		debugLogRenderer('currentQuestion antes: ', { ...currentQuestion }, false);

		// LÃ³gica de consolidaÃ§Ã£o para evitar duplicaÃ§Ãµes
		if (options.isInterim) {
			// Para interims: substituir o interim atual (Deepgram envia versÃµes progressivas)
			currentQuestion.interimText = cleaned;
		} else {
			// Para finais: limpar interim e ACUMULAR no finalText
			currentQuestion.interimText = '';
			currentQuestion.finalText = (currentQuestion.finalText ? currentQuestion.finalText + ' ' : '') + cleaned;
		}

		debugLogRenderer('currentQuestion durante: ', { ...currentQuestion }, false);

		// Atualizar o texto total
		currentQuestion.text =
			currentQuestion.finalText.trim() + (currentQuestion.interimText ? ' ' + currentQuestion.interimText : '');

		debugLogRenderer('currentQuestion depois: ', { ...currentQuestion }, false);

		// ğŸŸ¦ CURRENT vira seleÃ§Ã£o padrÃ£o ao receber fala
		if (!selectedQuestionId) {
			selectedQuestionId = CURRENT_QUESTION_ID;
			clearAllSelections();
		}

		// Adiciona TUDO Ã  conversa visual em tempo real ao elemento "currentQuestionText"
		renderCurrentQuestion();

		// SÃ³ finaliza se estivermos em silÃªncio e NÃƒO for um interim
		if (options.shouldFinalizeAskCurrent && !options.isInterim) {
			debugLogRenderer('ğŸŸ¢ ********  EstÃ¡ em silÃªncio, feche a pergunta e chame o GPT ğŸ¤– ******** ğŸŸ¢', true);

			// fecha/finaliza a pergunta atual
			finalizeCurrentQuestion();
		}
	}

	debugLogRenderer('Fim da funÃ§Ã£o: "handleCurrentQuestion"');
}

/* =============================== */
//	GPT (Novo)
/* =============================== */

// Envia pergunta selecionada ao GPT
async function askGpt() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "askGpt"');

	const questionId = selectedQuestionId;
	const isCurrent = questionId === CURRENT_QUESTION_ID;
	const text = getSelectedQuestionText();
	const normalizedText = normalizeForCompare(text);
	transcriptionMetrics.gptStartTime = Date.now(); // Marca inÃ­cio GPT

	// Evita reenvio da mesma pergunta atual ao GPT (dedupe)
	if (isCurrent && normalizedText && lastAskedQuestionNormalized === normalizedText) {
		updateStatusMessage('â›” Pergunta jÃ¡ enviada');
		console.log('â›” askGpt: mesma pergunta jÃ¡ enviada, pulando');
		return;
	}

	// ğŸ›¡ï¸ MODO ENTREVISTA â€” bloqueia duplicaÃ§Ã£o APENAS para histÃ³rico
	if (ModeController.isInterviewMode() && !isCurrent) {
		const existingAnswer = findAnswerByQuestionId(questionId);
		if (existingAnswer) {
			updateStatusMessage('ğŸ“Œ Essa pergunta jÃ¡ foi respondida');
			return;
		}
	}

	// Nota log temporario para testar a aplicaÃ§Ã£o remover depois
	debugLogRenderer(
		'ğŸ¤– ğŸ§¾ askGpt diagnÃ³stico',
		{
			currentQuestion,
			gptAnsweredTurnId,
			interviewTurnId,
			isCurrent,
			isInterviewMode: ModeController.isInterviewMode(),
			questionId_variable: questionId, // ğŸ”¥ DEBUG: mostrar a variÃ¡vel questionId
			selectedQuestionId,
			textGPT: normalizedText,
			textLength: text.length,
		},
		false,
	);

	// marca que este turno teve uma requisiÃ§Ã£o ao GPT (apenas para CURRENT)
	if (isCurrent) {
		gptRequestedTurnId = interviewTurnId;
		gptRequestedQuestionId = CURRENT_QUESTION_ID; // ğŸ”¥ [IMPORTANTE] Rastreia qual pergunta foi solicitada
		lastAskedQuestionNormalized = normalizedText;
		lastSentQuestionText = text.trim();
	}

	// ï¿½ MODO ENTREVISTA â€” STREAMING
	if (ModeController.isInterviewMode()) {
		let streamedText = '';

		debugLogRenderer('â³ enviando para o GPT via stream...', true);

		ipcRenderer
			.invoke('ask-gpt-stream', [
				{ role: 'system', content: SYSTEM_PROMPT },
				{ role: 'user', content: text },
			])
			.catch(err => {
				console.error('âŒ Erro ao chamar ask-gpt-stream:', err);
				updateStatusMessage('âŒ Erro ao enviar para GPT');
			});

		const onChunk = (_, token) => {
			streamedText += token;

			// ğŸ”¥ PROTEÃ‡ÃƒO: Valida se o questionId ainda Ã© vÃ¡lido
			// (evita renderizar em question ID antigo/invÃ¡lido)
			if (
				!questionId ||
				(isCurrent && gptRequestedQuestionId !== CURRENT_QUESTION_ID) ||
				(!isCurrent && !questionsHistory.find(q => q.id === questionId))
			) {
				console.warn('ğŸš¨ onChunk: questionId invÃ¡lido ou desatualizado, ignorando token:', {
					questionId,
					isCurrent,
					gptRequestedQuestionId,
					token,
				});
				return;
			}

			emitUIChange('onAnswerStreamChunk', {
				questionId,
				token,
				accum: streamedText,
			});

			transcriptionMetrics.gptFirstTokenTime = transcriptionMetrics.gptFirstTokenTime || Date.now();

			debugLogRenderer(`ğŸ¬ ğŸŸ¢ GPT_STREAM_CHUNK recebido (token parcial): "${token}"`, false);
		};

		const onEnd = () => {
			debugLogRenderer('âœ… GPT_STREAM_END recebido - Stream finalizado!', true);

			ipcRenderer.removeListener('GPT_STREAM_CHUNK', onChunk);
			ipcRenderer.removeListener('GPT_STREAM_END', onEnd);

			// Finaliza mediÃ§Ãµes
			transcriptionMetrics.gptEndTime = Date.now();
			transcriptionMetrics.totalTime = Date.now() - transcriptionMetrics.audioStartTime;

			// Log mÃ©tricas
			logTranscriptionMetrics();

			if (ENABLE_INTERVIEW_TIMING_DEBUG_METRICS) {
				let finalText = streamedText;
				const endAt = Date.now();
				const elapsed = endAt - transcriptionMetrics.gptStartTime;

				const startTime = new Date(transcriptionMetrics.gptStartTime).toLocaleTimeString();
				const endTime = new Date(endAt).toLocaleTimeString();

				finalText +=
					`\n\nâ±ï¸ GPT iniciou: ${startTime}` + `\nâ±ï¸ GPT finalizou: ${endTime}` + `\nâ±ï¸ Resposta em ${elapsed}ms`;

				debugLogRenderer(
					'ğŸ¤– Resposta GPT â“' +
						finalText +
						`\nâ±ï¸ Primeiro Token: ${new Date(transcriptionMetrics.gptFirstTokenTime).toLocaleTimeString()}`,
					false,
				);
			}

			// garante que o turno foi realmente fechado
			const wasRequestedForThisTurn = gptRequestedTurnId === interviewTurnId;
			const requestedQuestionId = gptRequestedQuestionId; // ğŸ”¥ Qual pergunta foi REALMENTE solicitada

			gptAnsweredTurnId = interviewTurnId;
			gptRequestedTurnId = null;
			gptRequestedQuestionId = null; // ğŸ”¥ Limpa apÃ³s usar

			// ğŸ”’ RENDERIZAR A RESPOSTA COM O ID CORRETO
			if (requestedQuestionId) {
				// const finalHtml = marked.parse(finalText); // Resposta jÃ¡ renderizada via streaming

				debugLogRenderer(
					'âœ… GPT_STREAM_END: Renderizando resposta para pergunta solicitada:',
					{
						requestedQuestionId,
						wasRequestedForThisTurn,
					},
					false,
				);

				// Se a pergunta solicitada foi CURRENT, promover para history ANTES de renderizar
				if (requestedQuestionId === CURRENT_QUESTION_ID && currentQuestion.text) {
					debugLogRenderer('ğŸ”„ GPT_STREAM_END: Promovendo CURRENT para history antes de renderizar resposta', true);
					promoteCurrentToHistory(currentQuestion.text);

					// Pega a pergunta recÃ©m-promovida
					const promotedQuestion = questionsHistory[questionsHistory.length - 1];
					if (promotedQuestion) {
						// Renderiza com o ID da pergunta promovida
						promotedQuestion.answered = true;
						answeredQuestions.add(promotedQuestion.id);
						renderQuestionsHistory();
						debugLogRenderer('âœ… Resposta renderizada para pergunta promovida:', promotedQuestion.id, false);
					} else {
						console.warn('âš ï¸ Pergunta promovida nÃ£o encontrada');
					}
				} else {
					// Para perguntas do histÃ³rico, renderiza com o ID recebido
					answeredQuestions.add(requestedQuestionId);

					// Se for do histÃ³rico, atualiza o flag tambÃ©m
					if (requestedQuestionId !== CURRENT_QUESTION_ID) {
						try {
							const q = questionsHistory.find(x => x.id === requestedQuestionId);
							if (q) {
								q.answered = true;
								renderQuestionsHistory();
							}
						} catch (err) {
							console.warn('âš ï¸ falha ao marcar pergunta como respondida:', err);
						}
					}
				}
			}

			// Resete o estado da pergunta atual se ainda for CURRENT
			resetCurrentQuestion();
			resetInterviewTurnState();

			// ğŸ”¥ Notificar config-manager que stream terminou (para limpar info de streaming)
			globalThis.RendererAPI?.emitUIChange?.('onAnswerStreamEnd', {});
		};

		ipcRenderer.on('GPT_STREAM_CHUNK', onChunk);
		ipcRenderer.once('GPT_STREAM_END', onEnd);
		return;
	}

	// ğŸ”µ MODO NORMAL â€” BATCH
	console.log('â³ enviando para o GPT (batch)...');
	const res = await ipcRenderer.invoke('ask-gpt', [
		{ role: 'system', content: SYSTEM_PROMPT },
		{ role: 'user', content: text },
	]);

	console.log('âœ… resposta do GPT recebida (batch): ', res);

	// Finaliza mediÃ§Ãµes
	transcriptionMetrics.gptEndTime = Date.now();
	transcriptionMetrics.totalTime = Date.now() - transcriptionMetrics.audioStartTime;

	// Log mÃ©tricas
	logTranscriptionMetrics();

	const wasRequestedForThisTurn = gptRequestedTurnId === interviewTurnId;

	// ğŸ”’ FECHAMENTO ATÃ”MICO DO CICLO
	if (isCurrent && wasRequestedForThisTurn) {
		promoteCurrentToHistory(text);
		// apÃ³s promover para o histÃ³rico, a pergunta jÃ¡ estÃ¡ no histÃ³rico e resposta vinculada
		try {
			// Encontra a Ãºltima pergunta adicionada (que acabamos de promover)
			const q = questionsHistory[questionsHistory.length - 1];
			if (q) {
				q.answered = true;
				renderQuestionsHistory();
			}
		} catch (err) {
			console.warn('âš ï¸ falha ao marcar pergunta como respondida (batch):', err);
		}
	}

	// marca que o GPT respondeu esse turno (batch)
	gptAnsweredTurnId = interviewTurnId;
	gptRequestedTurnId = null;

	debugLogRenderer('Fim da funÃ§Ã£o: "askGpt"');
}

/* =============================== */
//	FECHAMENTO DE PERGUNTAS
/* =============================== */

// Finaliza a pergunta atual para histÃ³rico.
function finalizeCurrentQuestion() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "finalizeCurrentQuestion"');

	// Se nÃ£o hÃ¡ texto, ignorar
	if (!currentQuestion.text || !currentQuestion.text.trim()) {
		console.log('âš ï¸ finalizeCurrentQuestion: Sem texto para finalizar');
		return;
	}

	// ğŸ”’ GUARDA ABSOLUTA: Se a pergunta jÃ¡ foi finalizada, NÃƒO faÃ§a nada.
	if (currentQuestion.finalized) {
		console.log('â›” finalizeCurrentQuestion ignorado â€” pergunta jÃ¡ finalizada');
		return;
	}

	// âš ï¸ No modo entrevista, NÃƒO abortar o fechamento
	if (ModeController.isInterviewMode()) {
		currentQuestion.text = finalizeQuestion(currentQuestion.text);
		currentQuestion.lastUpdateTime = Date.now();
		currentQuestion.finalized = true;

		// garante seleÃ§Ã£o lÃ³gica
		selectedQuestionId = CURRENT_QUESTION_ID;

		// chama GPT automaticamente se ainda nÃ£o respondeu este turno
		if (gptRequestedTurnId !== interviewTurnId && gptAnsweredTurnId !== interviewTurnId) {
			askGpt();
		}

		return;
	}

	//  âš ï¸ No modo normal - trata perguntas que parecem incompletas
	if (!ModeController.isInterviewMode()) {
		console.log('âš ï¸ No modo normal detectado â€” promovendo ao histÃ³rico sem chamar GPT:', currentQuestion.text);

		// promoteCurrentToHistory(currentQuestion.text);
		const newId = String(questionsHistory.length + 1);
		questionsHistory.push({
			id: newId,
			text: currentQuestion.text,
			createdAt: currentQuestion.createdAt || Date.now(),
			lastUpdateTime: currentQuestion.lastUpdateTime || currentQuestion.createdAt || Date.now(),
		});

		selectedQuestionId = newId;
		resetCurrentQuestion();
		renderQuestionsHistory();

		return;
	}
}

// ForÃ§a o fechamento da pergunta atual, promovendo-a ao histÃ³rico  (Antigo)
function closeCurrentQuestionForced() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "closeCurrentQuestionForced"');

	// log temporario para testar a aplicaÃ§Ã£o sÃ³ remover depois
	console.log('ğŸšª Fechando pergunta:', currentQuestion.text);

	resetInterviewTurnState();

	if (!currentQuestion.text) return;

	questionsHistory.push({
		id: crypto.randomUUID(),
		text: finalizeQuestion(currentQuestion.text),
		createdAt: currentQuestion.createdAt || Date.now(),
	});

	currentQuestion.text = '';
	selectedQuestionId = null; // ğŸ‘ˆ libera seleÃ§Ã£o
	renderQuestionsHistory();
	renderCurrentQuestion();

	debugLogRenderer('Fim da funÃ§Ã£o: "closeCurrentQuestionForced"');
}

/* =============================== */
//	SCREENSHOT CAPTURE - FUNÃ‡Ã•ES
/* =============================== */

// Captura screenshot discretamente e armazena em memÃ³ria
async function captureScreenshot() {
	if (isCapturing) {
		console.log('â³ Captura jÃ¡ em andamento...');
		return;
	}

	isCapturing = true;
	updateStatusMessage('ğŸ“¸ Capturando tela...');

	try {
		const result = await ipcRenderer.invoke('CAPTURE_SCREENSHOT');

		if (!result.success) {
			console.warn('âš ï¸ Falha na captura:', result.error);
			updateStatusMessage(`âŒ ${result.error}`);
			emitUIChange('onScreenshotBadgeUpdate', {
				count: capturedScreenshots.length,
				visible: capturedScreenshots.length > 0,
			});
			return;
		}

		// âœ… Armazena referÃªncia do screenshot
		capturedScreenshots.push({
			filepath: result.filepath,
			filename: result.filename,
			timestamp: result.timestamp,
			size: result.size,
		});

		console.log(`âœ… Screenshot capturado: ${result.filename}`);
		console.log(`ğŸ“¦ Total em memÃ³ria: ${capturedScreenshots.length}`);

		// Atualiza UI
		updateStatusMessage(`âœ… ${capturedScreenshots.length} screenshot(s) capturado(s)`);
		emitUIChange('onScreenshotBadgeUpdate', {
			count: capturedScreenshots.length,
			visible: true,
		});
	} catch (error) {
		console.error('âŒ Erro ao capturar screenshot:', error);
		updateStatusMessage('âŒ Erro na captura');
	} finally {
		isCapturing = false;
	}
}

// Envia screenshots para anÃ¡lise com OpenAI Vision
async function analyzeScreenshots() {
	if (isAnalyzing) {
		console.log('â³ AnÃ¡lise jÃ¡ em andamento...');
		return;
	}

	if (capturedScreenshots.length === 0) {
		console.warn('âš ï¸ Nenhum screenshot para analisar');
		updateStatusMessage('âš ï¸ Nenhum screenshot para analisar (capture com Ctrl+Shift+F)');
		return;
	}

	isAnalyzing = true;
	updateStatusMessage(`ğŸ” Analisando ${capturedScreenshots.length} screenshot(s)...`);

	try {
		// Extrai caminhos dos arquivos
		const filepaths = capturedScreenshots.map(s => s.filepath);

		console.log('ğŸš€ Enviando para anÃ¡lise:', filepaths);

		// Envia para main.js
		const result = await ipcRenderer.invoke('ANALYZE_SCREENSHOTS', filepaths);

		if (!result.success) {
			console.error('âŒ Falha na anÃ¡lise:', result.error);
			updateStatusMessage(`âŒ ${result.error}`);
			return;
		}

		// âœ… Renderiza resposta do GPT
		const questionText = `ğŸ“¸ AnÃ¡lise de ${capturedScreenshots.length} screenshot(s)`;
		// ğŸ”¢ USA ID SEQUENCIAL COMO AS PERGUNTAS NORMAIS (nÃ£o UUID)
		const questionId = String(questionsHistory.length + 1);

		// Adiciona "pergunta" ao histÃ³rico ANTES de renderizar respostas
		questionsHistory.push({
			id: questionId,
			text: questionText,
			createdAt: Date.now(),
			lastUpdateTime: Date.now(),
			answered: true,
		});

		// âœ… MARCA COMO RESPONDIDA (importante para clique nÃ£o gerar duplicata)
		answeredQuestions.add(questionId);

		renderQuestionsHistory();

		// âœ… RENDERIZA VIA STREAMING (fluxo real) - usa onAnswerStreamChunk como GPT normal
		// Divide anÃ¡lise em tokens e emite como se fosse stream
		const analysisText = result.analysis;
		const tokens = analysisText.split(/(\s+|[.,!?;:\-\(\)\[\]{}\n])/g).filter(t => t.length > 0);

		console.log(`ğŸ“¸ [ANÃLISE] Simulando stream: ${tokens.length} tokens`);

		// Emite tokens assim como o GPT faz (permite UI renderizar em tempo real)
		let accumulated = '';
		for (const token of tokens) {
			accumulated += token;

			// âœ… USA O MESMO EVENTO onAnswerStreamChunk (fluxo real)
			emitUIChange('onAnswerStreamChunk', {
				questionId: questionId,
				token: token,
				accum: accumulated,
			});

			// Pequeno delay entre tokens para simular streaming real
			await new Promise(resolve => setTimeout(resolve, 2));
		}

		console.log('âœ… AnÃ¡lise concluÃ­da e renderizada');
		updateStatusMessage('âœ… AnÃ¡lise concluÃ­da');

		// ğŸ—‘ï¸ Limpa screenshots apÃ³s anÃ¡lise
		console.log(`ğŸ—‘ï¸ Limpando ${capturedScreenshots.length} screenshot(s) da memÃ³ria...`);
		capturedScreenshots = [];

		// Atualiza badge
		emitUIChange('onScreenshotBadgeUpdate', {
			count: 0,
			visible: false,
		});

		// ForÃ§a limpeza no sistema
		await ipcRenderer.invoke('CLEANUP_SCREENSHOTS');
	} catch (error) {
		console.error('âŒ Erro ao analisar screenshots:', error);
		updateStatusMessage('âŒ Erro na anÃ¡lise');
	} finally {
		isAnalyzing = false;
	}
}

// Limpa todos os screenshots armazenados
function clearScreenshots() {
	if (capturedScreenshots.length === 0) return;

	console.log(`ğŸ—‘ï¸ Limpando ${capturedScreenshots.length} screenshot(s)...`);
	capturedScreenshots = [];

	updateStatusMessage('âœ… Screenshots limpos');
	emitUIChange('onScreenshotBadgeUpdate', {
		count: 0,
		visible: false,
	});

	// ForÃ§a limpeza no sistema
	ipcRenderer.invoke('CLEANUP_SCREENSHOTS').catch(err => {
		console.warn('âš ï¸ Erro na limpeza:', err);
	});
}

/* =============================== */
// ğŸ§¹ RESET COMPLETO DO APP
/* =============================== */

// Reseta todo o estado do app
async function resetAppState() {
	console.log('ğŸ§¹ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
	console.log('ğŸ§¹ INICIANDO RESET COMPLETO DO APP');
	console.log('ğŸ§¹ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');

	try {
		// 1ï¸âƒ£ PARAR AUTOPLAY DO MOCK (prevent async operations)
		mockAutoPlayActive = false;
		mockScenarioIndex = 0;
		console.log('âœ… Autoplay do mock parado');

		// 2ï¸âƒ£ PARAR ÃUDIO IMEDIATAMENTE (input/output)
		if (isRunning) {
			console.log('ğŸ¤ Parando captura de Ã¡udio...');
			isRunning = false;
		}

		// 3ï¸âƒ£ RESET DE ESTADO DE ÃUDIO
		inputSpeaking = false;
		outputSpeaking = false;
		console.log('âœ… Estado de Ã¡udio resetado');

		// 4ï¸âƒ£ LIMPAR CHUNKS DE ÃUDIO
		inputChunks = [];
		outputChunks = [];
		inputPartialChunks = [];
		outputPartialChunks = [];
		outputPartialText = '';
		voskAccumulatedText = '';
		console.log('âœ… Chunks de Ã¡udio limpos');

		// 5ï¸âƒ£ LIMPAR TIMERS DE ÃUDIO
		if (inputSilenceTimer) {
			clearTimeout(inputSilenceTimer);
			inputSilenceTimer = null;
		}
		if (outputSilenceTimer) {
			clearTimeout(outputSilenceTimer);
			outputSilenceTimer = null;
		}
		if (inputPartialTimer) {
			clearTimeout(inputPartialTimer);
			inputPartialTimer = null;
		}
		if (outputPartialTimer) {
			clearTimeout(outputPartialTimer);
			outputPartialTimer = null;
		}
		if (voskPartialTimer) {
			clearTimeout(voskPartialTimer);
			voskPartialTimer = null;
		}
		if (autoCloseQuestionTimer) {
			clearTimeout(autoCloseQuestionTimer);
			autoCloseQuestionTimer = null;
		}
		console.log('âœ… Timers limpos');

		// 6ï¸âƒ£ LIMPAR PERGUNTAS E RESPOSTAS
		currentQuestion = {
			text: '',
			lastUpdate: 0,
			finalized: false,
			lastUpdateTime: null,
			createdAt: null,
			finalText: '',
			interimText: '',
		};
		questionsHistory = [];
		answeredQuestions.clear();
		selectedQuestionId = null;
		lastSentQuestionText = '';
		lastAskedQuestionNormalized = null;
		console.log('âœ… Perguntas e respostas limpas');

		// 7ï¸âƒ£ LIMPAR ESTADO GPT/ENTREVISTA
		interviewTurnId = 0;
		gptAnsweredTurnId = null;
		gptRequestedTurnId = null;
		gptRequestedQuestionId = null;
		console.log('âœ… Estado de entrevista resetado');

		// 8ï¸âƒ£ LIMPAR PLACEHOLDERS
		lastInputStartAt = null;
		lastInputStopAt = null;
		lastOutputStartAt = null;
		lastOutputStopAt = null;
		pendingOutputStartAt = null;
		pendingOutputStopAt = null;
		lastPartialSttAt = null;
		lastOutputPlaceholderId = null;
		lastInputPlaceholderEl = null;
		lastOutputPlaceholderEl = null;
		console.log('âœ… Placeholders limpos');

		// 9ï¸âƒ£ RESETAR MÃ‰TRICAS
		transcriptionMetrics = {
			audioStartTime: null,
			gptStartTime: null,
			gptEndTime: null,
			totalTime: null,
			audioSize: 0,
		};
		console.log('âœ… MÃ©tricas resetadas');

		// ğŸ”Ÿ LIMPAR SCREENSHOTS (sem chamar API!)
		if (capturedScreenshots.length > 0) {
			console.log(`ğŸ—‘ï¸ Limpando ${capturedScreenshots.length} screenshot(s)...`);
			capturedScreenshots = [];
			emitUIChange('onScreenshotBadgeUpdate', {
				count: 0,
				visible: false,
			});
			// ForÃ§a limpeza no sistema
			try {
				await ipcRenderer.invoke('CLEANUP_SCREENSHOTS');
			} catch (err) {
				console.warn('âš ï¸ Erro ao limpar screenshots no sistema:', err);
			}
		}
		console.log('âœ… Screenshots limpos');

		// 1ï¸âƒ£1ï¸âƒ£ LIMPAR FLAGS
		isCapturing = false;
		isAnalyzing = false;
		console.log('âœ… Flags resetadas');

		// 1ï¸âƒ£2ï¸âƒ£ ATUALIZAR UI - PERGUNTAS
		emitUIChange('onCurrentQuestionUpdate', {
			text: '',
			isSelected: false,
		});
		emitUIChange('onQuestionsHistoryUpdate', []);
		console.log('âœ… Perguntas UI limpa');

		// 1ï¸âƒ£3ï¸âƒ£ ATUALIZAR UI - TRANSCRIÃ‡Ã•ES E RESPOSTAS
		emitUIChange('onTranscriptionCleared');
		emitUIChange('onAnswersCleared');
		console.log('âœ… TranscriÃ§Ãµes e respostas UI limpas');

		// 1ï¸âƒ£4ï¸âƒ£ ATUALIZAR UI - BOTÃƒO LISTEN
		emitUIChange('onListenButtonToggle', {
			isRunning: false,
			buttonText: 'ğŸ¤ ComeÃ§ar a Ouvir... (Ctrl+D)',
		});
		console.log('âœ… BotÃ£o listen resetado');

		// 1ï¸âƒ£5ï¸âƒ£ ATUALIZAR UI - STATUS
		emitUIChange('onStatusUpdate', {
			status: 'ready',
			message: 'âœ… Pronto',
		});
		console.log('âœ… Status atualizado');

		// 1ï¸âƒ£6ï¸âƒ£ LIMPAR SELEÃ‡Ã•ES
		clearAllSelections();
		console.log('âœ… SeleÃ§Ãµes limpas');

		// 1ï¸âƒ£7ï¸âƒ£ LOG FINAL
		console.log('âœ… â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
		console.log('âœ… RESET COMPLETO CONCLUÃDO COM SUCESSO');
		console.log('âœ… â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');

		return true;
	} catch (error) {
		console.error('âŒ Erro ao resetar app:', error);
		return false;
	}
}

// Reseta o estado especÃ­fico do turno de entrevista atual
function resetInterviewTurnState() {
	// Limpa apenas o output parcial desta volta especÃ­fica
	outputPartialText = '';
	outputPartialChunks = [];
	// NÃ£o limpa lastAskedQuestionNormalized aqui - mantÃ©m para evitar duplicatas
}

/* =============================== */
//   EXPORTAÃ‡Ã•ES PÃšBLICAS
/* =============================== */

// Exporta funÃ§Ãµes pÃºblicas que podem ser chamadas de fora
const RendererAPI = {
	// Ãudio - GravaÃ§Ã£o
	// startInput,
	// stopInput,
	// stopInputMonitor,
	listenToggleBtn,
	askGpt,
	// startOutput,
	// stopOutput,
	// stopOutputMonitor,
	restartAudioPipeline,

	// Ãudio - Monitoramento de volume
	startInputVolumeMonitoring,
	startOutputVolumeMonitoring,
	stopInputVolumeMonitoring,
	stopOutputVolumeMonitoring,
	// Entrevista - Reset (centralizado em resetAppState)
	resetAppState,

	// Modo
	changeMode: mode => {
		CURRENT_MODE = mode;
	},
	getMode: () => CURRENT_MODE,

	// Questions
	handleCurrentQuestion,
	handleQuestionClick,

	// UI
	applyOpacity,
	updateMockBadge: show => {
		emitUIChange('onMockBadgeUpdate', { visible: show });
	},
	setMockToggle: checked => {
		if (UIElements.mockToggle) {
			UIElements.mockToggle.checked = checked;
		}
		APP_CONFIG.MODE_DEBUG = checked;
	},
	setModeSelect: mode => {
		emitUIChange('onModeSelectUpdate', { mode });
	},

	// Drag
	initDragHandle: (dragHandle, documentElement) => {
		if (!dragHandle) return;
		const doc = documentElement || document; // fallback para document global
		dragHandle.addEventListener('pointerdown', async event => {
			console.log('ğŸªŸ Drag iniciado (pointerdown)');
			isDraggingWindow = true;
			dragHandle.classList.add('drag-active');

			const _pid = event.pointerId;
			try {
				dragHandle.setPointerCapture && dragHandle.setPointerCapture(_pid);
			} catch (err) {
				console.warn('setPointerCapture falhou:', err);
			}

			setTimeout(() => ipcRenderer.send('START_WINDOW_DRAG'), 40);

			const startBounds = (await ipcRenderer.invoke('GET_WINDOW_BOUNDS')) || {
				x: 0,
				y: 0,
			};
			const startCursor = { x: event.screenX, y: event.screenY };
			let lastAnimation = 0;

			function onPointerMove(ev) {
				const now = performance.now();
				if (now - lastAnimation < 16) return;
				lastAnimation = now;

				const dx = ev.screenX - startCursor.x;
				const dy = ev.screenY - startCursor.y;

				ipcRenderer.send('MOVE_WINDOW_TO', {
					x: startBounds.x + dx,
					y: startBounds.y + dy,
				});
			}

			function onPointerUp(ev) {
				try {
					dragHandle.removeEventListener('pointermove', onPointerMove);
					dragHandle.removeEventListener('pointerup', onPointerUp);
				} catch (err) {}

				if (dragHandle.classList.contains('drag-active')) {
					dragHandle.classList.remove('drag-active');
				}

				try {
					dragHandle.releasePointerCapture && dragHandle.releasePointerCapture(_pid);
				} catch (err) {}

				isDraggingWindow = false;
			}

			dragHandle.addEventListener('pointermove', onPointerMove);
			dragHandle.addEventListener('pointerup', onPointerUp, { once: true });
			event.stopPropagation();
		});

		doc.addEventListener('pointerup', () => {
			if (!dragHandle.classList.contains('drag-active')) return;
			console.log('ğŸªŸ Drag finalizado (pointerup)');
			dragHandle.classList.remove('drag-active');
			isDraggingWindow = false;
		});

		dragHandle.addEventListener('pointercancel', () => {
			if (dragHandle.classList.contains('drag-active')) {
				dragHandle.classList.remove('drag-active');
				isDraggingWindow = false;
			}
		});
	},

	// Click-through
	setClickThrough: enabled => {
		ipcRenderer.send('SET_CLICK_THROUGH', enabled);
	},
	updateClickThroughButton: (enabled, btnToggle) => {
		if (!btnToggle) return;
		btnToggle.style.opacity = enabled ? '0.5' : '1';
		btnToggle.title = enabled
			? 'Click-through ATIVO (clique para desativar)'
			: 'Click-through INATIVO (clique para ativar)';
		console.log('ğŸ¨ BotÃ£o atualizado - opacity:', btnToggle.style.opacity);
	},

	// UI Registration
	registerUIElements: elements => {
		registerUIElements(elements);
	},
	onUIChange: (eventName, callback) => {
		onUIChange(eventName, callback);
	},
	// Emit UI changes (para config-manager enviar eventos para renderer)
	emitUIChange,

	// API Key
	setAppConfig: config => {
		APP_CONFIG = config;
	},
	getAppConfig: () => APP_CONFIG,

	// Navegacao de perguntas (Ctrl+Shift+ArrowUp/Down via globalShortcut IPC)
	navigateQuestions: direction => {
		const all = getNavigableQuestionIds();
		if (all.length === 0) return;

		let index = all.indexOf(selectedQuestionId);
		if (index === -1) {
			index = direction === 'up' ? all.length - 1 : 0;
		} else {
			index += direction === 'up' ? -1 : 1;
			index = Math.max(0, Math.min(index, all.length - 1));
		}

		selectedQuestionId = all[index];
		clearAllSelections();
		renderQuestionsHistory();
		renderCurrentQuestion();

		if (APP_CONFIG.MODE_DEBUG) {
			const msg = direction === 'up' ? 'ğŸ§ª Ctrl+ArrowUp detectado (teste)' : 'ğŸ§ª Ctrl+ArrowDown detectado (teste)';
			updateStatusMessage(msg);
			console.log('ğŸ“Œ Atalho Selecionou:', selectedQuestionId);
		}
	},

	// IPC Listeners
	onApiKeyUpdated: callback => {
		ipcRenderer.on('API_KEY_UPDATED', callback);
	},
	onToggleAudio: callback => {
		// ComeÃ§ar a ouvir / Parar de ouvir (Ctrl+D)
		ipcRenderer.on('CMD_TOGGLE_AUDIO', callback);
	},
	onAskGpt: callback => {
		ipcRenderer.on('CMD_ASK_GPT', callback);
	},
	onGptStreamChunk: callback => {
		ipcRenderer.on('GPT_STREAM_CHUNK', callback);
	},
	onGptStreamEnd: callback => {
		ipcRenderer.on('GPT_STREAM_END', callback);
	},
	sendRendererError: error => {
		try {
			console.error('RENDERER ERROR', error.error || error.message || error);
			ipcRenderer.send('RENDERER_ERROR', {
				message: String(error.message || error),
				stack: error.error?.stack || null,
			});
		} catch (err) {
			console.error('Falha ao enviar RENDERER_ERROR', err);
		}
	},

	// ğŸ“¸ NOVO: Screenshot functions
	captureScreenshot,
	analyzeScreenshots,
	clearScreenshots,
	getScreenshotCount: () => capturedScreenshots.length,

	// ğŸ“¸ NOVO: Screenshot shortcuts
	onCaptureScreenshot: callback => {
		ipcRenderer.on('CMD_CAPTURE_SCREENSHOT', callback);
	},
	onAnalyzeScreenshots: callback => {
		ipcRenderer.on('CMD_ANALYZE_SCREENSHOTS', callback);
	},
	// Navegacao de perguntas (Ctrl+Shift+ArrowUp/Down via globalShortcut)
	onNavigateQuestions: callback => {
		ipcRenderer.on('CMD_NAVIGATE_QUESTIONS', (_, direction) => {
			callback(direction);
		});
	},
};
if (typeof module !== 'undefined' && module.exports) {
	// Node.js / CommonJS export
	module.exports = RendererAPI;
}

// ğŸ­ Exporta para o escopo global (usado em mocks e testes)
if (typeof globalThis !== 'undefined') {
	globalThis.RendererAPI = RendererAPI; // ğŸ­ Exporta API para escopo global
	globalThis.runMockAutoPlay = runMockAutoPlay; // ğŸ­ Exportar Mock autoplay
	globalThis.mockScenarioIndex = 0; // ğŸ­ Ãndice global para cenÃ¡rios
	globalThis.mockAutoPlayActive = false; // ğŸ­ Flag global para evitar mÃºltiplas execuÃ§Ãµes
}

/* =============================== */
// FUNÃ‡Ã•ES PARA LOGAR
/* =============================== */

// Log de debug padronizado para renderer. Ãšltimo argumento opcional Ã© booleano para mostrar ou nÃ£o o log
function debugLogRenderer(...args) {
	const maybeFlag = args.at(-1);
	const showLog = typeof maybeFlag === 'boolean' ? maybeFlag : false;

	const nowLog = new Date();
	const timeStr =
		`${nowLog.getHours().toString().padStart(2, '0')}:` +
		`${nowLog.getMinutes().toString().padStart(2, '0')}:` +
		`${nowLog.getSeconds().toString().padStart(2, '0')}.` +
		`${nowLog.getMilliseconds().toString().padStart(3, '0')}`;

	if (showLog) {
		const cleanArgs = typeof maybeFlag === 'boolean' ? args.slice(0, -1) : args;
		// prettier-ignore
		console.log(
			`%câ±ï¸ [${timeStr}] ğŸª² â¯â¯â¯â¯ Debug em renderer.js:`,
			'color: brown; font-weight: bold;', 
			...cleanArgs
		);
	}
}

// Log detalhado das mÃ©tricas de tempo da transcriÃ§Ã£o
function logTranscriptionMetrics() {
	if (!transcriptionMetrics.audioStartTime) return;

	const gptTime = transcriptionMetrics.gptEndTime - transcriptionMetrics.gptStartTime;
	const totalTime = transcriptionMetrics.totalTime;

	console.log(`ğŸ“Š ================================`);
	console.log(`ğŸ“Š MÃ‰TRICAS DE TEMPO DETALHADAS:`);
	console.log(`ğŸ“Š ================================`);
	console.log(`ğŸ“Š TAMANHO ÃUDIO: ${transcriptionMetrics.audioSize} bytes`);
	console.log(`ğŸ“Š GPT: ${gptTime}ms`);
	console.log(`ğŸ“Š TOTAL: ${totalTime}ms`);
	console.log(`ğŸ“Š GPT % DO TOTAL: ${Math.round((gptTime / totalTime) * 100)}%`);
	console.log(`ğŸ“Š ================================`);

	// Reset para prÃ³xima mediÃ§Ã£o
	transcriptionMetrics = {
		audioStartTime: null,
		gptStartTime: null,
		gptEndTime: null,
		totalTime: null,
		audioSize: 0,
	};
}

/* =============================== */
//	RESET COMPLETO (TEMPORÃRIO PARA TESTES)
/* =============================== */

// FunÃ§Ã£o acionada pelo botÃ£o de reset na UI
function resetHomeSection() {
	console.log('\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
	console.log('ğŸ”„ RESET COMPLETO ACIONADO PELO BOTÃƒO resetHomeBtn');
	console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');

	// ğŸ”¥ Usar a funÃ§Ã£o centralizada de reset
	resetAppState().then(success => {
		if (success) {
			console.log('âœ… Reset via resetAppState() concluÃ­do com sucesso!');
		} else {
			console.error('âŒ Erro ao executar resetAppState()');
		}
		console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');
	});
}

// Adiciona listener ao botÃ£o de reset apÃ³s o DOM carregar
document.addEventListener('DOMContentLoaded', () => {
	const resetBtn = document.getElementById('resetHomeBtn');
	if (resetBtn) {
		resetBtn.addEventListener('click', () => {
			const confirmed = confirm('âš ï¸ Isso vai limpar toda transcriÃ§Ã£o, histÃ³rico e respostas.\n\nTem certeza?');
			if (confirmed) {
				resetHomeSection();
			}
		});
		console.log('âœ… Listener do botÃ£o reset instalado');
	} else {
		console.warn('âš ï¸ BotÃ£o reset nÃ£o encontrado no DOM');
	}
});

/* =============================== */
//	ğŸ­  MOCK / DEBUGGING FUNCTIONS
/* =============================== */

// ğŸ” Respostas mockadas por pergunta
const MOCK_RESPONSES = {
	'Mock - O que Ã© JVM e para que serve?':
		'Mock - A JVM (Java Virtual Machine) Ã© uma mÃ¡quina virtual que executa bytecode Java. Ela permite que programas Java rodem em qualquer plataforma sem modificaÃ§Ã£o. A JVM gerencia memÃ³ria, garbage collection e fornece um ambiente isolado e seguro para execuÃ§Ã£o de cÃ³digo.',
	'Mock - Qual a diferenÃ§a entre JDK e JRE?':
		'Mock - JDK (Java Development Kit) Ã© o kit completo para desenvolvimento, incluindo compilador, ferramentas e bibliotecas. JRE (Java Runtime Environment) contÃ©m apenas o necessÃ¡rio para executar aplicaÃ§Ãµes Java compiladas. Todo desenvolvedor precisa do JDK, mas usuÃ¡rios finais precisam apenas da JRE.',
	'Mock - O que Ã© uma classe em Java?':
		'Mock - Uma classe Ã© o molde ou blueprint para criar objetos. Define atributos (propriedades) e mÃ©todos (comportamentos). As classes sÃ£o fundamentais na programaÃ§Ã£o orientada a objetos. Por exemplo, uma classe Carro pode ter atributos como cor e velocidade, e mÃ©todos como acelerar e frear.',
	'Mock - Explique sobre heranÃ§a em Java':
		'Mock - HeranÃ§a permite que uma classe herde propriedades e mÃ©todos de outra classe. A classe filha estende a classe pai usando a palavra-chave extends. Isso promove reutilizaÃ§Ã£o de cÃ³digo e cria uma hierarquia de classes. Por exemplo, a classe Bicicleta pode herdar de Veiculo.',
	'Mock - Como funciona polimorfismo?':
		'Mock - Polimorfismo significa muitas formas. Permite que objetos de diferentes tipos respondam a mesma chamada de mÃ©todo de forma diferente. Pode ser atravÃ©s de sobrescrita de mÃ©todos (heranÃ§a) ou interface. Exemplo: diferentes animais implementam o mÃ©todo fazer_som() diferentemente.',
	'Mock - O que Ã© encapsulamento?':
		'Mock - Encapsulamento Ã© o princÃ­pio de ocultar detalhes internos da implementaÃ§Ã£o. Usa modificadores de acesso como private, protected e public. Protege dados e mÃ©todos crÃ­ticos, permitindo controle sobre como sÃ£o acessados. Ã‰ uma pilar da seguranÃ§a e manutenÃ§Ã£o do cÃ³digo orientado a objetos.',
};

// ğŸ¬ CenÃ¡rios automÃ¡ticos para teste. (screenshotsCount: 0 = sem screenshot, 1 = tira 1 foto, 2 = tira 2 fotos, etc)
const MOCK_SCENARIOS = [
	{ question: 'Mock - O que Ã© JVM e para que serve?', screenshotsCount: 1 },
	{ question: 'Mock - Qual a diferenÃ§a entre JDK e JRE?', screenshotsCount: 0 },
	{ question: 'Mock - O que Ã© uma classe em Java?', screenshotsCount: 0 },
	{ question: 'Mock - Explique sobre heranÃ§a em Java', screenshotsCount: 2 },
	{ question: 'Mock - Como funciona polimorfismo?', screenshotsCount: 0 },
	{ question: 'Mock - O que Ã© encapsulamento?', screenshotsCount: 0 },
];

let mockScenarioIndex = 0;
let mockAutoPlayActive = false;

//	ğŸ­ Retorna resposta mockada para pergunta (busca exata ou parcial)
function getMockResponse(question) {
	// Match exato
	if (MOCK_RESPONSES[question]) {
		return MOCK_RESPONSES[question];
	}

	// Match parcial
	for (const [key, value] of Object.entries(MOCK_RESPONSES)) {
		if (question.toLowerCase().includes(key.toLowerCase())) {
			return value;
		}
	}

	// Fallback
	return `Resposta mockada para: "${question}"\n\nEste Ã© um teste do sistema em modo Mock.`;
}

// ğŸ­ Intercepta chamadas IPC para MOCK quando APP_CONFIG.MODE_DEBUG estÃ¡ ativo
const originalInvoke = ipcRenderer.invoke;
ipcRenderer.invoke = function (channel, ...args) {
	// Intercepta anÃ¡lise de screenshots quando MODE_DEBUG
	// IMPORTANTE: CAPTURE_SCREENSHOT Ã© REAL (tira foto mesmo), ANALYZE_SCREENSHOTS Ã© MOCK (simula resposta)
	if (channel === 'ANALYZE_SCREENSHOTS' && APP_CONFIG.MODE_DEBUG) {
		console.log('ğŸ“¸ [MOCK] Interceptando ANALYZE_SCREENSHOTS...');
		const filepaths = args[0] || [];
		const screenshotCount = filepaths.length;

		// Retorna anÃ¡lise mockada
		const mockAnalysis = `
		## ğŸ“¸ AnÃ¡lise de ${screenshotCount} Screenshot(s) - MOCK

		### Esta Ã© uma resposta simulada para o teste do sistema.

		Para resolver o problema apresentado na captura de tela, que Ã© o "Remove Element" do LeetCode, vamos implementar uma funÃ§Ã£o em Java que remove todas as ocorrÃªncias de um valor especÃ­fico de um array. A funÃ§Ã£o deve modificar o array in-place e retornar o novo comprimento do array.

		Resumo do Problema
		Entrada: Um array de inteiros nums e um inteiro val que queremos remover.
		SaÃ­da: O novo comprimento do array apÃ³s remover todas as ocorrÃªncias de val.
		Passos para a SoluÃ§Ã£o
		Iterar pelo array: Vamos percorrer o array e verificar cada elemento.
		Manter um Ã­ndice: Usaremos um Ã­ndice para rastrear a posiÃ§Ã£o onde devemos colocar os elementos que nÃ£o sÃ£o iguais a val.
		Modificar o array in-place: Sempre que encontrarmos um elemento que nÃ£o Ã© igual a val, colocamos esse elemento na posiÃ§Ã£o do Ã­ndice e incrementamos o Ã­ndice.
		Retornar o comprimento: No final, o Ã­ndice representarÃ¡ o novo comprimento do array.
		ImplementaÃ§Ã£o do CÃ³digo
		Aqui estÃ¡ a implementaÃ§Ã£o em Java:

		class Solution {
			public int removeElement(int[] nums, int val) {
				// Inicializa um Ã­ndice para rastrear a nova posiÃ§Ã£o
				int index = 0;

				// Percorre todos os elementos do array
				for (int i = 0; i &lt; nums.length; i++) {
					// Se o elemento atual nÃ£o Ã© igual a val
					if (nums[i] != val) {
						// Coloca o elemento na posiÃ§Ã£o do Ã­ndice
						nums[index] = nums[i];
						// Incrementa o Ã­ndice
						index++;
					}
				}

				// Retorna o novo comprimento do array
				return index;
			}
		}

		ExplicaÃ§Ã£o do CÃ³digo
		Classe e MÃ©todo: Criamos uma classe chamada Solution e um mÃ©todo removeElement que recebe um array de inteiros nums e um inteiro val.
		Ãndice Inicial: Inicializamos uma variÃ¡vel index em 0.
		`;

		return Promise.resolve({
			success: true,
			analysis: mockAnalysis,
			filesAnalyzed: screenshotCount,
			timestamp: Date.now(),
		});
	}

	// Intercepta ask-gpt-stream quando MODE_DEBUG
	if (channel === 'ask-gpt-stream' && APP_CONFIG.MODE_DEBUG) {
		console.log('ğŸ­ [MOCK] Interceptando ask-gpt-stream...');

		// ObtÃ©m a pergunta do primeiro argumento (array de mensagens)
		const messages = args[0] || [];
		const userMessage = messages.find(m => m.role === 'user');
		const questionText = userMessage ? userMessage.content : 'Pergunta desconhecida';

		// Busca resposta mockada
		const mockResponse = getMockResponse(questionText);

		// Divide em tokens (remove vazios)
		const tokens = mockResponse.split(/(\s+|[.,!?;:\-\(\)\[\]{}\n])/g).filter(t => t.length > 0);

		console.log(`ğŸ­ [MOCK] Emitindo ${tokens.length} tokens para pergunta: "${questionText.substring(0, 50)}..."`);

		// FunÃ§Ã£o para emitir tokens com pequeno delay entre eles
		async function emitTokens() {
			let accumulated = '';
			for (let i = 0; i < tokens.length; i++) {
				const token = tokens[i];
				accumulated += token;

				// Emite o evento com delay mÃ­nimo
				await new Promise(resolve => {
					setTimeout(() => {
						// âœ… CORRETO: Emite apenas o token como 2Âº argumento
						ipcRenderer.emit('GPT_STREAM_CHUNK', null, token);
						resolve();
					}, 5); // 5ms entre tokens
				});
			}

			// Sinaliza fim do stream apÃ³s todos os tokens
			await new Promise(resolve => {
				setTimeout(() => {
					ipcRenderer.emit('GPT_STREAM_END');
					resolve();
				}, 10);
			});
		}

		// Inicia emissÃ£o de tokens de forma assÃ­ncrona
		emitTokens().catch(err => {
			console.error('âŒ Erro ao emitir tokens mock:', err);
		});

		// Retorna promise resolvida imediatamente (esperado pela API)
		return Promise.resolve({ success: true });
	}

	// Todas as outras chamadas passam para o invoke real
	return originalInvoke.call(this, channel, ...args);
};

//	ğŸ­ FunÃ§Ã£o de autoplay automÃ¡tico para mockar perguntas e respostas
async function runMockAutoPlay() {
	if (mockAutoPlayActive) return;
	mockAutoPlayActive = true;

	while (mockScenarioIndex < MOCK_SCENARIOS.length && APP_CONFIG.MODE_DEBUG && mockAutoPlayActive) {
		const scenario = MOCK_SCENARIOS[mockScenarioIndex];
		console.log(
			`\nğŸ¬ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nğŸ¬ MOCK CENÃRIO ${mockScenarioIndex + 1}/${
				MOCK_SCENARIOS.length
			}\nğŸ¬ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•`,
		);

		// FASE 1: Simula captura de Ã¡udio (2-4s)
		console.log(`ğŸ¤ [FASE-1] Capturando Ã¡udio da pergunta...`);
		const audioStartTime = Date.now();
		const placeholderId = `placeholder-${audioStartTime}-${Math.random()}`;

		// Emite placeholder
		emitUIChange('onTranscriptAdd', {
			author: 'Outros',
			text: '...',
			timeStr: new Date().toLocaleTimeString(),
			elementId: 'conversation',
			placeholderId: placeholderId,
		});

		// Aguarda captura
		await new Promise(resolve => setTimeout(resolve, 2000 + Math.random() * 2000));

		// ğŸ”¥ CHECK: Se modo debug foi desativado, para imediatamente
		if (!APP_CONFIG.MODE_DEBUG || !mockAutoPlayActive) {
			console.log('ğŸ›‘ [PARADA] Modo debug desativado - parando mock autoplay');
			break;
		}

		const audioEndTime = Date.now();
		console.log(`âœ… [FASE-1] Ãudio capturado`);

		// Calcula latÃªncia (arredonda para inteiro - sem casas decimais)
		const latencyMs = Math.round(800 + Math.random() * 400);
		const totalMs = audioEndTime - audioStartTime + latencyMs;

		// Atualiza placeholder com texto real
		emitUIChange('onPlaceholderFulfill', {
			speaker: 'Outros',
			text: scenario.question,
			startStr: new Date(audioStartTime).toLocaleTimeString(),
			stopStr: new Date(audioEndTime).toLocaleTimeString(),
			recordingDuration: audioEndTime - audioStartTime,
			latency: latencyMs,
			total: totalMs,
			placeholderId: placeholderId,
		});

		// FASE 2: Processa pergunta (handleSpeech + closeCurrentQuestion)
		console.log(`ğŸ“ [FASE-2] Processando pergunta...`);
		//handleSpeech(OTHER, scenario.question, { skipAddToUI: true });

		// Aguarda consolidaÃ§Ã£o (800ms para garantir que pergunta saia do CURRENT)
		await new Promise(resolve => setTimeout(resolve, 800));

		// ğŸ”¥ CHECK: Se modo debug foi desativado, para imediatamente
		if (!APP_CONFIG.MODE_DEBUG || !mockAutoPlayActive) {
			console.log('ğŸ›‘ [PARADA] Modo debug desativado - parando mock autoplay');
			break;
		}

		// Simula silÃªncio e fecha pergunta
		console.log(`ğŸ”‡ [FASE-2] SilÃªncio detectado, fechando pergunta...`);
		//closeCurrentQuestion();

		// FASE 3: askGpt serÃ¡ acionado automaticamente, o interceptor (ask-gpt-stream) que irÃ¡ mockar
		console.log(`ğŸ¤– [FASE-3] askGpt acionado - mock stream serÃ¡ emitido pelo interceptor`);

		// Aguarda stream terminar (~30ms por token)
		const mockResponse = getMockResponse(scenario.question);
		const estimatedTime = mockResponse.length * 30;
		await new Promise(resolve => setTimeout(resolve, estimatedTime + 1000));

		// ğŸ”¥ CHECK: Se modo debug foi desativado, para imediatamente SEM TIRAR SCREENSHOT
		if (!APP_CONFIG.MODE_DEBUG || !mockAutoPlayActive) {
			console.log('ğŸ›‘ [PARADA] Modo debug desativado - parando sem capturar screenshot');
			break;
		}

		// FASE 4 (Opcional): Captura N screenshots REAIS e depois aciona anÃ¡lise
		if (scenario.screenshotsCount && scenario.screenshotsCount > 0) {
			// FASE 4A: Captura mÃºltiplos screenshots
			for (let i = 1; i <= scenario.screenshotsCount; i++) {
				// ğŸ”¥ CHECK: Verifica antes de cada screenshot
				if (!APP_CONFIG.MODE_DEBUG || !mockAutoPlayActive) {
					console.log(
						`ğŸ›‘ [PARADA] Modo debug desativado - cancelando captura de screenshot ${i}/${scenario.screenshotsCount}`,
					);
					break;
				}

				console.log(`ğŸ“¸ [FASE-4A] Capturando screenshot ${i}/${scenario.screenshotsCount} REAL da resposta...`);
				await captureScreenshot();

				// Delay entre mÃºltiplas capturas para respeitar cooldown de 2s do main.js
				if (i < scenario.screenshotsCount) {
					console.log(`   â³ Aguardando 2200ms antes da prÃ³xima captura (cooldown CAPTURE_COOLDOWN)...`);
					await new Promise(resolve => setTimeout(resolve, 2200));
				}
			}

			// ğŸ”¥ CHECK: Verifica antes de anÃ¡lise
			if (!APP_CONFIG.MODE_DEBUG || !mockAutoPlayActive) {
				console.log('ğŸ›‘ [PARADA] Modo debug desativado - cancelando anÃ¡lise de screenshots');
				break;
			}

			// Log de validaÃ§Ã£o: quantas fotos tem antes de analisar
			console.log(
				`ğŸ“¸ [PRÃ‰-ANÃLISE] Total de screenshots em memÃ³ria: ${capturedScreenshots.length}/${scenario.screenshotsCount}`,
			);

			// FASE 4B: AnÃ¡lise dos screenshots capturados
			console.log(`ğŸ“¸ [FASE-4B] Analisando ${scenario.screenshotsCount} screenshot(s)...`);
			await analyzeScreenshots();
		}

		mockScenarioIndex++;

		if (mockScenarioIndex < MOCK_SCENARIOS.length) {
			console.log(`\nâ³ Aguardando 1s antes do prÃ³ximo cenÃ¡rio...\n`);
			await new Promise(resolve => setTimeout(resolve, 1000));
		}
	}

	console.log('âœ… Mock autoplay finalizado');
	mockAutoPlayActive = false;
}

/* =============================== */
//	DISPOSITIVOS / CONTROLE DE ÃUDIO
/* =============================== */

async function stopAudio() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "stopAudio"');

	// Fecha pergunta atual se estava aberta
	if (currentQuestion.text) closeCurrentQuestionForced();

	const sttModel = getConfiguredSTTModel();
	console.log(`ğŸ›‘ stopAudio: Modelo STT = ${sttModel}`);

	try {
		// ğŸ”¥ ROTEAMENTO: Por modelo STT
		if (sttModel === 'deepgram') {
			stopAudioDeepgram();
		} else if (sttModel === 'vosk') {
			stopAudioVosk();
		} else if (sttModel === 'whisper-cpp-local' || sttModel === 'whisper-1') {
			stopAudioWhisper();
		} else {
			// Modelo nÃ£o suportado
			console.error('âŒ Erro ao obter modelo STT configurado');
			return;
		}
	} catch (error) {
		console.error('âŒ Erro em stopAudio:', error);
	}

	debugLogRenderer('Fim da funÃ§Ã£o: "stopAudio"');
}

async function restartAudioPipeline() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "restartAudioPipeline"');

	stopAudio();

	debugLogRenderer('Fim da funÃ§Ã£o: "restartAudioPipeline"');
}
