/* ===============================
   IMPORTS
=============================== */
const { ipcRenderer } = require('electron');
const { marked } = require('marked');
const hljs = require('highlight.js');

// üåä Transcri√ß√£o Deepgram
const { startAudioDeepgram, stopAudioDeepgram, finalizePendingTranscription } = require('./deepgram-transcribe.js');

// üî• Transcri√ß√£o Whisper
const { transcribeWhisperComplete, transcribeWhisperPartial } = require('./whisper-transcribe.js');

// üî• Transcri√ß√£o Vosk
const { transcribeVoskComplete, transcribeVoskPartial } = require('./vosk-transcribe.js');

// üîí DESABILITADO TEMPORARIAMENTE
const DESABILITADO_TEMPORARIAMENTE = false;

// üî• Sistema de eventos para m√≥dulos de transcri√ß√£o (desacoplamento)
window.transcriptionEvents = new EventTarget();

/* ===============================
   üîê PROTE√á√ÉO CONTRA CAPTURA DE TELA EXTERNA
   Desabilita/limita APIs usadas por Zoom, Teams, Meet, OBS, Discord, Snipping Tool, etc.
=============================== */
(function protectAgainstScreenCapture() {
	// ‚úÖ Desabilita getDisplayMedia (usado por Zoom, Meet, Teams para capturar)
	if (navigator && navigator.mediaDevices && navigator.mediaDevices.getDisplayMedia) {
		const originalGetDisplayMedia = navigator.mediaDevices.getDisplayMedia.bind(navigator.mediaDevices);
		navigator.mediaDevices.getDisplayMedia = async function (...args) {
			console.warn('üîê BLOQUEADO: Tentativa de usar getDisplayMedia (captura de tela externa)');
			throw new Error('Screen capture not available in this window');
		};
	}

	// ‚úÖ Desabilita captureStream (usado para captura de janela)
	if (window.HTMLCanvasElement && window.HTMLCanvasElement.prototype.captureStream) {
		Object.defineProperty(window.HTMLCanvasElement.prototype, 'captureStream', {
			value: function () {
				console.warn('üîê BLOQUEADO: Tentativa de usar Canvas.captureStream()');
				throw new Error('Capture stream not available');
			},
			writable: false,
			configurable: false,
		});
	}

	// ‚úÖ Intercepta getUserMedia para avisar sobre tentativas de captura de √°udio (pode ser usado em combo com v√≠deo)
	if (navigator && navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
		const originalGetUserMedia = navigator.mediaDevices.getUserMedia.bind(navigator.mediaDevices);
		navigator.mediaDevices.getUserMedia = async function (constraints) {
			if (constraints && constraints.video) {
				console.warn('üîê AVISO: Tentativa de usar getUserMedia com v√≠deo detectada');
				// Ainda permite √°udio, mas bloqueia v√≠deo para captura
				if (constraints.video) {
					delete constraints.video;
				}
			}
			return originalGetUserMedia(constraints);
		};
	}

	console.log('‚úÖ Prote√ß√£o contra captura externa ativada');
})();

/* ===============================
   CONSTANTES
=============================== */

const YOU = 'Voc√™';
const OTHER = 'Outros';

const ENABLE_INTERVIEW_TIMING_DEBUG = true; // ‚Üê desligar depois = false
const QUESTION_IDLE_TIMEOUT = 300; // Tempo de espera para a pergunta ser considerada inativa = 300
const CURRENT_QUESTION_SILENCE_TIMEOUT = 1500; // üî• Tempo sem novos interims para considerar pergunta finalizada = 1500ms
const CURRENT_QUESTION_ID = 'CURRENT'; // ID da pergunta atual

const INPUT_SPEECH_THRESHOLD = 20; // Valor limite (threshold) para detectar fala mais cedo = 20
const INPUT_SILENCE_TIMEOUT = 100; // Tempo de espera para sil√™ncio = 100
const MIN_INPUT_AUDIO_SIZE = 1000; // Valor m√≠nimo de tamanho de √°udio para a normal = 1000
const MIN_INPUT_AUDIO_SIZE_INTERVIEW = 350; // Valor m√≠nimo de tamanho de √°udio para a entrevista = 350

const OUTPUT_SPEECH_THRESHOLD = 20; // Valor limite (threshold) para detectar fala mais cedo = 8
const OUTPUT_SILENCE_TIMEOUT = 100; // üî• OTIMIZADO: detecta fim de fala MAIS r√°pido = 80ms para lat√™ncia menor
const AUTO_CLOSE_QUESTION_TIMEOUT = 900; // 900ms ‚Äî aguarda sem novo √°udio antes de fechar pergunta + GPT
const MIN_OUTPUT_AUDIO_SIZE = 1000; // Valor m√≠nimo de tamanho de √°udio para a normal = 2500
const MIN_OUTPUT_AUDIO_SIZE_INTERVIEW = 350; // Valor m√≠nimo para enviar parcial (~3-4 chunks, ~3KB)
// controla intervalo m√≠nimo entre requisi√ß√µes STT parciais (ms) - mant√©m rate-limit para n√£o sobrecarregar API
const PARTIAL_MIN_INTERVAL_MS = 800; // üî• OTIMIZADO: transcri√ß√£o parcial a cada 800ms (era 3000ms)

const OUTPUT_ENDING_PHRASES = ['tchau', 'tchau tchau', 'obrigado', 'valeu', 'falou', 'beleza', 'ok']; // Palavras finais para detectar o fim da fala

const SYSTEM_PROMPT = `
Voc√™ √© um assistente para entrevistas t√©cnicas de Java. Responda como candidato.
Regras de resposta (priorize sempre estas):
- Seja natural e conciso: responda em no m√°ximo 1‚Äì2 frases curtas.
- Use linguagem coloquial e direta, como algu√©m explicando rapidamente verbalmente.
- Evite listas longas, exemplos extensos ou par√°grafos detalhados.
- N√£o comece com cumprimentos ou palavras de preenchimento (ex.: "Claro", "Ok").
- Quando necess√°rio, entregue um exemplo m√≠nimo de 1 linha apenas.
`;

/* ===============================
   SCREENSHOT CAPTURE - ESTADO E CONTROLE
=============================== */

let capturedScreenshots = []; // Array de { filepath, filename, timestamp }
let isCapturing = false;
let isAnalyzing = false;

/* ===============================
   ESTADO GLOBAL
=============================== */

let APP_CONFIG = {
	MODE_DEBUG: false, // ‚Üê alterado via config-manager.js (true = modo mock)
};

// ü™ü Estado do Drag and Drop da janela
let isDraggingWindow = false;

let isRunning = false;
let audioContext;
// let mockInterviewRunning = false;

// üî• MODIFICADO: STT model vem da config agora (removido USE_LOCAL_WHISPER)
let transcriptionMetrics = {
	audioStartTime: null,
	gptStartTime: null,
	gptEndTime: null,
	totalTime: null,
	audioSize: 0,
};

/* üé§ INPUT (VOC√ä) */
let inputStream;
let inputAnalyser;
let inputData;
let inputRecorder;
let inputChunks = [];
let inputSpeaking = false;
let inputSilenceTimer = null;
let inputPartialChunks = [];
let inputPartialTimer = null;

/* üîä OUTPUT (OUTROS) */
let outputStream;
let outputAnalyser;
let outputData;
let outputRecorder;
let outputChunks = [];
let outputSpeaking = false;
let outputSilenceTimer = null;
let outputPartialChunks = [];
let outputPartialTimer = null;
let outputPartialText = '';

// üî• NOVO: IDs para rastrear e parar os loops de animation
let inputVolumeAnimationId = null;
let outputVolumeAnimationId = null;

/* üß† PERGUNTAS */
let currentQuestion = {
	text: '',
	lastUpdate: 0,
	finalized: false,
	lastUpdateTime: null,
	createdAt: null,
	finalText: '',
	interimText: '',
};
let questionsHistory = [];
const answeredQuestions = new Set(); // üîí Armazena respostas j√° geradas (questionId -> true)
let selectedQuestionId = null;
let interviewTurnId = 0;
let gptAnsweredTurnId = null;
let gptRequestedTurnId = null;
let gptRequestedQuestionId = null; // üî• [IMPORTANTE] Rastreia QUAL pergunta foi realmente solicitada ao GPT
let lastSentQuestionText = '';
let autoCloseQuestionTimer = null;
let currentQuestionSilenceTimer = null; // üî• Timer para detectar fim de fala no CURRENT
let lastInputStartAt = null;
let lastInputStopAt = null;
let lastOutputStartAt = null;
let lastOutputStopAt = null;
let lastInputPlaceholderEl = null;
let lastOutputPlaceholderEl = null;
let lastAskedQuestionNormalized = null;
let lastPartialSttAt = null;
let lastOutputPlaceholderId = null; // üî• ID √∫nico para rastrear qual placeholder atualizar

// üî• Vari√°veis tempor√°rias para transcri√ß√£o atual (imunes a race conditions)
// Armazenam os timestamps capturados NO MOMENTO de onstop() para uso exclusivo por transcribeOutput()
let pendingOutputStartAt = null;
let pendingOutputStopAt = null;

/* ===============================
   CALLBACKS / OBSERVERS SYSTEM
   renderer.js √© "cego" para DOM
   config-manager.js se inscreve em mudan√ßas
=============================== */

const UICallbacks = {
	onError: null, // üî• NOVO: Para mostrar erros de valida√ß√£o
	onTranscriptAdd: null,
	onCurrentQuestionUpdate: null,
	onQuestionsHistoryUpdate: null,
	// onAnswerAdd: null,
	onStatusUpdate: null, // ‚Üê Adicionado: Para atualizar status na UI
	onInputVolumeUpdate: null,
	onOutputVolumeUpdate: null,
	onMockBadgeUpdate: null,
	onDOMElementsReady: null, // callback para pedir elementos ao config-manager
	onListenButtonToggle: null,
	onAnswerSelected: null,
	onClearAllSelections: null,
	onScrollToQuestion: null,
	onTranscriptionCleared: null,
	onAnswersCleared: null,
	onAnswerStreamChunk: null,
	onAnswerIdUpdate: null,
	onModeSelectUpdate: null,
	onAnswerStreamEnd: null,
	onPlaceholderFulfill: null,
	onPlaceholderUpdate: null,
	onUpdateInterim: null,
	onClearInterim: null,
	onScreenshotBadgeUpdate: null,
};

// Fun√ß√£o para config-manager se inscrever em eventos
function onUIChange(eventName, callback) {
	if (UICallbacks.hasOwnProperty(eventName)) {
		UICallbacks[eventName] = callback;
		console.log(`üì° UI callback registrado em renderer.js: ${eventName}`);
	}
}

// Fun√ß√£o para emitir/enviar eventos para config-manager
function emitUIChange(eventName, data) {
	if (UICallbacks[eventName] && typeof UICallbacks[eventName] === 'function') {
		UICallbacks[eventName](data);
	} else {
		console.warn(`‚ö†Ô∏è DEBUG: Nenhum callback registrado para '${eventName}'`);
	}
}

/* ===============================
   STT EVENTS - Sistema Unificado de Eventos
   Disparado quando qualquer modelo STT termina uma transcri√ß√£o
=============================== */

const STTEvents = {
	onTranscriptionComplete: null, // Disparado quando STT termina
};

/**
 * üî• Registra listener para eventos STT
 * @param {string} eventName - Nome do evento ('transcriptionComplete')
 * @param {function} callback - Callback a ser executado
 */
function onSTTEvent(eventName, callback) {
	if (eventName === 'transcriptionComplete') {
		STTEvents.onTranscriptionComplete = callback;
		console.log('üì° STT Event listener registrado: transcriptionComplete');
	}
}

/**
 * üî• Emite evento STT para todas as camadas superiores
 * @param {string} eventName - Nome do evento ('transcriptionComplete')
 * @param {object} data - Dados do evento
 */
function emitSTTEvent(eventName, data) {
	if (eventName === 'transcriptionComplete') {
		STTEvents.onTranscriptionComplete?.(data);
	}
}

/* ===============================
   ELEMENTOS UI - Solicitado por callback
   (config-manager.js fornece esses elementos)
=============================== */

let UIElements = {
	inputSelect: null,
	outputSelect: null,
	listenBtn: null,
	statusText: null,
	transcriptionBox: null, // Mantido para compatibilidade, mas pode receber 'conversation'
	currentQuestionBox: null,
	currentQuestionTextBox: null,
	questionsHistoryBox: null,
	answersHistoryBox: null,
	askBtn: null,
	inputVu: null,
	outputVu: null,
	inputVuHome: null,
	outputVuHome: null,
	mockToggle: null,
	mockBadge: null,
	interviewModeSelect: null,
	btnClose: null,
	btnToggleClick: null,
	dragHandle: null,
	darkToggle: null,
	opacitySlider: null,
};

// config-manager.js chama isso para registrar elementos
function registerUIElements(elements) {
	UIElements = { ...UIElements, ...elements };
	console.log('‚úÖ UI Elements registrados no renderer.js');
}

/* ===============================
   MODO / ORQUESTRADOR
=============================== */

const MODES = {
	NORMAL: 'NORMAL',
	INTERVIEW: 'INTERVIEW',
};

// üîÑ modo atual (default = comportamento atual)
let CURRENT_MODE = MODES.NORMAL;

// üéº controlador central de estrat√©gia
const ModeController = {
	isInterviewMode() {
		return CURRENT_MODE === MODES.INTERVIEW;
	},

	// ‚è±Ô∏è MediaRecorder.start(timeslice)
	mediaRecorderTimeslice() {
		if (!this.isInterviewMode()) return null;

		// OUTPUT pode ser mais agressivo que INPUT
		return 60; // reduzido para janelas parciais mais responsivas
	},

	// ü§ñ GPT streaming
	allowGptStreaming() {
		return this.isInterviewMode();
	},

	// üì¶ tamanho m√≠nimo de √°udio aceito
	minInputAudioSize(defaultSize) {
		return this.isInterviewMode() ? Math.min(400, defaultSize) : defaultSize;
	},
};

/* ===============================
   HELPERS PUROS
=============================== */

function finalizeQuestion(t) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "finalizeQuestion"');
	debugLogRenderer('Fim da fun√ß√£o: "finalizeQuestion"');
	return t.trim().endsWith('?') ? t.trim() : t.trim() + '?';
}

function normalizeForCompare(t) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "normalizeForCompare"');
	debugLogRenderer('Fim da fun√ß√£o: "normalizeForCompare"');
	return (t || '')
		.toLowerCase()
		.replace(/[?!.\n\r]/g, '')
		.replace(/\s+/g, ' ')
		.trim();
}

function looksLikeQuestion(t) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "looksLikeQuestion"');
	const s = t.toLowerCase().trim();

	// precisa ter ? OU come√ßar com palavra t√≠pica de pergunta
	const questionStarters = [
		'o que',
		'por que',
		'porque',
		'como',
		'qual',
		'quais',
		'quando',
		'onde',
		'fale',
		'me fale',
		'me explica',
		'me explique',
		'me diga',
		'diga',
		'voc√™',
		'explique',
		'descreva',
		'j√°',
		'tu j√°',
	];

	debugLogRenderer('Fim da fun√ß√£o: "looksLikeQuestion"');
	return s.includes('?') || questionStarters.some(q => s.startsWith(q));
}

function isGarbageSentence(t) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "isGarbageSentence"');
	const s = t.toLowerCase().trim();

	// üî• Detec√ß√£o inteligente: se tem pergunta real, N√ÉO √© lixo
	// Mesmo que tenha "muito bom", se tem "?" ou palavra de pergunta, passa!
	if (looksLikeQuestion(s)) {
		console.log('‚úÖ isGarbageSentence: cont√©m pergunta real, retornando FALSE (n√£o √© lixo)');
		debugLogRenderer('Fim da fun√ß√£o: "isGarbageSentence"');
		return false;
	}

	// üî• Lista expandida de padr√µes de "lixo" PURO (confirma√ß√µes/finaliza√ß√µes sozinhas)
	const garbagePatterns = [
		// Finaliza√ß√µes e agradecimentos
		'obrigado',
		'muito obrigado',
		'valeu',
		'falou',
		'tchau',
		'at√© a pr√≥xima',
		'at√© logo',
		// Confirma√ß√µes simples (sem pergunta real)
		'combinado',
		'certo',
		'beleza',
		'ok',
		't√° bom',
		'est√° bom',
		'perfeito',
		'√≥timo',
		// Frases de continua√ß√£o (n√£o perguntas)
		'responder',
		'responda',
		// Interjei√ß√µes e express√µes vazias
		'e a√≠',
		'u√©',
		'h√£',
		'ahn',
		'e ent√£o',
		'e depois',
		// Finalizando
		'finalizando',
		'pronto',
		'fim',
		'acabou',
	];

	// Detecta se √© lixo puro
	const isGarbage = garbagePatterns.some(w => s.includes(w));

	// Detecta frases muito curtas ou vazias (< 3 caracteres significa ru√≠do)
	const isTooShort = s.length < 3;

	debugLogRenderer('Fim da fun√ß√£o: "isGarbageSentence"');
	return isGarbage || isTooShort;
}

function isIncompleteQuestion(t) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "isIncompleteQuestion"');
	if (!t) return false;
	const s = t.trim();
	// casos √≥bvios: cont√©m retic√™ncias (..., ‚Ä¶) ‚Äî normalmente placeholders ou cortes
	if (s.includes('...') || s.includes('‚Ä¶')) return true;

	// termina com fragmento muito curto seguido de pontua√ß√£o (ex: "O que √© a...")
	// ou termina com apenas 1-3 letras antes do fim (sinal de corte)
	if (/\b\w{1,3}[\.]{0,3}$/.test(s) && /\.\.{1,3}$/.test(s)) return true;

	// termina com palavra muito curta e sem contexto (ex: endsWith ' a' )
	if (/\b[a-z]{1,2}$/.test(s.toLowerCase())) return true;

	debugLogRenderer('Fim da fun√ß√£o: "isIncompleteQuestion"');
	return false;
}

function getNavigableQuestionIds() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "getNavigableQuestionIds"');
	const ids = [];

	// CURRENT s√≥ entra se tiver texto
	if (currentQuestion.text && currentQuestion.text.trim().length > 0) {
		ids.push(CURRENT_QUESTION_ID);
	}

	// Hist√≥rico (mais recente primeiro)
	ids.push(
		...questionsHistory
			.slice()
			.reverse()
			.map(q => q.id),
	);

	debugLogRenderer('Fim da fun√ß√£o: "getNavigableQuestionIds"');
	return ids;
}

function findAnswerByQuestionId(questionId) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "findAnswerByQuestionId"');

	if (!questionId) {
		// ID inv√°lido
		debugLogRenderer('Fim da fun√ß√£o: "findAnswerByQuestionId"');
		return false;
	}

	debugLogRenderer('Fim da fun√ß√£o: "findAnswerByQuestionId"');
	return answeredQuestions.has(questionId);
}

function promoteCurrentToHistory(text) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "promoteCurrentToHistory"');
	console.log('üìö promovendo pergunta para hist√≥rico:', text);

	// evita duplica√ß√£o no hist√≥rico: se a √∫ltima entrada √© igual (normalizada), n√£o adiciona
	const last = questionsHistory.length ? questionsHistory[questionsHistory.length - 1] : null;
	if (last && normalizeForCompare(last.text) === normalizeForCompare(text)) {
		console.log('üîï pergunta igual j√° presente no hist√≥rico ‚Äî pulando promo√ß√£o');

		// limpa CURRENT mas preserva sele√ß√£o conforme antes
		const prevSelected = selectedQuestionId;
		currentQuestion = {
			text: '',
			lastUpdate: 0,
			finalized: false,
			lastUpdateTime: null,
			createdAt: null,
			finalText: '',
			interimText: '',
		};

		// üî• Limpar timer de sil√™ncio
		if (currentQuestionSilenceTimer) {
			clearTimeout(currentQuestionSilenceTimer);
			currentQuestionSilenceTimer = null;
		}

		if (prevSelected === null || prevSelected === CURRENT_QUESTION_ID) {
			selectedQuestionId = CURRENT_QUESTION_ID;
		} else {
			selectedQuestionId = prevSelected;
		}

		renderQuestionsHistory();
		renderCurrentQuestion();
		return;
	}

	const newId = String(questionsHistory.length + 1);

	questionsHistory.push({
		id: newId,
		text,
		createdAt: currentQuestion.createdAt || Date.now(),
		lastUpdateTime: currentQuestion.lastUpdateTime || currentQuestion.createdAt || Date.now(),
	});

	// üî• [IMPORTANTE] Migrar resposta de CURRENT para o novo ID no history
	if (answeredQuestions.has(CURRENT_QUESTION_ID)) {
		answeredQuestions.delete(CURRENT_QUESTION_ID);
		answeredQuestions.add(newId);
		console.log('üîÑ [IMPORTANTE] Migrada resposta de CURRENT para newId:', newId);
	}

	// üî• [CR√çTICO] Atualizar o ID do bloco de resposta no DOM se ele foi criado com CURRENT
	console.log('üîÑ [IMPORTANTE] Emitindo onAnswerIdUpdate para atualizar bloco de resposta: CURRENT ‚Üí ', newId);
	emitUIChange('onAnswerIdUpdate', {
		oldId: CURRENT_QUESTION_ID,
		newId: newId,
	});

	// üî• [IMPORTANTE] Se uma pergunta CURRENT foi solicitada ao GPT,
	// atualizar o rastreamento para apontar para o novo ID promovido
	if (gptRequestedQuestionId === CURRENT_QUESTION_ID) {
		gptRequestedQuestionId = newId;
		console.log('üîÑ [IMPORTANTE] gptRequestedQuestionId atualizado de CURRENT para newId:', newId);
	}

	// preserva sele√ß√£o do usu√°rio: se n√£o havia sele√ß√£o expl√≠cita ou estava no CURRENT,
	// mant√©m a sele√ß√£o no CURRENT para que o novo CURRENT seja principal.
	const prevSelected = selectedQuestionId;

	// üî• RESET COMPLETO: Limpar timer de sil√™ncio antes de resetar
	if (currentQuestionSilenceTimer) {
		console.log('üî• Limpando timer de sil√™ncio durante promo√ß√£o');
		clearTimeout(currentQuestionSilenceTimer);
		currentQuestionSilenceTimer = null;
	}

	resetCurrentQuestion();

	if (prevSelected === null || prevSelected === CURRENT_QUESTION_ID) {
		selectedQuestionId = CURRENT_QUESTION_ID;
	} else {
		// usu√°rio tinha selecionado algo no hist√≥rico ‚Äî preserva essa sele√ß√£o
		selectedQuestionId = prevSelected;
	}

	renderQuestionsHistory();
	renderCurrentQuestion();

	debugLogRenderer('Fim da fun√ß√£o: "promoteCurrentToHistory"');
}

function isQuestionReady(text) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "isQuestionReady"');
	if (!ModeController.isInterviewMode()) return true;

	const trimmed = text.trim();

	// üî• entrevistas podem ter perguntas curtas ("O que √© POO")
	if (trimmed.length < 10) return false;

	// ignora despedidas
	if (isEndingPhrase(trimmed)) return false;

	// heur√≠stica simples de pergunta
	const questionIndicators = [
		'o que',
		'por que',
		'porque',
		'como',
		'qual',
		'quais',
		'quando',
		'onde',
		'fale',
		'me fale',
		'me explica',
		'me explique',
		'me diga',
		'diga',
		'voc√™',
		'explique',
		'descreva',
		'j√°',
		'tu j√°',
	];

	const lower = trimmed.toLowerCase();

	const hasIndicator = questionIndicators.some(q => lower.includes(q));

	const hasQuestionMark = trimmed.includes('?');

	debugLogRenderer('Fim da fun√ß√£o: "isQuestionReady"'); // s√≥ dispara se houver ind√≠cio real
	return hasIndicator || hasQuestionMark;
}

/**
 * üî• AUTO-ASK: Tenta chamar GPT automaticamente em modo entrevista
 * Disparada por: STTEvents.onTranscriptionComplete (ap√≥s 900ms sem √°udio)
 *
 * Precondi√ß√µes:
 * - Modo entrevista ativo
 * - CURRENT tem texto
 * - Pergunta ainda n√£o foi respondida neste turno
 * - Texto n√£o √© "lixo"
 */
function autoAskGptIfReady() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "autoAskGptIfReady"');

	// Valida√ß√µes b√°sicas
	if (!ModeController.isInterviewMode()) {
		console.log('‚è≠Ô∏è autoAskGptIfReady: modo normal (n√£o entrevista), abortando');
		return;
	}

	if (!currentQuestion.text) {
		console.log('‚è≠Ô∏è autoAskGptIfReady: CURRENT est√° vazio, abortando');
		return;
	}

	if (gptRequestedTurnId === interviewTurnId) {
		console.log('‚è≠Ô∏è autoAskGptIfReady: GPT j√° foi solicitado neste turno, abortando');
		return;
	}

	if (gptAnsweredTurnId === interviewTurnId) {
		console.log('‚è≠Ô∏è autoAskGptIfReady: GPT j√° respondeu neste turno, abortando');
		return;
	}

	const text = currentQuestion.text.trim();

	// üî• REMOVIDO: L√≥gica duplicada de concatena√ß√£o de interim
	// O currentQuestion.text j√° inclui interimText atrav√©s de handleCurrentQuestion

	// Verifica se √© lixo
	if (isGarbageSentence(currentQuestion.text.trim())) {
		console.log('‚ùå autoAskGptIfReady: pergunta √© lixo, abortando');
		return;
	}

	console.log('‚úÖ autoAskGptIfReady: chamando askGpt automaticamente');
	askGpt();

	debugLogRenderer('Fim da fun√ß√£o: "autoAskGptIfReady"');
}

function isEndingPhrase(text) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "isEndingPhrase"');
	const normalized = text.toLowerCase().trim();

	debugLogRenderer('Fim da fun√ß√£o: "isEndingPhrase"');
	return OUTPUT_ENDING_PHRASES.some(p => normalized === p);
}

/* ===============================
   üî• RESET COMPLETO DO APP
   Fun√ß√£o centralizada e reutiliz√°vel para limpar tudo
   Pode ser chamada por: mock toggle, resetHomeBtn, ou qualquer outro
=============================== */

/**
 * üßπ Reseta o aplicativo completamente para estado inicial
 * - Substitui resetInterviewState() e resetHomeSection()
 * - Centraliza TODA l√≥gica de limpeza em um √∫nico lugar
 * - Pode ser reutilizada por qualquer bot√£o/controle
 *
 * Uso:
 *   await resetAppState(); // Completo e seguro
 */
async function resetAppState() {
	console.log('üßπ ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê');
	console.log('üßπ INICIANDO RESET COMPLETO DO APP');
	console.log('üßπ ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê');

	try {
		// 1Ô∏è‚É£ PARAR AUTOPLAY DO MOCK (prevent async operations)
		mockAutoPlayActive = false;
		mockScenarioIndex = 0;
		console.log('‚úÖ Autoplay do mock parado');

		// 2Ô∏è‚É£ PARAR √ÅUDIO IMEDIATAMENTE (input/output)
		if (isRunning) {
			console.log('üé§ Parando captura de √°udio...');
			await stopInput();
			await stopOutput();
			isRunning = false;
		}

		// 3Ô∏è‚É£ RESET DE ESTADO DE √ÅUDIO
		inputSpeaking = false;
		outputSpeaking = false;
		console.log('‚úÖ Estado de √°udio resetado');

		// 4Ô∏è‚É£ LIMPAR CHUNKS DE √ÅUDIO
		inputChunks = [];
		outputChunks = [];
		inputPartialChunks = [];
		outputPartialChunks = [];
		outputPartialText = '';
		voskAccumulatedText = '';
		console.log('‚úÖ Chunks de √°udio limpos');

		// 5Ô∏è‚É£ LIMPAR TIMERS DE √ÅUDIO
		if (inputSilenceTimer) {
			clearTimeout(inputSilenceTimer);
			inputSilenceTimer = null;
		}
		if (outputSilenceTimer) {
			clearTimeout(outputSilenceTimer);
			outputSilenceTimer = null;
		}
		if (inputPartialTimer) {
			clearTimeout(inputPartialTimer);
			inputPartialTimer = null;
		}
		if (outputPartialTimer) {
			clearTimeout(outputPartialTimer);
			outputPartialTimer = null;
		}
		if (voskPartialTimer) {
			clearTimeout(voskPartialTimer);
			voskPartialTimer = null;
		}
		if (autoCloseQuestionTimer) {
			clearTimeout(autoCloseQuestionTimer);
			autoCloseQuestionTimer = null;
		}
		console.log('‚úÖ Timers limpos');

		// 6Ô∏è‚É£ LIMPAR PERGUNTAS E RESPOSTAS
		currentQuestion = {
			text: '',
			lastUpdate: 0,
			finalized: false,
			lastUpdateTime: null,
			createdAt: null,
			finalText: '',
			interimText: '',
		};
		questionsHistory = [];
		answeredQuestions.clear();
		selectedQuestionId = null;
		lastSentQuestionText = '';
		lastAskedQuestionNormalized = null;
		console.log('‚úÖ Perguntas e respostas limpas');

		// 7Ô∏è‚É£ LIMPAR ESTADO GPT/ENTREVISTA
		interviewTurnId = 0;
		gptAnsweredTurnId = null;
		gptRequestedTurnId = null;
		gptRequestedQuestionId = null;
		console.log('‚úÖ Estado de entrevista resetado');

		// 8Ô∏è‚É£ LIMPAR PLACEHOLDERS
		lastInputStartAt = null;
		lastInputStopAt = null;
		lastOutputStartAt = null;
		lastOutputStopAt = null;
		pendingOutputStartAt = null;
		pendingOutputStopAt = null;
		lastPartialSttAt = null;
		lastOutputPlaceholderId = null;
		lastInputPlaceholderEl = null;
		lastOutputPlaceholderEl = null;
		console.log('‚úÖ Placeholders limpos');

		// 9Ô∏è‚É£ RESETAR M√âTRICAS
		transcriptionMetrics = {
			audioStartTime: null,
			gptStartTime: null,
			gptEndTime: null,
			totalTime: null,
			audioSize: 0,
		};
		console.log('‚úÖ M√©tricas resetadas');

		// üîü LIMPAR SCREENSHOTS (sem chamar API!)
		if (capturedScreenshots.length > 0) {
			console.log(`üóëÔ∏è Limpando ${capturedScreenshots.length} screenshot(s)...`);
			capturedScreenshots = [];
			emitUIChange('onScreenshotBadgeUpdate', {
				count: 0,
				visible: false,
			});
			// For√ßa limpeza no sistema
			try {
				await ipcRenderer.invoke('CLEANUP_SCREENSHOTS');
			} catch (err) {
				console.warn('‚ö†Ô∏è Erro ao limpar screenshots no sistema:', err);
			}
		}
		console.log('‚úÖ Screenshots limpos');

		// 1Ô∏è‚É£1Ô∏è‚É£ LIMPAR FLAGS
		isCapturing = false;
		isAnalyzing = false;
		console.log('‚úÖ Flags resetadas');

		// 1Ô∏è‚É£2Ô∏è‚É£ ATUALIZAR UI - PERGUNTAS
		emitUIChange('onCurrentQuestionUpdate', {
			text: '',
			isSelected: false,
		});
		emitUIChange('onQuestionsHistoryUpdate', []);
		console.log('‚úÖ Perguntas UI limpa');

		// 1Ô∏è‚É£3Ô∏è‚É£ ATUALIZAR UI - TRANSCRI√á√ïES E RESPOSTAS
		emitUIChange('onTranscriptionCleared');
		emitUIChange('onAnswersCleared');
		console.log('‚úÖ Transcri√ß√µes e respostas UI limpas');

		// 1Ô∏è‚É£4Ô∏è‚É£ ATUALIZAR UI - BOT√ÉO LISTEN
		emitUIChange('onListenButtonToggle', {
			isRunning: false,
			buttonText: 'üé§ Come√ßar a Ouvir... (Ctrl+D)',
		});
		console.log('‚úÖ Bot√£o listen resetado');

		// 1Ô∏è‚É£5Ô∏è‚É£ ATUALIZAR UI - STATUS
		emitUIChange('onStatusUpdate', {
			status: 'ready',
			message: '‚úÖ Pronto',
		});
		console.log('‚úÖ Status atualizado');

		// 1Ô∏è‚É£6Ô∏è‚É£ LIMPAR SELE√á√ïES
		clearAllSelections();
		console.log('‚úÖ Sele√ß√µes limpas');

		// 1Ô∏è‚É£7Ô∏è‚É£ LOG FINAL
		console.log('‚úÖ ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê');
		console.log('‚úÖ RESET COMPLETO CONCLU√çDO COM SUCESSO');
		console.log('‚úÖ ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê');

		return true;
	} catch (error) {
		console.error('‚ùå Erro ao resetar app:', error);
		return false;
	}
}

/**
 * üîß Limpa parcialmente o estado de uma volta de entrevista (turn)
 * Usado internamente durante streaming para n√£o perder contexto
 * N√ÉO substitui resetAppState() - √© um helper minor
 */
function resetInterviewTurnState() {
	// Limpa apenas o output parcial desta volta espec√≠fica
	outputPartialText = '';
	outputPartialChunks = [];
	// N√£o limpa lastAskedQuestionNormalized aqui - mant√©m para evitar duplicatas
}

/* ===============================
   TRANSCRI√á√ÉO (STT) - MODELO DIN√ÇMICO
=============================== */

/**
 * Obt√©m o modelo STT configurado para o provider ativo
 * @returns {string} 'vosk-local' | 'whisper-1' | 'google-stt' etc
 */
function getConfiguredSTTModel() {
	try {
		if (!window.configManager || !window.configManager.config) {
			console.warn('‚ö†Ô∏è configManager n√£o dispon√≠vel, usando padr√£o: whisper-1');
			return 'whisper-1';
		}

		const config = window.configManager.config;
		const activeProvider = config.api?.activeProvider || 'openai';
		const sttModel = config.api?.[activeProvider]?.selectedSTTModel;

		if (!sttModel) {
			console.warn(`‚ö†Ô∏è Modelo STT n√£o configurado para ${activeProvider}, usando padr√£o: whisper-1`);
			return 'whisper-1';
		}

		console.log(`üé§ STT Model selecionado: ${sttModel} (provider: ${activeProvider})`);
		console.log(`   [DEBUG] config.api.${activeProvider}.selectedSTTModel = "${sttModel}"`);
		console.log(
			`   [DEBUG] select#${activeProvider}-stt-model.value = "${
				document.getElementById(activeProvider + '-stt-model')?.value
			}"`,
		);
		return sttModel;
	} catch (err) {
		console.error('‚ùå Erro ao obter modelo STT da config:', err);
		return 'whisper-1'; // fallback
	}
}

/**
 * Roteia transcri√ß√£o de √°udio para o modelo STT configurado
 *
 * Modelos suportados (via config-manager):
 * 1. vosk-local          ‚Üí main.js handlers: vosk-transcribe + vosk-finalize
 * 2. whisper-cpp-local   ‚Üí main.js handler: transcribe-local (alta precis√£o)
 * 3. whisper-1           ‚Üí main.js handler: transcribe-audio (online, OpenAI)
 *
 * Retorna: texto transcrito ou erro
 */
async function transcribeAudio(blob) {
	transcriptionMetrics.audioStartTime = Date.now();
	transcriptionMetrics.audioSize = blob.size;

	const buffer = Buffer.from(await blob.arrayBuffer());
	const sttModel = getConfiguredSTTModel();
	console.log(`üé§ Transcri√ß√£o (${sttModel}): ${blob.size} bytes`);
	console.log(
		`‚è±Ô∏è In√≠cio: ${new Date(transcriptionMetrics.audioStartTime).toLocaleTimeString()}.${
			transcriptionMetrics.audioStartTime % 1000
		}`,
	);

	// Roteia para o modelo configurado
	if (sttModel === 'vosk-local') {
		return await transcribeVoskComplete(buffer, source);
	} else if (sttModel === 'whisper-cpp-local' || sttModel === 'whisper-1') {
		return await transcribeWhisperComplete(buffer, source);
	} else {
		// üî• [CR√çTICO] Modelo desconhecido = ERRO, n√£o fallback!
		throw new Error(
			`Modelo STT desconhecido: ${sttModel}. Configure um modelo v√°lido em "Configura√ß√µes ‚Üí API e Modelos"`,
		);
	}
}

async function transcribeAudioPartial(blob) {
	const buffer = Buffer.from(await blob.arrayBuffer());
	const sttModel = getConfiguredSTTModel();

	if (sttModel === 'vosk-local') {
		return await transcribeVoskPartial(buffer, source);
	} else if (sttModel === 'whisper-cpp-local' || sttModel === 'whisper-1') {
		return await transcribeWhisperPartial(buffer, source);
	} else {
		// üî• [CR√çTICO] Modelo desconhecido = ERRO, n√£o fallback!
		console.warn(`‚ö†Ô∏è Modelo STT desconhecido em transcribeAudioPartial: ${sttModel}`);
		return ''; // Retorna vazio para parcial desconhecido
	}
}

/* ===============================
   TRANSCRI√á√ÉO VOSK (MODO ENTREVISTA)
=============================== */

let voskAccumulatedText = ''; // Acumula resultado parcial do Vosk
let voskPartialTimer = null;
let voskScriptProcessor = null; // ScriptProcessorNode para capturar PCM bruto
let voskAudioBuffer = []; // Acumula PCM entre envios

/* ===============================
   DISPOSITIVOS / CONTROLE DE √ÅUDIO
=============================== */

async function startAudio() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "startAudio"');

	// üî• [NOVO ORQUESTRADOR] Detecta modelo STT e roteia
	const sttModel = getConfiguredSTTModel();
	console.log(`üé§ startAudio: Modelo STT = ${sttModel}`);

	try {
		// üî• ROTEAMENTO: Por modelo STT
		if (sttModel === 'deepgram') {
			console.log('üåä Roteando para startAudioDeepgram');
			await startAudioDeepgram(UIElements);
		} else {
			// üî• Inicia servidor Whisper se necess√°rio
			if (sttModel === 'whisper-cpp-local') {
				const serverStarted = await ipcRenderer.invoke('start-whisper-server');
				if (serverStarted) {
					console.log('‚úÖ Servidor Whisper.cpp iniciado');
				}
			}

			console.log('üé§ Roteando para startInputOutput (Vosk/OpenAI)');
			await startInputOutput();
		}
	} catch (error) {
		console.error('‚ùå Erro em startAudio:', error);
		throw error;
	}

	debugLogRenderer('Fim da fun√ß√£o: "startAudio"');
}

/**
 * üé§ Inicia captura INPUT (voc√™) + OUTPUT (outros)
 * Usado por Vosk, OpenAI, e qualquer modelo que n√£o √© Deepgram
 */
async function startInputOutput() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "startInputOutput"');

	try {
		// Se houver dispositivo de entrada selecionado, inicia a captura de √°udio
		if (UIElements.inputSelect?.value) await startInput();
		// Se houver dispositivo de sa√≠da selecionado, inicia a captura de √°udio
		if (UIElements.outputSelect?.value) await startOutput();

		console.log('‚úÖ startInputOutput: INPUT + OUTPUT iniciados');
	} catch (error) {
		console.error('‚ùå Erro em startInputOutput:', error);
		throw error;
	}

	debugLogRenderer('Fim da fun√ß√£o: "startInputOutput"');
}

async function stopAudio() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "stopAudio"');

	if (currentQuestion.text) closeCurrentQuestionForced();

	const sttModel = getConfiguredSTTModel();
	console.log(`üõë stopAudio: Modelo STT = ${sttModel}`);

	try {
		// üî• ROTEAMENTO: Por modelo STT
		if (sttModel === 'deepgram') {
			console.log('üåä Rotando para stopAudioDeepgram');
			await stopAudioDeepgram();

			// Fecha pergunta atual se estava aberta
			if (currentQuestion.text) closeCurrentQuestionForced();
		} else {
			console.log('üé§ Rotando para stopInputOutput (Vosk/OpenAI)');
			await stopInputOutput();
		}

		// üî• Para servidor Whisper se necess√°rio
		if (sttModel === 'whisper-cpp-local') {
			await ipcRenderer.invoke('stop-whisper-server');
			console.log('üõë Servidor Whisper.cpp parado');
		}
	} catch (error) {
		console.error('‚ùå Erro em stopAudio:', error);
	}

	debugLogRenderer('Fim da fun√ß√£o: "stopAudio"');
}

/**
 * üõë Para captura INPUT (voc√™) + OUTPUT (outros)
 * Usado por Vosk, OpenAI, e qualquer modelo que n√£o √© Deepgram
 */
async function stopInputOutput() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "stopInputOutput"');

	try {
		inputRecorder?.state === 'recording' && inputRecorder.stop();
		outputRecorder?.state === 'recording' && outputRecorder.stop();

		// üÜï VOSK: Reset do estado
		if (ModeController.isInterviewMode()) {
			voskAccumulatedText = '';
			if (voskPartialTimer) {
				clearTimeout(voskPartialTimer);
				voskPartialTimer = null;
			}
		}

		stopInputMonitor();
		stopOutputMonitor();

		console.log('‚úÖ stopInputOutput: INPUT + OUTPUT parados');
	} catch (error) {
		console.error('‚ùå Erro em stopInputOutput:', error);
	}

	debugLogRenderer('Fim da fun√ß√£o: "stopInputOutput"');
}

async function restartAudioPipeline() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "restartAudioPipeline"');

	stopAudio();

	debugLogRenderer('Fim da fun√ß√£o: "restartAudioPipeline"');
}

/* ===============================
   AUDIO - VOLUME MONITORING
=============================== */

// Inicia apenas monitoramento de volume (sem gravar)
async function startInputVolumeMonitoring() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "startInputVolumeMonitoring"');

	if (APP_CONFIG.MODE_DEBUG) {
		console.log('üé§ Monitoramento de volume entrada (modo teste)...');
		return;
	}

	if (!UIElements.inputSelect?.value) {
		console.log('‚ö†Ô∏è Nenhum dispositivo input selecionado');
		return;
	}

	if (!audioContext) {
		audioContext = new AudioContext();
	}

	// üî• NOVO: Se j√° tem stream ativa, n√£o faz nada
	if (inputStream && inputAnalyser) {
		console.log('‚ÑπÔ∏è Monitoramento de volume de entrada j√° ativo');
		return;
	}

	try {
		// Verificar se isRunning √© false antes de iniciar o stream
		if (!isRunning) {
			console.log('üîÑ Iniciando stream de √°udio (input)...');

			inputStream = await navigator.mediaDevices.getUserMedia({
				audio: { deviceId: { exact: UIElements.inputSelect.value } },
			});

			const source = audioContext.createMediaStreamSource(inputStream);

			inputAnalyser = audioContext.createAnalyser();
			inputAnalyser.fftSize = 256;
			inputData = new Uint8Array(inputAnalyser.frequencyBinCount);
			source.connect(inputAnalyser);

			console.log('‚úÖ Monitoramento de volume de entrada iniciado com sucesso');
			updateInputVolume(); // üî• Inicia o loop de atualiza√ß√£o
		}
	} catch (error) {
		console.error('‚ùå Erro ao iniciar monitoramento de volume de entrada:', error);
		inputStream = null;
		inputAnalyser = null;
	}

	debugLogRenderer('Fim da fun√ß√£o: "startInputVolumeMonitoring"');
}

// Inicia apenas monitoramento de volume para output (sem gravar)
async function startOutputVolumeMonitoring() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "startOutputVolumeMonitoring"');

	// Se o modo de debug estiver ativo, retorna
	if (APP_CONFIG.MODE_DEBUG) {
		console.log('üîä Monitoramento de volume sa√≠da (modo teste)...');
		return;
	}

	// Se n√£o houver dispositivo de sa√≠da selecionado, retorna
	if (!UIElements.outputSelect?.value) {
		console.log('‚ö†Ô∏è Nenhum dispositivo output selecionado');
		return;
	}

	// Se n√£o houver contexto de √°udio, cria um novo
	if (!audioContext) {
		audioContext = new AudioContext();
	}

	// Se j√° houver stream e analisador de frequ√™ncia ativos, retorna
	if (outputStream && outputAnalyser) {
		console.log('‚ÑπÔ∏è Monitoramento de volume de sa√≠da j√° ativo');
		return;
	}

	try {
		// Se isRunning for false, inicia o stream de √°udio (output)
		if (!isRunning) {
			console.log('üîÑ Iniciando stream de √°udio (output)...');

			// Cria a stream de √°udio (outputStream)
			await createOutputStream();

			// Inicia o loop de atualiza√ß√£o do volume de sa√≠da
			updateOutputVolume();
		}

		debugLogRenderer('Fim da fun√ß√£o: "startOutputVolumeMonitoring"');
	} catch (error) {
		console.error('‚ùå Erro ao iniciar monitoramento de volume de sa√≠da:', error);

		// Limpa a stream e o analisador de frequ√™ncia (outputStream e outputAnalyser)
		outputStream = null;
		outputAnalyser = null;
	}
}

function stopInputVolumeMonitoring() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "stopInputVolumeMonitoring"');

	// Se isRunning true, n√£o para o monitoramento
	if (isRunning) {
		console.log('‚ÑπÔ∏è Monitoramento de volume de entrada em execu√ß√£o, isRunning = true ‚Äî pulando parada');

		debugLogRenderer('Fim da fun√ß√£o: "stopInputVolumeMonitoring"');
		return;
	}

	// 1. Para o loop de anima√ß√£o
	if (inputVolumeAnimationId) {
		cancelAnimationFrame(inputVolumeAnimationId);
		inputVolumeAnimationId = null;
	}

	// 2. Para as tracks de √°udio para economizar energia/recurso
	if (inputStream) {
		inputStream.getTracks().forEach(track => track.stop());
		inputStream = null;
	}

	inputAnalyser = null;
	inputData = null;

	// 3. Zera a UI
	emitUIChange('onInputVolumeUpdate', { percent: 0 });

	console.log('üõë Monitoramento de volume de entrada parado');

	debugLogRenderer('Fim da fun√ß√£o: "stopInputVolumeMonitoring"');
}

function stopOutputVolumeMonitoring() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "stopOutputVolumeMonitoring"');

	// Se isRunning true, n√£o para o monitoramento
	if (isRunning) {
		console.log('‚ÑπÔ∏è Monitoramento de volume de sa√≠da em execu√ß√£o, isRunning = true ‚Äî pulando parada');

		debugLogRenderer('Fim da fun√ß√£o: "stopOutputVolumeMonitoring"');
		return;
	}

	// 1. Para o loop de anima√ß√£o
	if (outputVolumeAnimationId) {
		cancelAnimationFrame(outputVolumeAnimationId);
		outputVolumeAnimationId = null;
	}

	// 2.Para as tracks de √°udio para economizar energia/recurso
	if (outputStream) {
		outputStream.getTracks().forEach(track => track.stop());
		outputStream = null;
	}

	outputAnalyser = null;
	outputData = null;

	// 3. Zera a UI
	emitUIChange('onOutputVolumeUpdate', { percent: 0 });

	console.log('üõë Monitoramento de volume de sa√≠da parado');

	debugLogRenderer('Fim da fun√ß√£o: "stopOutputVolumeMonitoring"');
}

/* ===============================
   AUDIO - INPUT (VOC√ä)
=============================== */

async function startInput() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "startInput"');

	if (APP_CONFIG.MODE_DEBUG) {
		const text = 'Iniciando monitoramento de entrada de √°udio (modo teste)...';
		addTranscript(YOU, text);
		return;
	}

	if (!UIElements.inputSelect?.value) return;

	if (!audioContext) {
		audioContext = new AudioContext();
	}

	// CR√çTICO: Evita recriar recorder E stream se j√° existem
	if (inputRecorder && inputRecorder.state !== 'inactive') {
		console.log('‚ÑπÔ∏è inputRecorder j√° existe e est√° ativo, pulando reconfigura√ß√£o');
		return;
	}

	// Se j√° existe stream mas precisa reconfigurar, limpa primeiro
	if (inputStream) {
		console.log('üßπ Limpando stream de entrada anterior antes de recriar');
		inputStream.getTracks().forEach(t => t.stop());
		inputStream = null;
	}

	try {
		inputStream = await navigator.mediaDevices.getUserMedia({
			audio: { deviceId: { exact: UIElements.inputSelect.value } },
		});

		const source = audioContext.createMediaStreamSource(inputStream);

		inputAnalyser = audioContext.createAnalyser();
		inputAnalyser.fftSize = 256;
		inputData = new Uint8Array(inputAnalyser.frequencyBinCount);
		source.connect(inputAnalyser);

		// recorder SEMPRE existe
		inputRecorder = new MediaRecorder(inputStream, {
			mimeType: 'audio/webm;codecs=opus',
		});

		inputRecorder.ondataavailable = e => {
			console.log('üî• input.ondataavailable - chunk tamanho:', e.data?.size || e.data?.byteLength || 'n/a');

			inputChunks.push(e.data);

			// MODO ENTREVISTA ‚Äì permite transcri√ß√£o incremental
			if (ModeController.isInterviewMode()) {
				console.log('üß© handlePartialInputChunk chamado (input)');
				handlePartialInputChunk(e.data);
			}
		};

		inputRecorder.onstop = () => {
			console.log('‚èπÔ∏è inputRecorder.onstop chamado');

			// marca o momento exato em que a grava√ß√£o parou
			lastInputStopAt = Date.now();

			// PROTE√á√ÉO CR√çTICA: Se lastInputStartAt for null/undefined, usar stopAt como fallback
			// MAS n√£o usar para calcular duration (isso causaria grav 0ms)
			const actualStartTime =
				lastInputStartAt !== null && lastInputStartAt !== undefined ? lastInputStartAt : lastInputStopAt;

			const recordingDuration = lastInputStopAt - actualStartTime;

			// Logs detalhados para debug
			console.log('‚è±Ô∏è Parada:', new Date(lastInputStopAt).toLocaleTimeString());
			if (lastInputStartAt !== null && lastInputStartAt !== undefined) {
				console.log('‚è±Ô∏è In√≠cio:', new Date(lastInputStartAt).toLocaleTimeString());
			} else {
				console.warn('‚ö†Ô∏è AVISO: lastInputStartAt √© null/undefined! Usando lastInputStopAt como fallback.');
				lastInputStartAt = lastInputStopAt;
			}
			console.log('‚è±Ô∏è Dura√ß√£o da grava√ß√£o:', recordingDuration, 'ms');

			// Cancela qualquer timer pendente de transcri√ß√£o parcial
			// Isso evita que handlePartialInputChunk processe chunks ap√≥s onstop
			if (inputPartialTimer) {
				clearTimeout(inputPartialTimer);
				inputPartialTimer = null;
				console.log('‚è±Ô∏è Cancelado timer de transcri√ß√£o parcial (inputPartialTimer)');
			}

			// Limpa chunks parciais acumulados para evitar duplica√ß√£o
			inputPartialChunks = [];
			console.log('üóëÔ∏è Limpos chunks parciais acumulados (inputPartialChunks)');

			// adiciona placeholder visual para indicar que estamos aguardando a transcri√ß√£o
			// usa startAt se dispon√≠vel para mostrar o hor√°rio inicial enquanto aguarda
			const timeForPlaceholder = lastInputStartAt || lastInputStopAt;
			lastInputPlaceholderEl = addTranscript(YOU, '...', timeForPlaceholder);
			if (lastInputPlaceholderEl) {
				lastInputPlaceholderEl.dataset.stopAt = lastInputStopAt;
				// SEMPRE salvar startAt se estiver dispon√≠vel (at√© que 0 √© v√°lido, n√£o null)
				if (lastInputStartAt !== null && lastInputStartAt !== undefined) {
					lastInputPlaceholderEl.dataset.startAt = lastInputStartAt;
				} else {
					// Se startAt n√£o foi setado corretamente, usar stopAt como fallback
					lastInputPlaceholderEl.dataset.startAt = lastInputStopAt;
				}
			}

			// ‚úÖ CHAMADA CR√çTICA: Transcreve o √°udio capturado
			transcribeInput();
		};

		// Inicia loop de volume apenas se n√£o estiver rodando
		if (!inputVolumeAnimationId) {
			updateInputVolume();
		}

		console.log('‚úÖ startInput: Configurado com sucesso');
	} catch (error) {
		console.error('‚ùå Erro em startInput:', error);
		inputStream = null;
		inputRecorder = null;
		throw error;
	}

	debugLogRenderer('Fim da fun√ß√£o: "startInput"');
}

function updateInputVolume() {
	//debugLogRenderer('In√≠cio da fun√ß√£o: "updateInputVolume"');

	// CR√çTICO: Verifica se deve continuar ANTES de fazer qualquer processamento
	if (!inputAnalyser || !inputData) {
		console.log('‚ö†Ô∏è updateInputVolume: analyser ou data n√£o dispon√≠vel, parando loop');
		if (inputVolumeAnimationId) {
			cancelAnimationFrame(inputVolumeAnimationId);
			inputVolumeAnimationId = null;
		}
		emitUIChange('onInputVolumeUpdate', { percent: 0 });
		return;
	}

	try {
		inputAnalyser.getByteFrequencyData(inputData);
		const avg = inputData.reduce((a, b) => a + b, 0) / inputData.length;
		const percent = Math.min(100, Math.round((avg / 80) * 100));

		// Emite evento em vez de atualizar DOM diretamente
		emitUIChange('onInputVolumeUpdate', { percent });

		if (avg > INPUT_SPEECH_THRESHOLD && inputRecorder && isRunning) {
			if (!inputSpeaking) {
				inputSpeaking = true;
				inputChunks = [];

				const slice = ModeController.mediaRecorderTimeslice();
				lastInputStartAt = Date.now();
				console.log(
					'üéôÔ∏è iniciando grava√ß√£o de entrada (inputRecorder.start) - startAt',
					new Date(lastInputStartAt).toLocaleTimeString(),
					'| inputSpeaking =',
					inputSpeaking,
				);
				slice ? inputRecorder.start(slice) : inputRecorder.start();
			}
			if (inputSilenceTimer) {
				clearTimeout(inputSilenceTimer);
				inputSilenceTimer = null;
			}
		} else if (inputSpeaking && !inputSilenceTimer && inputRecorder) {
			inputSilenceTimer = setTimeout(() => {
				inputSpeaking = false;
				inputSilenceTimer = null;
				console.log(
					'‚èπÔ∏è parando grava√ß√£o de entrada por sil√™ncio (inputRecorder.stop) | lastInputStartAt =',
					lastInputStartAt ? new Date(lastInputStartAt).toLocaleTimeString() : 'NULL',
				);
				if (inputRecorder && inputRecorder.state === 'recording') {
					inputRecorder.stop();
				}
			}, INPUT_SILENCE_TIMEOUT);
		}
	} catch (error) {
		console.error('‚ùå Erro em updateInputVolume:', error);
		if (inputVolumeAnimationId) {
			cancelAnimationFrame(inputVolumeAnimationId);
			inputVolumeAnimationId = null;
		}
		emitUIChange('onInputVolumeUpdate', { percent: 0 });
		return;
	}

	// Continua o loop apenas se tudo estiver OK
	inputVolumeAnimationId = requestAnimationFrame(updateInputVolume);

	//debugLogRenderer('Fim da fun√ß√£o: "updateInputVolume"');
}

function stopInputMonitor() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "stopInputMonitor"');

	// 1. Para o loop de animation PRIMEIRO
	if (inputVolumeAnimationId) {
		cancelAnimationFrame(inputVolumeAnimationId);
		inputVolumeAnimationId = null;
		console.log('‚úÖ Loop de anima√ß√£o de entrada cancelado');
	}

	// 2. Para o recorder se estiver gravando
	if (inputRecorder) {
		if (inputRecorder.state === 'recording') {
			console.log('‚èπÔ∏è Parando recorder de entrada...');
			inputRecorder.stop();
		}
		inputRecorder = null;
	}

	// 3. Fecha a stream
	if (inputStream) {
		inputStream.getTracks().forEach(t => {
			t.stop();
			console.log('‚úÖ Track de entrada parada:', t.label);
		});
		inputStream = null;
	}

	// 4. Limpa analyser e dados
	inputAnalyser = null;
	inputData = null;

	// 5. Reseta estado
	inputSpeaking = false;
	if (inputSilenceTimer) {
		clearTimeout(inputSilenceTimer);
		inputSilenceTimer = null;
	}

	// 6. Atualiza UI
	emitUIChange('onInputVolumeUpdate', { percent: 0 });

	debugLogRenderer('Fim da fun√ß√£o: "stopInputMonitor"');
	return Promise.resolve();
}

/* ===============================
   AUDIO - OUTPUT (OUTROS) - VIA VOICEMEETER
=============================== */

async function createOutputStream() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "createOutputStream"');

	// Cria a stream de √°udio (outputStream)
	outputStream = await navigator.mediaDevices.getUserMedia({
		audio: { deviceId: { exact: UIElements.outputSelect.value } },
	});

	// Cria o source de √°udio (source)
	const source = audioContext.createMediaStreamSource(outputStream);

	// Cria o analisador de frequ√™ncia (outputAnalyser)
	outputAnalyser = audioContext.createAnalyser();
	// Define o tamanho do FFT (fftSize) como 256
	outputAnalyser.fftSize = 256;
	// Cria os dados (outputData)
	outputData = new Uint8Array(outputAnalyser.frequencyBinCount);
	// Conecta o source ao analisador de frequ√™ncia
	source.connect(outputAnalyser);

	debugLogRenderer('Fim da fun√ß√£o: "createOutputStream"');

	return source;
}

async function startOutput() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "startOutput"');

	// Se o modo de debug estiver ativo, retorna
	if (APP_CONFIG.MODE_DEBUG) {
		const text = 'Iniciando monitoramento de sa√≠da de √°udio (modo teste)...';
		addTranscript(OTHER, text);
		return;
	}

	// Se n√£o houver dispositivo de sa√≠da selecionado, retorna
	if (!UIElements.outputSelect?.value) {
		console.log('‚ö†Ô∏è Nenhum dispositivo output selecionado');
		return;
	}

	// Se n√£o houver contexto de √°udio, cria um novo
	if (!audioContext) {
		audioContext = new AudioContext();
	}

	// Se j√° houver outputRecorder e ele estiver ativo, retorna
	if (outputRecorder && outputRecorder.state !== 'inactive') {
		console.log('‚ÑπÔ∏è outputRecorder j√° existe e est√° ativo, pulando reconfigura√ß√£o');
		return;
	}

	// Se j√° houver outputStream, limpa primeiro
	if (outputStream) {
		console.log('üßπ Limpando stream de sa√≠da anterior antes de recriar');
		outputStream.getTracks().forEach(t => t.stop());
		outputStream = null;
	}

	try {
		console.log('üîÑ startOutput: Configurando monitoramento de sa√≠da de √°udio...');

		// Cria a stream de √°udio (outputStream)
		await createOutputStream();

		// Cria o recorder (outputRecorder), recorder SEMPRE existe
		outputRecorder = new MediaRecorder(outputStream, {
			mimeType: 'audio/webm;codecs=opus',
		});

		// Define o callback para quando houver dados dispon√≠veis no outputRecorder, acionado ao chamar outputRecorder.start()
		outputRecorder.ondataavailable = e => {
			console.log(
				'üî• outputRecorder.ondataavailable chamado - chunk tamanho:',
				e.data?.size || e.data?.byteLength || 'n/a',
			);

			// Adiciona o chunk (peda√ßos de dados) ao array de chunks de sa√≠da
			outputChunks.push(e.data);

			// MODO ENTREVISTA ‚Äì permite transcri√ß√£o incremental
			if (ModeController.isInterviewMode()) {
				console.log('üß© handlePartialOutputChunk chamado (output)');
				handlePartialOutputChunk(e.data);
			}
		};

		// Define o callback para quando o outputRecorder for parado, acionado ao chamar outputRecorder.stop()
		outputRecorder.onstop = () => {
			console.log('‚èπÔ∏è outputRecorder.onstop chamado');

			// Marca o momento exato em que a grava√ß√£o parou
			lastOutputStopAt = Date.now();

			// üî• CR√çTICO: Capturar timestamps AGORA em vari√°veis tempor√°rias
			// Essas vari√°veis s√£o isoladas e N√ÉO ser√£o sobrescritas por updateOutputVolume()
			pendingOutputStartAt = lastOutputStartAt;
			pendingOutputStopAt = lastOutputStopAt;

			// Debug: Verificar valores de lastOutputStartAt
			console.log('üîç DEBUG outputRecorder.onstop:');
			console.log('  ‚Üí lastOutputStartAt:', lastOutputStartAt, `(tipo: ${typeof lastOutputStartAt})`);
			console.log('  ‚Üí lastOutputStopAt:', lastOutputStopAt, `(tipo: ${typeof lastOutputStopAt})`);
			console.log('  ‚Üí üî• Capturado em pending: start=', pendingOutputStartAt, 'stop=', pendingOutputStopAt);

			// Calcula dura√ß√£o com prote√ß√£o contra valores inv√°lidos
			let recordingDuration = 0;
			if (lastOutputStartAt !== null && lastOutputStartAt !== undefined && typeof lastOutputStartAt === 'number') {
				recordingDuration = lastOutputStopAt - lastOutputStartAt;
			} else {
				console.warn('‚ö†Ô∏è AVISO: lastOutputStartAt √© inv√°lido, usando 0 como dura√ß√£o');
				recordingDuration = 0;
			}

			console.log('‚è±Ô∏è Parada: ' + new Date(lastOutputStopAt).toLocaleTimeString());
			console.log('‚è±Ô∏è Dura√ß√£o da grava√ß√£o:', recordingDuration, 'ms');

			// Cancela qualquer timer pendente de transcri√ß√£o parcial
			// Isso evita que transcribeOutputPartial processe chunks ap√≥s onstop
			if (outputPartialTimer) {
				clearTimeout(outputPartialTimer);
				outputPartialTimer = null;
				console.log('‚è±Ô∏è Cancelado timer de transcri√ß√£o parcial (outputPartialTimer)');
			}

			// Limpa chunks parciais acumulados para evitar duplica√ß√£o
			outputPartialChunks = [];
			console.log('üóëÔ∏è Limpos chunks parciais acumulados (outputPartialChunks)');

			// Inicia a transcri√ß√£o do √°udio de sa√≠da (Vosk)
			// ‚ö†Ô∏è O placeholder ser√° criado direto no transcribeOutput() com as m√©tricas corretas
			transcribeOutput();
		};

		// Inicia o loop de atualiza√ß√£o do volume de sa√≠da, se n√£o estiver rodando
		if (!outputVolumeAnimationId) {
			updateOutputVolume();
		}

		console.log('‚úÖ startOutput: Monitoramento de sa√≠da de √°udio configurado com sucesso');
	} catch (error) {
		console.error('‚ùå Erro em startOutput:', error);

		outputStream = null;
		outputRecorder = null;
		throw error;
	}

	debugLogRenderer('Fim da fun√ß√£o: "startOutput"');
}

function updateOutputVolume() {
	//debugLogRenderer('In√≠cio da fun√ß√£o: "updateOutputVolume"');

	// Cr√≠tico: Verifica se o analisador de frequ√™ncia (outputAnalyser) e os dados (outputData)
	// est√£o dispon√≠veis antes de continuar o loop de anima√ß√£o
	if (!outputAnalyser || !outputData) {
		console.log('‚ö†Ô∏è updateOutputVolume: outputAnalyser ou outputData n√£o dispon√≠vel, parando loop de anima√ß√£o');

		// Se o loop de anima√ß√£o (outputVolumeAnimationId) estiver definido, limpa o loop de anima√ß√£o
		if (outputVolumeAnimationId) {
			// Para o loop de anima√ß√£o
			cancelAnimationFrame(outputVolumeAnimationId);
			// Limpa o loop de anima√ß√£o
			outputVolumeAnimationId = null;
		}

		// Emite o evento 'onOutputVolumeUpdate' para atualizar o volume de sa√≠da
		emitUIChange('onOutputVolumeUpdate', { percent: 0 });

		return;
	}

	try {
		// Obt√©m os dados do analisador de frequ√™ncia (outputAnalyser)
		outputAnalyser.getByteFrequencyData(outputData);
		// Calcula o volume m√©dio (avg) dos dados do analisador de frequ√™ncia (outputData)
		const avg = outputData.reduce((a, b) => a + b, 0) / outputData.length;
		// Calcula o percentual de volume (percent) dos dados do analisador de frequ√™ncia (outputData)
		const percent = Math.min(100, Math.round((avg / 60) * 100));

		// Emite o evento 'onOutputVolumeUpdate' para atualizar o volume de sa√≠da
		emitUIChange('onOutputVolumeUpdate', { percent });

		// Se o volume m√©dio (avg) estiver acima do limite (OUTPUT_SPEECH_THRESHOLD)
		// e o recorder (outputRecorder) estiver rodando e o isRunning for true, inicia a grava√ß√£o de sa√≠da
		if (avg > OUTPUT_SPEECH_THRESHOLD && outputRecorder && isRunning) {
			// Se o outputSpeaking for false, inicia a grava√ß√£o de sa√≠da
			if (!outputSpeaking) {
				// üî• [NOVO] Se houver timer de auto-close pendente, cancela
				// (novo √°udio come√ßou, ent√£o n√£o devemos fechar agora)
				if (autoCloseQuestionTimer) {
					console.log('‚è∏Ô∏è Auto-close cancelado: novo √°udio detectado!');
					clearTimeout(autoCloseQuestionTimer);
					autoCloseQuestionTimer = null;
				}

				// RESET: Limpa valores da frase anterior ANTES de iniciar nova frase
				lastOutputPlaceholderEl = null;
				lastOutputStopAt = null;
				// Nota: lastOutputStartAt ser√° atualizado abaixo
				console.log('üßπ LIMPAR: Resetando lastOutputPlaceholderEl e lastOutputStopAt ANTES de nova frase');

				// Define o estado de outputSpeaking como true
				outputSpeaking = true;
				// Limpa o array de chunks de sa√≠da
				outputChunks = [];

				// Define o momento exato em que a grava√ß√£o de sa√≠da foi iniciada
				lastOutputStartAt = Date.now();

				console.log('üéôÔ∏è In√≠cio: ' + new Date(lastOutputStartAt).toLocaleTimeString());
				console.log('üìä lastOutputStartAt definido para:', lastOutputStartAt);

				// üî• PASSO 1: Criar placeholder IMEDIATAMENTE quando fala inicia
				// Isso garante que "Outros: ..." apare√ßa na tela assim que detecta fala
				try {
					// üî• Gerar ID ANTES de criar o placeholder
					lastOutputPlaceholderId = 'placeholder-' + lastOutputStartAt + '-' + Math.random();
					// üî• Passar o ID para ser atribu√≠do ao elemento real no DOM
					lastOutputPlaceholderEl = addTranscript(OTHER, '...', lastOutputStartAt, lastOutputPlaceholderId);
					if (lastOutputPlaceholderEl && lastOutputPlaceholderEl.dataset) {
						lastOutputPlaceholderEl.dataset.startAt = lastOutputStartAt;
						lastOutputPlaceholderEl.dataset.stopAt = lastOutputStartAt; // provis√≥rio, ser√° atualizado
					}
					console.log('‚ú® Placeholder criado no in√≠cio da fala para "Outros" (id=' + lastOutputPlaceholderId + ')');
				} catch (err) {
					console.warn('‚ö†Ô∏è Falha ao criar placeholder no in√≠cio:', err);
				}

				// Usar o mesmo timeslice que INPUT para manter consist√™ncia
				const slice = ModeController.mediaRecorderTimeslice();
				slice ? outputRecorder.start(slice) : outputRecorder.start();
			}
			if (outputSilenceTimer) {
				clearTimeout(outputSilenceTimer);
				outputSilenceTimer = null;
			}
		} else if (outputSpeaking && !outputSilenceTimer && outputRecorder) {
			// Define o timer de sil√™ncio (outputSilenceTimer)
			outputSilenceTimer = setTimeout(() => {
				// Define o estado de outputSpeaking como false
				outputSpeaking = false;
				// Limpa o timer de sil√™ncio (outputSilenceTimer)
				outputSilenceTimer = null;

				console.log('‚èπÔ∏è parando grava√ß√£o de sa√≠da por sil√™ncio (outputRecorder.stop)');

				// Se o recorder (outputRecorder) estiver rodando, para a grava√ß√£o de sa√≠da
				if (outputRecorder && outputRecorder.state === 'recording') {
					// Para a grava√ß√£o de sa√≠da
					outputRecorder.stop();
				}
			}, OUTPUT_SILENCE_TIMEOUT); // Tempo de espera para sil√™ncio
		}
	} catch (error) {
		console.error('‚ùå Erro em updateOutputVolume:', error);
		// Se o loop de anima√ß√£o (outputVolumeAnimationId) estiver definido, limpa o loop de anima√ß√£o
		if (outputVolumeAnimationId) {
			// Para o loop de anima√ß√£o
			cancelAnimationFrame(outputVolumeAnimationId);
			// Limpa o loop de anima√ß√£o
			outputVolumeAnimationId = null;
		}
		// Emite o evento 'onOutputVolumeUpdate' para atualizar o volume de sa√≠da
		emitUIChange('onOutputVolumeUpdate', { percent: 0 });
		return;
	}

	// Continua o loop de anima√ß√£o apenas se tudo estiver OK
	outputVolumeAnimationId = requestAnimationFrame(updateOutputVolume);

	//debugLogRenderer('Fim da fun√ß√£o: "updateOutputVolume"');
}

function stopOutputMonitor() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "stopOutputMonitor"');

	// 1. Para o loop de animation PRIMEIRO
	if (outputVolumeAnimationId) {
		cancelAnimationFrame(outputVolumeAnimationId);
		outputVolumeAnimationId = null;
		console.log('‚úÖ Loop de anima√ß√£o de sa√≠da cancelado');
	}

	// 2. Para o recorder se estiver gravando
	if (outputRecorder) {
		if (outputRecorder.state === 'recording') {
			console.log('‚èπÔ∏è Parando recorder de sa√≠da...');
			outputRecorder.stop();
		}
		outputRecorder = null;
	}

	// 3. Fecha a stream
	if (outputStream) {
		outputStream.getTracks().forEach(t => {
			t.stop();
			console.log('‚úÖ Track de sa√≠da parada:', t.label);
		});
		outputStream = null;
	}

	// 4. Limpa analyser e dados
	outputAnalyser = null;
	outputData = null;

	// 5. Reseta estado
	outputSpeaking = false;
	if (outputSilenceTimer) {
		clearTimeout(outputSilenceTimer);
		outputSilenceTimer = null;
	}

	// 6. Atualiza UI
	emitUIChange('onOutputVolumeUpdate', { percent: 0 });

	debugLogRenderer('Fim da fun√ß√£o: "stopOutputMonitor"');
	return Promise.resolve();
}

/* ===============================
   MODO ENTREVISTA - TRANSCRI√á√ÉO PARCIAL
=============================== */

async function handlePartialInputChunk(blobChunk) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "handlePartialInputChunk"');
	if (!ModeController.isInterviewMode()) return;

	// ignora ru√≠do
	if (blobChunk.size < 200) return;

	inputPartialChunks.push(blobChunk);

	if (inputPartialTimer) clearTimeout(inputPartialTimer);

	inputPartialTimer = setTimeout(async () => {
		if (!inputPartialChunks.length) return;

		const blob = new Blob(inputPartialChunks, { type: 'audio/webm' });
		inputPartialChunks = [];

		try {
			const buffer = Buffer.from(await blob.arrayBuffer());
			const partialText = (await transcribeAudioPartial(blob))?.trim();

			if (partialText && !isGarbageSentence(partialText)) {
				addTranscript(YOU, partialText);
				handleSpeech(YOU, partialText, { skipAddToUI: true });
			}
		} catch (err) {
			console.warn('‚ö†Ô∏è erro na transcri√ß√£o parcial (INPUT)', err);
		}
	}, 180); // janela curta (reduzida de 250 -> 180)

	debugLogRenderer('Fim da fun√ß√£o:  "handlePartialInputChunk"');
}

async function handlePartialOutputChunk(blobChunk) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "handlePartialOutputChunk"');
	if (!ModeController.isInterviewMode()) return;

	// ignora ru√≠do
	if (blobChunk.size < 200) return;

	outputPartialChunks.push(blobChunk);

	if (outputPartialTimer) clearTimeout(outputPartialTimer);

	outputPartialTimer = setTimeout(async () => {
		if (!outputPartialChunks.length) return;

		const blob = new Blob(outputPartialChunks, { type: 'audio/webm' });
		const blobSize = blob.size;
		outputPartialChunks = [];

		try {
			const partialStart = Date.now();
			const buffer = Buffer.from(await blob.arrayBuffer());
			const partialText = (await transcribeAudioPartial(blob))?.trim();
			const partialDuration = Date.now() - partialStart;

			if (partialText && !isGarbageSentence(partialText)) {
				console.log(`‚ö° PARCIAL: ${blobSize}bytes ‚Üí "${partialText.substring(0, 50)}" em ${partialDuration}ms`);
				addTranscript(OTHER, partialText);
				// N√ÉO chamar handleSpeech aqui - evita consolida√ß√£o nas parciais
				// consolida√ß√£o s√≥ acontece em transcribeOutput() para o √°udio final
			}
		} catch (err) {
			console.warn('‚ö†Ô∏è erro na transcri√ß√£o parcial (OUTPUT)', err);
		}
	}, 100); // üî• OTIMIZADO: debounce reduzido para 100ms (era 180) para lat√™ncia menor

	debugLogRenderer('Fim da fun√ß√£o:  "handlePartialOutputChunk"');
}

function transcribeOutputPartial(blobChunk) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "transcribeOutputPartial"');

	// Se n√£o estiver no modo entrevista, retorna
	if (!ModeController.isInterviewMode()) {
		console.log('‚ÑπÔ∏è transcribeOutputPartial: retornando, modo entrevista n√£o ativo');

		debugLogRenderer('Fim da fun√ß√£o: "transcribeOutputPartial"');
		return;
	}

	// Desabilitado temporariamente (teste)
	if (DESABILITADO_TEMPORARIAMENTE) {
		debugLogRenderer('Fim da fun√ß√£o: "transcribeOutputPartial" üîí DESABILITADO TEMPORARIAMENTE');
		return;
	}

	// MODO ENTREVISTA ‚Äì permite transcri√ß√£o incremental

	// Ignora ru√≠do, evita blobs pequenos demais
	if (blobChunk.size < MIN_OUTPUT_AUDIO_SIZE_INTERVIEW) {
		console.log('‚ö†Ô∏è Ignorando blobChunk pequeno demais para transcri√ß√£o parcial (OUTPUT) - size:', blobChunk.size);

		debugLogRenderer('Fim da fun√ß√£o: "transcribeOutputPartial"');
		return;
	}

	// Adiciona o chunk ao array de chunks parciais de sa√≠da
	outputPartialChunks.push(blobChunk);
	console.log('üì¶ Chunk acumulado:', blobChunk.size, 'bytes | Total chunks:', outputPartialChunks.length);

	// Reinicia o timer para processar o chunk parcial ap√≥s um curto per√≠odo
	if (outputPartialTimer) clearTimeout(outputPartialTimer);

	// calcula delay respeitando um intervalo m√≠nimo entre requisi√ß√µes STT parciais
	const now = Date.now();
	const elapsedSinceLast = typeof lastPartialSttAt === 'number' ? now - lastPartialSttAt : Infinity;
	let intendedDelay = 120; // janela base para agrupar chunks
	if (elapsedSinceLast < PARTIAL_MIN_INTERVAL_MS) {
		intendedDelay = PARTIAL_MIN_INTERVAL_MS - elapsedSinceLast + 50; // pequeno buffer extra
		console.log('‚è±Ô∏è Ajustando delay parcial para respeitar rate-limit (ms):', intendedDelay);
	}

	// Define um timer para processar o chunk parcial ap√≥s X(ms)
	// Timeout curto (300ms) para agrupar ~5-8 chunks e enviar r√°pido para STT
	outputPartialTimer = setTimeout(async () => {
		// Se n√£o houver chunks parciais de sa√≠da, retorna
		if (!outputPartialChunks.length) {
			console.log('‚ö†Ô∏è Nenhum chunk parcial para processar');
			return;
		}

		// Cria um blob a partir dos chunks parciais de sa√≠da
		const blob = new Blob(outputPartialChunks, { type: 'audio/webm' });

		// Loga o tamanho total do blob parcial
		const totalSize = outputPartialChunks.reduce((acc, chunk) => acc + chunk.size, 0);
		console.log('üéµ Processando blob parcial:', totalSize, 'bytes de', outputPartialChunks.length, 'chunks');

		// Limpa o array de chunks parciais de sa√≠da ap√≥s criar blob
		outputPartialChunks = [];

		try {
			// Envia para transcri√ß√£o o blob parcial de sa√≠da
			const partialText = await transcribeAudioPartial(blob);
			// marca √∫ltimo envio parcial
			lastPartialSttAt = Date.now();
			console.log('üìù transcribeOutputPartial: Transcri√ß√£o recebida: ', partialText);

			// Ignora transcri√ß√£o vazia
			if (!partialText || partialText.trim().length === 0) {
				console.log('‚ö†Ô∏è Transcri√ß√£o vazia - ignorando');
				return;
			}

			// Ignora senten√ßas garbage
			if (isGarbageSentence(partialText)) {
				console.log('üóëÔ∏è Senten√ßa descartada (garbage):', partialText);
				return;
			}

			// acumula texto parcial
			outputPartialText += ' ' + partialText;
			outputPartialText = outputPartialText.trim();
			console.log('üìã Texto acumulado:', outputPartialText);

			// Atualiza UI com transcri√ß√£o parcial imediatamente (usa placeholder incremental)
			try {
				// cria placeholder se ainda n√£o existe (usa startAt se dispon√≠vel)
				if (!lastOutputPlaceholderEl) {
					const placeholderTime = lastOutputStartAt || Date.now();
					lastOutputPlaceholderEl = addTranscript(OTHER, '...', placeholderTime);
					if (lastOutputPlaceholderEl && lastOutputPlaceholderEl.dataset) {
						lastOutputPlaceholderEl.dataset.startAt = placeholderTime;
						// marca um stop provis√≥rio para o UI mostrar intervalo din√¢mico
						lastOutputPlaceholderEl.dataset.stopAt = Date.now();
					}
				} else if (lastOutputPlaceholderEl && lastOutputPlaceholderEl.dataset) {
					// atualiza stop provis√≥rio a cada parcial
					lastOutputPlaceholderEl.dataset.stopAt = Date.now();
				}

				// solicita ao config-manager atualiza√ß√£o parcial do placeholder (inclui m√©tricas provis√≥rias)
				emitUIChange('onPlaceholderUpdate', {
					speaker: OTHER,
					text: outputPartialText,
					timeStr: new Date(lastOutputStartAt || Date.now()).toLocaleTimeString(),
					startStr: new Date(lastOutputStartAt || Date.now()).toLocaleTimeString(),
					stopStr: new Date().toLocaleTimeString(),
					recordingDuration: Date.now() - (lastOutputStartAt || Date.now()),
					latency: 0,
					total: Date.now() - (lastOutputStartAt || Date.now()),
					provisional: true,
				});

				// atualiza currentQuestion para refletir texto parcial
				if (
					!currentQuestion.text ||
					normalizeForCompare(currentQuestion.text) !== normalizeForCompare(outputPartialText)
				) {
					currentQuestion.text = outputPartialText;
					currentQuestion.lastUpdate = Date.now();
					currentQuestion.lastUpdateTime = Date.now();
					currentQuestion.finalized = false;
					selectedQuestionId = CURRENT_QUESTION_ID;
					renderCurrentQuestion();
				}
			} catch (err) {
				console.warn('‚ö†Ô∏è falha ao atualizar UI com transcri√ß√£o parcial:', err);
			}

			// verifica se a pergunta est√° "pronta" (heur√≠stica)
			if (isQuestionReady(outputPartialText)) {
				console.log('‚ùì Pergunta detectada (parcial):', outputPartialText);

				// limpa texto parcial acumulado
				const newText = outputPartialText.trim();

				// verifica se o novo texto √© igual ao texto atual da pergunta, se sim, ignora
				if (newText === currentQuestion.text) {
					// üü° No modo entrevista, se a pergunta ainda N√ÉO foi fechada,
					// permitimos seguir para fechamento e chamada do GPT
					if (!currentQuestion.finalized) {
						console.log('üü° Pergunta repetida, mas v√°lida no modo entrevista ‚Äî permitindo fechamento');
					} else {
						console.log('üîï Ignorando nova transcri√ß√£o igual √† currentQuestion');
						return;
					}
				}

				// se currentQuestion ainda n√£o tinha texto, marca como um novo turno
				if (!currentQuestion.text) {
					currentQuestion.createdAt = Date.now();
					interviewTurnId++; // novo turno detectado
					console.log('üÜï Novo turno iniciado:', interviewTurnId);
				}

				// atualiza a pergunta atual com o novo texto parcial
				currentQuestion.text = newText;
				// atualiza timestamp de √∫ltima modifica√ß√£o
				currentQuestion.lastUpdate = Date.now();
				currentQuestion.lastUpdateTime = Date.now();
				// marca como n√£o finalizada
				currentQuestion.finalized = false;

				// atualiza UI
				selectedQuestionId = CURRENT_QUESTION_ID;
				renderCurrentQuestion();

				console.log('üß† currentQuestion (parcial):', currentQuestion.text);
				console.log('üéØ interviewTurnId:', interviewTurnId);
				console.log('ü§ñ gptAnsweredTurnId:', gptAnsweredTurnId);

				// reseta o timer de auto fechamento
				if (autoCloseQuestionTimer) {
					clearTimeout(autoCloseQuestionTimer);
				}

				// ‚è±Ô∏è agenda timer para auto fechamento da pergunta ap√≥s per√≠odo ocioso
				autoCloseQuestionTimer = setTimeout(() => {
					console.log('‚è±Ô∏è Auto close question disparado (timeout)');

					if (
						ModeController.isInterviewMode() &&
						currentQuestion.text &&
						!currentQuestion.finalized &&
						gptAnsweredTurnId !== interviewTurnId
					) {
						// fecha a pergunta atual automaticamente
						closeCurrentQuestion();
					}
				}, QUESTION_IDLE_TIMEOUT);

				console.log('‚è≤Ô∏è Timer de auto-fechamento agendado para', QUESTION_IDLE_TIMEOUT, 'ms');
			} else {
				console.log('‚è≥ Aguardando mais texto para formar pergunta completa');
			}
		} catch (err) {
			console.error('‚ùå Erro na transcri√ß√£o parcial (OUTPUT):', err);
		}
	}, 300); // Janela de 300ms para m√°xima responsividade - envia ~5-8 chunks a cada 3s (rate-limit)

	debugLogRenderer('Fim da fun√ß√£o: "transcribeOutputPartial"');
}

/* ===============================
   MODO NORMAL - TRANSCRI√á√ÉO
=============================== */

async function transcribeInput() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "transcribeInput"');
	if (!inputChunks.length) return;

	const blob = new Blob(inputChunks, { type: 'audio/webm' });
	console.log('üîÅ transcrever entrada - blob.size:', blob.size); // diagn√≥stico

	// ignora ru√≠do / respira√ß√£o
	const minSize = ModeController.isInterviewMode() ? MIN_INPUT_AUDIO_SIZE_INTERVIEW : MIN_INPUT_AUDIO_SIZE;

	if (blob.size < minSize) return;

	inputChunks = [];

	// medir tempo de convers√£o blob -> buffer
	const tBlobToBuffer = Date.now();
	const buffer = Buffer.from(await blob.arrayBuffer());
	console.log('timing: bufferConv', Date.now() - tBlobToBuffer, 'ms, size', buffer.length);

	// medir tempo IPC + STT (roundtrip)
	const tSend = Date.now();
	const text = (await transcribeAudio(blob))?.trim();
	console.log('timing: ipc_stt_roundtrip', Date.now() - tSend, 'ms');
	if (!text || isGarbageSentence(text)) return;

	// Se existia um placeholder (timestamp do stop), calcula m√©tricas e emite evento para atualizar
	if (lastInputPlaceholderEl && lastInputPlaceholderEl.dataset) {
		// Extrai timestamps do dataset (sempre como n√∫meros, nunca null)
		const stop = lastInputPlaceholderEl.dataset.stopAt
			? Number(lastInputPlaceholderEl.dataset.stopAt)
			: lastInputStopAt;

		// Para startAt, SEMPRE preferir dataset (mesmo que seja 0), nunca deixar undefined
		const start =
			lastInputPlaceholderEl.dataset.startAt !== undefined
				? Number(lastInputPlaceholderEl.dataset.startAt)
				: lastInputStartAt !== null && lastInputStartAt !== undefined
				? lastInputStartAt
				: stop;

		const now = Date.now();
		const recordingDuration = stop - start;
		const latency = now - stop;
		const total = now - start;
		const startStr = new Date(start).toLocaleTimeString();
		const stopStr = new Date(stop).toLocaleTimeString();
		const displayStr = new Date(now).toLocaleTimeString();

		// Log detalhado de timing
		console.log('‚è±Ô∏è TIMING COMPLETO:');
		console.log(`  ‚úÖ In√≠cio: ${startStr}`);
		console.log(`  ‚èπÔ∏è Parada: ${stopStr}`);
		console.log(`  üì∫ Exibi√ß√£o: ${displayStr}`);
		console.log(`  üìä Dura√ß√£o grava√ß√£o: ${recordingDuration}ms | Lat√™ncia: ${latency}ms | Total: ${total}ms`);

		// Emite para config-manager atualizar o placeholder com texto final e m√©tricas
		emitUIChange('onPlaceholderFulfill', {
			speaker: YOU,
			text,
			stopStr,
			startStr,
			recordingDuration,
			latency,
			total,
		});

		lastInputPlaceholderEl = null;
		lastInputStopAt = null;
		console.log('üóëÔ∏è Resetando timestamps: lastInputStartAt = null, lastInputStopAt = null');
		lastInputStartAt = null;
	} else {
		addTranscript(YOU, text);
	}

	handleSpeech(YOU, text, { skipAddToUI: true });

	debugLogRenderer('Fim da fun√ß√£o: "transcribeInput"');
}

async function transcribeOutput() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "transcribeOutput"');

	// Desabilitado temporariamente (teste)
	if (DESABILITADO_TEMPORARIAMENTE) {
		debugLogRenderer('Fim da fun√ß√£o: "transcribeOutput" üîí DESABILITADO TEMPORARIAMENTE');
		return;
	}

	// Se n√£o houver chunks de sa√≠da, retorna
	if (!outputChunks.length) {
		console.log('‚ö†Ô∏è transcribeOutput: nenhum chunk de sa√≠da dispon√≠vel');

		debugLogRenderer('Fim da fun√ß√£o: "transcribeOutput"');
		return;
	}

	// Cria um blob a partir dos chunks de sa√≠da
	const blob = new Blob(outputChunks, { type: 'audio/webm' });
	console.log('üéµ transcribeOutput: blob.size =', blob.size, 'bytes | chunks =', outputChunks.length);

	// Valida tamanho m√≠nimo dependendo do modo (evita ru√≠do / respira√ß√£o)
	const minSize = ModeController.isInterviewMode() ? MIN_OUTPUT_AUDIO_SIZE_INTERVIEW : MIN_OUTPUT_AUDIO_SIZE;
	if (blob.size < minSize) {
		console.log('‚ö†Ô∏è transcribeOutput: Blob muito pequeno (', blob.size, '/', minSize, ') - ignorando');

		debugLogRenderer('Fim da fun√ß√£o: "transcribeOutput"');
		return;
	}

	// Limpa o array de chunks de sa√≠da
	outputChunks = [];

	try {
		// Envia para transcri√ß√£o o blob de sa√≠da
		const text = await transcribeAudio(blob);
		console.log('üìù transcribeOutput: Transcri√ß√£o recebida: ', text);

		// Ignora transcri√ß√£o vazia
		if (!text || text.trim().length === 0) {
			console.log('‚ö†Ô∏è transcribeOutput: Transcri√ß√£o vazia - ignorando');
			return;
		}

		// ‚ö†Ô∏è [NOVO] Se √© lixo, loga mas N√ÉO retorna - deixa passar para aparecer na UI
		if (isGarbageSentence(text)) {
			console.log('üóëÔ∏è transcribeOutput: √â frase de lixo, mas permitindo que apare√ßa na Transcri√ß√£o:', text);
			// N√ÉO retorna! Deixa a frase passar para emitir placeholder
		}

		// Se existia um placeholder (timestamp do stop), atualiza esse placeholder com o texto final e lat√™ncia
		if (lastOutputPlaceholderEl && lastOutputPlaceholderEl.dataset) {
			console.log('üîÑ Atualizando placeholder com transcri√ß√£o final...');

			// üî• USAR VARI√ÅVEIS PENDENTES (imunes a race condition)
			// Essas vari√°veis foram capturadas em onstop() e n√£o foram sobrescritas por updateOutputVolume()
			const stop = pendingOutputStopAt || lastOutputStopAt;
			const start = pendingOutputStartAt || lastOutputStartAt || stop;

			// Debug: verificar se pending* foi usada
			console.log(
				'üî• DEBUG transcribeOutput: pendingOutputStopAt=' +
					pendingOutputStopAt +
					', pendingOutputStartAt=' +
					pendingOutputStartAt,
			);

			// calcula m√©tricas
			const now = Date.now();
			const recordingDuration = stop - start;
			const latency = now - stop;
			const total = now - start;
			const startStr = new Date(start).toLocaleTimeString();
			const stopStr = new Date(stop).toLocaleTimeString();
			const displayStr = new Date(now).toLocaleTimeString();

			// Log detalhado de timing
			console.log('‚è±Ô∏è TIMING COMPLETO (Output):');
			console.log(`  ‚úÖ In√≠cio: ${startStr}`);
			console.log(`  ‚èπÔ∏è Parada: ${stopStr}`);
			console.log(`  üì∫ Exibi√ß√£o: ${displayStr}`);
			console.log(`  üìä Dura√ß√£o grava√ß√£o: ${recordingDuration}ms | Lat√™ncia: ${latency}ms | Total: ${total}ms`);

			// Emite atualiza√ß√£o de UI ao placeholder com texto final e m√©tricas
			// üî• PASSA O ID DO PLACEHOLDER para que config-manager atualize o elemento CORRETO
			emitUIChange('onPlaceholderFulfill', {
				speaker: OTHER,
				text,
				stopStr,
				startStr,
				recordingDuration,
				latency,
				total,
				placeholderId: lastOutputPlaceholderId, // üî• ESSENCIAL para encontrar o placeholder correto
			});

			// reseta vari√°veis de placeholder
			lastOutputPlaceholderEl = null;
			// N√ÉO resetar lastOutputStopAt e lastOutputStartAt aqui!
			// Eles ser√£o preservados para timing correto da pr√≥xima frase
			// Ser√£o resetados apenas quando uma NOVA frase inicia em updateOutputVolume()
			console.log(
				'üßπ RESET #1: lastOutputPlaceholderEl resetado | lastOutputStartAt/StopAt PRESERVADOS para pr√≥xima frase',
			);

			// processa a fala transcrita (consolida√ß√£o de perguntas)
			// Usa Date.now() para pegar o tempo exato que chegou no renderer
			console.log('entrou aqui no if do placeholder existente');
			handleSpeech(OTHER, text, { skipAddToUI: true });
		} else {
			// Sem placeholder - cria placeholder e emite fulfill para garantir m√©tricas
			console.log('‚ûï Nenhum placeholder existente - criando e preenchendo com m√©tricas');
			// üî• USAR VARI√ÅVEIS PENDENTES (imunes a race condition)
			const stop = pendingOutputStopAt || lastOutputStopAt || Date.now();
			const start = pendingOutputStartAt || lastOutputStartAt || stop;

			// Debug: verificar se pending* foi usada
			console.log(
				'üî• DEBUG transcribeOutput: pendingOutputStopAt=' +
					pendingOutputStopAt +
					', pendingOutputStartAt=' +
					pendingOutputStartAt,
			);

			const now = Date.now();
			const recordingDuration = stop - start;
			const latency = now - stop;
			const total = now - start;
			const startStr = new Date(start).toLocaleTimeString();
			const stopStr = new Date(stop).toLocaleTimeString();
			const displayStr = new Date(now).toLocaleTimeString();

			// Log detalhado de timing
			console.log('‚è±Ô∏è TIMING COMPLETO (Output):');
			console.log(`  ‚úÖ In√≠cio: ${startStr}`);
			console.log(`  ‚èπÔ∏è Parada: ${stopStr}`);
			console.log(`  üì∫ Exibi√ß√£o: ${displayStr}`);
			console.log(`  üìä Dura√ß√£o grava√ß√£o: ${recordingDuration}ms | Lat√™ncia: ${latency}ms | Total: ${total}ms`);

			// cria um placeholder vis√≠vel antes de preencher (garante consist√™ncia com fluxo parcial)
			const elIdForFallback = 'placeholder-' + start + '-' + Math.random();
			const placeholderEl = addTranscript(OTHER, '...', start, elIdForFallback);

			if (placeholderEl && placeholderEl.dataset) {
				placeholderEl.dataset.startAt = start;
				placeholderEl.dataset.stopAt = stop;
			}

			// Emite atualiza√ß√£o final para preencher o placeholder com texto e m√©tricas
			// üî• PASSA O ID DO PLACEHOLDER para que config-manager atualize o elemento CORRETO
			emitUIChange('onPlaceholderFulfill', {
				speaker: OTHER,
				text,
				stopStr,
				startStr,
				recordingDuration,
				latency,
				total,
				placeholderId: elIdForFallback, // üî• ESSENCIAL para encontrar o placeholder correto
			});

			// reseta vari√°veis de placeholder
			console.log(
				'üßπ RESET #2: lastOutputPlaceholderEl resetado | lastOutputStartAt/StopAt PRESERVADOS para pr√≥xima frase',
			);
			lastOutputPlaceholderEl = null;
			// N√ÉO resetar lastOutputStopAt e lastOutputStartAt aqui!
			// Eles ser√£o preservados para timing correto da pr√≥xima frase

			// processa a fala transcrita (consolida√ß√£o de perguntas)
			// Usa Date.now() para pegar o tempo exato que chegou no renderer
			console.log('entrou aqui no else do placeholder inexistente');
			handleSpeech(OTHER, text, { skipAddToUI: true });
		}

		// üî• Limpar vari√°veis pendentes ap√≥s transcri√ß√£o completa
		// Elas j√° foram usadas para calcular m√©tricas, agora podem ser limpas
		console.log('üßπ RESET #3: Limpando pendingOutputStartAt e pendingOutputStopAt');
		pendingOutputStartAt = null;
		pendingOutputStopAt = null;

		// üî• [NOVO] MODO ENTREVISTA: Emitir evento de transcri√ß√£o completa
		// O listener em DOMContentLoaded cuidar√° do timer de auto-close
		if (ModeController.isInterviewMode() && currentQuestion.text) {
			console.log('üé§ transcribeOutput: Emitindo evento STT onTranscriptionComplete');

			// Emite evento para todas as camadas superiores (agn√≥stico ao modelo)
			emitSTTEvent('transcriptionComplete', {
				text: currentQuestion.text,
				speaker: OTHER,
				isFinal: true,
				model: 'vosk-or-openai', // Vosk/OpenAI compartilham este fluxo
			});
		}
	} catch (err) {
		console.warn('‚ö†Ô∏è erro na transcri√ß√£o (OUTPUT)', err);
	}

	debugLogRenderer('Fim da fun√ß√£o: "transcribeOutput"');
}

/* ===============================
   CONSOLIDA√á√ÉO DE PERGUNTAS
=============================== */

function handleSpeech(author, text, options = {}) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "handleSpeech"');

	const cleaned = text.replace(/√ä+|hum|ahn/gi, '').trim();
	console.log('üîä handleSpeech', { author, raw: text, cleaned });

	// ignora frases muito curtas
	if (cleaned.length < 3) return;

	// Usa o tempo exato que chegou no renderer (Date.now)
	const now = Date.now();

	if (author === OTHER) {
		// üëâ Se j√° existe uma pergunta finalizada,
		//    significa que uma NOVA pergunta come√ßou
		if (currentQuestion.finalized) {
			console.log(
				'‚ÑπÔ∏è Quest√£o anterior finalizada ‚Äî promovendo para a hist√≥ria e continuando a processar o novo discurso.',
			);
			promoteCurrentToHistory(currentQuestion.text);
		}

		// üß† Detecta in√≠cio de NOVA pergunta e fecha a anterior
		// ‚ö†Ô∏è IMPORTANTE: Consolida ANTES de fechar, para evitar perder falas intermidi√°rias
		if (
			currentQuestion.text &&
			looksLikeQuestion(cleaned) &&
			now - currentQuestion.lastUpdate > 500 &&
			!currentQuestion.finalized &&
			!isGarbageSentence(cleaned) // üî• N√ÉO consolidar lixo com pergunta real
		) {
			// üîÄ CONSOLIDA√á√ÉO: Adiciona a fala atual antes de fechar a pergunta anterior
			// Isso garante que "explique o que √©... Y" seja parte da pergunta "Vou come√ßar... X"
			console.log('üîÄ [IMPORTANTE] Consolidando nova fala com pergunta atual antes de fechar:', {
				current: currentQuestion.text,
				new: cleaned,
				currentLength: currentQuestion.text.length,
				newLength: cleaned.length,
				cleanedIsGarbage: isGarbageSentence(cleaned),
			});
			const beforeConsolidate = currentQuestion.text;
			currentQuestion.text += (currentQuestion.text ? ' ' : '') + cleaned;
			currentQuestion.lastUpdateTime = now;
			currentQuestion.lastUpdate = now;
			console.log('üîÄ [IMPORTANTE] Ap√≥s consolida√ß√£o:', {
				before: beforeConsolidate,
				after: currentQuestion.text,
				finalLength: currentQuestion.text.length,
			});

			closeCurrentQuestion();

			// üõë Retorna para evitar processar a mesma fala novamente abaixo
			renderCurrentQuestion();
			debugLogRenderer('Fim da fun√ß√£o: "handleSpeech"');
			return;
		}

		// evita criar novo turno se a transcri√ß√£o final for igual √† √∫ltima pergunta j√° enviada
		if (lastSentQuestionText && cleaned.trim() === lastSentQuestionText) {
			console.log('üîï transcri√ß√£o igual √† √∫ltima pergunta enviada ‚Äî ignorando novo turno');
			return;
		}

		// üî• [NOVO] Se a fala √© lixo (confirma√ß√£o, interjei√ß√£o), N√ÉO consolida em CURRENT
		// Mas ainda aparece na Transcri√ß√£o (porque onPlaceholderFulfill j√° foi emitido)
		if (isGarbageSentence(cleaned)) {
			console.log('üóëÔ∏è handleSpeech: frase √© lixo, N√ÉO consolidando em CURRENT =', cleaned);
			// N√£o retorna! Deixa processar abaixo caso precise
		}

		if (!currentQuestion.text) {
			currentQuestion.createdAt = Date.now();
			currentQuestion.lastUpdateTime = Date.now();
			interviewTurnId++; // üî• novo turno
		}

		// evita duplica√ß√£o quando a mesma frase parcial/final chega novamente
		if (currentQuestion.text && normalizeForCompare(currentQuestion.text) === normalizeForCompare(cleaned)) {
			console.log('üîÅ speech igual ao currentQuestion ‚Äî ignorando concatena√ß√£o');
		} else if (!isGarbageSentence(cleaned)) {
			// üî• [NOVO] S√≥ consolida se N√ÉO for lixo
			currentQuestion.text += (currentQuestion.text ? ' ' : '') + cleaned;
			currentQuestion.lastUpdateTime = now;
		}
		currentQuestion.lastUpdate = now;

		// üü¶ CURRENT vira sele√ß√£o padr√£o ao receber fala
		if (!selectedQuestionId) {
			selectedQuestionId = CURRENT_QUESTION_ID;
			clearAllSelections();
		}

		// üî• NOVO: Adiciona TUDO √† conversa visual em tempo real
		// (mesmo lixo, para o usu√°rio ver o que foi transcrito)
		console.log('üí¨ Adicionando √† conversa:', cleaned);
		if (!options.skipAddToUI) {
			addTranscript(OTHER, cleaned, now);
		} else {
			console.log('‚ö™ addTranscript pulado por skipAddToUI');
		}

		renderCurrentQuestion();
	}

	debugLogRenderer('Fim da fun√ß√£o: "handleSpeech"');
}

/**
 * üî• handleCurrentQuestion - Fluxo espec√≠fico para Deepgram OUTPUT
 * Similar ao handleSpeech, mas focado em consolidar transcri√ß√µes no CURRENT
 * sem l√≥gicas de fechamento ou detec√ß√£o de perguntas. Apenas concatena e renderiza.
 * Usado para interims e finais do Deepgram output.
 */
function handleCurrentQuestion(author, text, options = {}) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "handleCurrentQuestion"');

	const cleaned = text.replace(/√ä+|hum|ahn/gi, '').trim();
	console.log('üîä handleCurrentQuestion', { author, raw: text, cleaned, isInterim: options.isInterim });

	// ignora frases muito curtas
	if (cleaned.length < 3) return;

	// Usa o tempo exato que chegou no renderer (Date.now)
	const now = Date.now();

	if (author === OTHER) {
		// Inicializa timestamps se for a primeira fala
		if (!currentQuestion.text) {
			currentQuestion.createdAt = Date.now();
			currentQuestion.lastUpdateTime = Date.now();
			interviewTurnId++; // üî• novo turno
		}

		// L√≥gica de consolida√ß√£o para evitar duplica√ß√µes
		if (options.isInterim) {
			// Para interims: substituir o interim atual (Deepgram envia vers√µes progressivas)
			currentQuestion.interimText = cleaned;
		} else {
			// Para finais: substituir completamente o finalText e limpar interim
			currentQuestion.finalText = cleaned;
			currentQuestion.interimText = '';

			// üî• Limpar timer de sil√™ncio pois j√° temos final
			if (currentQuestionSilenceTimer) {
				clearTimeout(currentQuestionSilenceTimer);
				currentQuestionSilenceTimer = null;
			}
		}

		// Atualizar o texto total
		currentQuestion.text =
			currentQuestion.finalText +
			(currentQuestion.finalText && currentQuestion.interimText ? ' ' : '') +
			currentQuestion.interimText;

		currentQuestion.lastUpdateTime = now;
		currentQuestion.lastUpdate = now;

		// üî• TIMER DE SIL√äNCIO PARA CURRENT: Reiniciar timer se for interim
		if (options.isInterim) {
			if (currentQuestionSilenceTimer) clearTimeout(currentQuestionSilenceTimer);
			currentQuestionSilenceTimer = setTimeout(() => {
				console.log('‚è∞ CURRENT_QUESTION_SILENCE_TIMEOUT disparado: Finalizando pergunta por sil√™ncio');

				// üî• FINALIZA TRANSCRI√á√ÉO PENDENTE: Quando finalizamos por sil√™ncio, for√ßa final da transcri√ß√£o atual
				finalizePendingTranscription(currentQuestion.interimText, OTHER);

				finalizeCurrentQuestion();
			}, CURRENT_QUESTION_SILENCE_TIMEOUT);
		}

		// üü¶ CURRENT vira sele√ß√£o padr√£o ao receber fala
		if (!selectedQuestionId) {
			selectedQuestionId = CURRENT_QUESTION_ID;
			clearAllSelections();
		}

		// üî• Adiciona √† conversa visual em tempo real (sempre, para mostrar tudo)
		console.log('üí¨ handleCurrentQuestion: Adicionando √† conversa:', cleaned);

		renderCurrentQuestion();
	}

	debugLogRenderer('Fim da fun√ß√£o: "handleCurrentQuestion"');
}

/* ===============================
   RESET CURRENT QUESTION
=============================== */

function resetCurrentQuestion() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "resetCurrentQuestion"');

	currentQuestion = {
		text: '',
		lastUpdate: 0,
		finalized: false,
		lastUpdateTime: null,
		createdAt: null,
		finalText: '',
		interimText: '',
	};

	// üî• Limpar timer de sil√™ncio
	if (currentQuestionSilenceTimer) {
		clearTimeout(currentQuestionSilenceTimer);
		currentQuestionSilenceTimer = null;
	}

	debugLogRenderer('Fim da fun√ß√£o: "resetCurrentQuestion"');
}

/* ===============================
   FINALIZA√á√ÉO DE PERGUNTAS POR SIL√äNCIO (CURRENT TIMER)
=============================== */

/**
 * üî• finalizeCurrentQuestion - Finaliza pergunta atual por timeout de sil√™ncio
 * Chamada quando n√£o h√° novos interims por CURRENT_QUESTION_SILENCE_TIMEOUT
 */
function finalizeCurrentQuestion() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "finalizeCurrentQuestion"');

	// Limpar timer
	if (currentQuestionSilenceTimer) {
		clearTimeout(currentQuestionSilenceTimer);
		currentQuestionSilenceTimer = null;
	}

	// Se n√£o h√° texto, ignorar
	if (!currentQuestion.text || !currentQuestion.text.trim()) {
		console.log('‚ö†Ô∏è finalizeCurrentQuestion: Sem texto para finalizar');
		return;
	}

	console.log('‚úÖ finalizeCurrentQuestion: Finalizando pergunta por sil√™ncio:', currentQuestion.text);

	// üîí GUARDA ABSOLUTA: Se a pergunta j√° foi finalizada, N√ÉO fa√ßa nada.
	if (currentQuestion.finalized) {
		console.log('‚õî finalizeCurrentQuestion ignorado ‚Äî pergunta j√° finalizada');
		return;
	}

	// Trata perguntas incompletas
	if (isIncompleteQuestion(currentQuestion.text)) {
		// üî• No modo entrevista, ignorar incompleta e for√ßar finaliza√ß√£o
		if (ModeController.isInterviewMode()) {
			console.log('‚ö†Ô∏è pergunta incompleta detectada, mas modo entrevista ativo ‚Äî for√ßando finaliza√ß√£o');
		} else {
			console.log('‚ö†Ô∏è pergunta incompleta detectada ‚Äî promovendo ao hist√≥rico como incompleta:', currentQuestion.text);

			const newId = String(questionsHistory.length + 1);
			questionsHistory.push({
				id: newId,
				text: currentQuestion.text,
				createdAt: currentQuestion.createdAt || Date.now(),
				lastUpdateTime: currentQuestion.lastUpdateTime || currentQuestion.createdAt || Date.now(),
				incomplete: true,
			});

			selectedQuestionId = newId;
			resetCurrentQuestion();
			renderQuestionsHistory();
			return;
		}
	}

	// Verifica se parece uma pergunta
	if (!looksLikeQuestion(currentQuestion.text)) {
		// ‚ö†Ô∏è No modo entrevista, N√ÉO abortar o fechamento
		if (ModeController.isInterviewMode()) {
			console.log('‚ö†Ô∏è looksLikeQuestion=false, mas modo entrevista ativo ‚Äî for√ßando fechamento');

			currentQuestion.text = finalizeQuestion(currentQuestion.text);
			currentQuestion.lastUpdateTime = Date.now();
			currentQuestion.finalized = true;

			// garante sele√ß√£o l√≥gica
			selectedQuestionId = CURRENT_QUESTION_ID;

			// chama GPT automaticamente se ainda n√£o respondeu este turno
			if (gptRequestedTurnId !== interviewTurnId && gptAnsweredTurnId !== interviewTurnId) {
				console.log('‚û°Ô∏è finalizeCurrentQuestion (fallback) chamou askGpt', {
					interviewTurnId,
					gptRequestedTurnId,
					gptAnsweredTurnId,
				});

				askGpt();
				resetCurrentQuestion();
			}
			return;
		}

		// modo normal mant√©m comportamento atual
		resetCurrentQuestion();
		renderCurrentQuestion();
		return;
	}

	// ‚úÖ consolida a pergunta
	currentQuestion.text = finalizeQuestion(currentQuestion.text);
	currentQuestion.lastUpdateTime = Date.now();
	currentQuestion.finalized = true;

	// üî• COMPORTAMENTO POR MODO
	if (ModeController.isInterviewMode()) {
		if (gptRequestedTurnId !== interviewTurnId && gptAnsweredTurnId !== interviewTurnId) {
			selectedQuestionId = CURRENT_QUESTION_ID;

			console.log('‚û°Ô∏è finalizeCurrentQuestion chamou askGpt (vou enviar para o GPT)', {
				interviewTurnId,
				gptRequestedTurnId,
				gptAnsweredTurnId,
			});

			askGpt();
		}
	} else {
		console.log('üîµ modo NORMAL ‚Äî promovendo CURRENT para hist√≥rico sem chamar GPT');

		promoteCurrentToHistory(currentQuestion.text);
		resetCurrentQuestion();
		renderCurrentQuestion();
	}

	debugLogRenderer('Fim da fun√ß√£o: "finalizeCurrentQuestion"');
}

/* ===============================
   FECHAMENTO DE PERGUNTAS
=============================== */

function closeCurrentQuestion() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "closeCurrentQuestion"');

	// üîí GUARDA ABSOLUTA:
	// Se a pergunta j√° foi finalizada, N√ÉO fa√ßa nada.
	if (currentQuestion.finalized) {
		console.log('‚õî closeCurrentQuestion ignorado ‚Äî pergunta j√° finalizada');
		return;
	}

	// Garante que lastUpdateTime seja definido quando se tenta fechar
	if (!currentQuestion.lastUpdateTime && currentQuestion.text) {
		currentQuestion.lastUpdateTime = Date.now();
	}

	console.log('üö™ closeCurrentQuestion called', {
		interviewTurnId,
		gptAnsweredTurnId,
		currentQuestionText: currentQuestion.text,
	});

	// trata perguntas incompletas
	if (isIncompleteQuestion(currentQuestion.text)) {
		console.log('‚ö†Ô∏è pergunta incompleta detectada ‚Äî promovendo ao hist√≥rico como incompleta:', currentQuestion.text);

		const newId = String(questionsHistory.length + 1);
		questionsHistory.push({
			id: newId,
			text: currentQuestion.text,
			createdAt: currentQuestion.createdAt || Date.now(),
			lastUpdateTime: currentQuestion.lastUpdateTime || currentQuestion.createdAt || Date.now(),
			incomplete: true,
		});

		selectedQuestionId = newId;

		currentQuestion.text = '';
		currentQuestion.lastUpdateTime = null;
		currentQuestion.createdAt = null;
		currentQuestion.finalized = false;

		renderQuestionsHistory();
		renderCurrentQuestion();
		return;
	}

	if (!looksLikeQuestion(currentQuestion.text)) {
		// ‚ö†Ô∏è No modo entrevista, N√ÉO abortar o fechamento
		if (ModeController.isInterviewMode()) {
			console.log('‚ö†Ô∏è looksLikeQuestion=false, mas modo entrevista ativo ‚Äî for√ßando fechamento');

			currentQuestion.text = finalizeQuestion(currentQuestion.text);
			currentQuestion.lastUpdateTime = Date.now();
			currentQuestion.finalized = true;

			// garante sele√ß√£o l√≥gica
			selectedQuestionId = CURRENT_QUESTION_ID;

			// chama GPT automaticamente se ainda n√£o respondeu este turno
			if (gptRequestedTurnId !== interviewTurnId && gptAnsweredTurnId !== interviewTurnId) {
				console.log('‚û°Ô∏è closeCurrentQuestion (fallback) chamou askGpt', {
					interviewTurnId,
					gptRequestedTurnId,
					gptAnsweredTurnId,
				});

				//console.error('closeCurrentQuestion: askGpt() 2281; üîí COMENTADA at√© transcri√ß√£o em tempo real funcionar');
				askGpt(); // üîí COMENTADA at√© transcri√ß√£o em tempo real funcionar
			}

			return;
		}

		// modo normal mant√©m comportamento atual
		currentQuestion.text = '';
		currentQuestion.lastUpdateTime = null;
		currentQuestion.createdAt = null;
		currentQuestion.finalized = false;
		renderCurrentQuestion();
		return;
	}

	// ‚úÖ consolida a pergunta
	currentQuestion.text = finalizeQuestion(currentQuestion.text);
	currentQuestion.lastUpdateTime = Date.now();
	currentQuestion.finalized = true;

	// ‚ö†Ô∏è NUNCA renderizar aqui no modo entrevista
	if (!ModeController.isInterviewMode()) {
		renderCurrentQuestion();
	}

	// üî• COMPORTAMENTO POR MODO
	if (ModeController.isInterviewMode()) {
		if (gptRequestedTurnId !== interviewTurnId && gptAnsweredTurnId !== interviewTurnId) {
			selectedQuestionId = CURRENT_QUESTION_ID;

			console.log('‚û°Ô∏è closeCurrentQuestion chamou askGpt (vou enviar para o GPT)', {
				interviewTurnId,
				gptRequestedTurnId,
				gptAnsweredTurnId,
			});

			//console.error('closeCurrentQuestion: askGpt() 2318; üîí COMENTADA at√© transcri√ß√£o em tempo real funcionar');
			askGpt(); // üîí COMENTADA at√© transcri√ß√£o em tempo real funcionar
		}
	} else {
		console.log('üîµ modo NORMAL ‚Äî promovendo CURRENT para hist√≥rico sem chamar GPT');

		promoteCurrentToHistory(currentQuestion.text);

		currentQuestion.text = '';
		currentQuestion.lastUpdateTime = null;
		currentQuestion.createdAt = null;
		currentQuestion.finalized = false;

		renderCurrentQuestion();
	}

	debugLogRenderer('Fim da fun√ß√£o: "closeCurrentQuestion"');
}

function closeCurrentQuestionForced() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "closeCurrentQuestionForced"');

	// log temporario para testar a aplica√ß√£o s√≥ remover depois
	console.log('üö™ Fechando pergunta:', currentQuestion.text);

	resetInterviewTurnState();

	if (!currentQuestion.text) return;

	questionsHistory.push({
		id: crypto.randomUUID(),
		text: finalizeQuestion(currentQuestion.text),
		createdAt: currentQuestion.createdAt || Date.now(),
	});

	currentQuestion.text = '';
	selectedQuestionId = null; // üëà libera sele√ß√£o
	renderQuestionsHistory();
	renderCurrentQuestion();

	debugLogRenderer('Fim da fun√ß√£o: "closeCurrentQuestionForced"');
}

/* ===============================
   GPT
=============================== */
async function askGpt() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "askGpt"');

	// Desabilitado temporariamente (teste)
	if (DESABILITADO_TEMPORARIAMENTE) {
		debugLogRenderer('Fim da fun√ß√£o: "askGpt" üîí DESABILITADO TEMPORARIAMENTE');
		return;
	}

	const text = getSelectedQuestionText();

	// üî• Valida√ß√µes rigorosas para impedir lixo
	if (!text || text.trim().length < 5) {
		updateStatusMessage('‚ö†Ô∏è Pergunta vazia ou incompleta');
		return;
	}

	// Detecta se √© lixo ANTES de enviar ao GPT
	if (isGarbageSentence(text)) {
		console.log('üö´ askGpt bloqueado: texto √© lixo =', text);
		updateStatusMessage('‚ö†Ô∏è Frase n√£o √© uma pergunta v√°lida');
		return;
	}

	// Verifica se tem uma pergunta real ("?" ou come√ßa com palavra t√≠pica)
	if (!looksLikeQuestion(text)) {
		console.log('üö´ askGpt bloqueado: n√£o parece pergunta =', text);
		updateStatusMessage('‚ö†Ô∏è Frase n√£o √© uma pergunta (falta ? ou come√ßo de pergunta)');

		// No modo entrevista, for√ßa mesmo assim (permitir perguntas um pouco imprecisas)
		if (!ModeController.isInterviewMode()) {
			return;
		}
		console.log('‚ÑπÔ∏è modo entrevista: enviando mesmo assim...');
	}

	const isCurrent = selectedQuestionId === CURRENT_QUESTION_ID;
	const normalizedText = normalizeForCompare(text);

	// Evita reenvio da mesma pergunta atual ao GPT (dedupe)
	if (isCurrent && normalizedText && lastAskedQuestionNormalized === normalizedText) {
		updateStatusMessage('‚õî Pergunta j√° enviada');
		console.log('‚õî askGpt: mesma pergunta j√° enviada, pulando');
		return;
	}
	const questionId = isCurrent ? CURRENT_QUESTION_ID : selectedQuestionId;

	// üõ°Ô∏è MODO ENTREVISTA ‚Äî bloqueia duplica√ß√£o APENAS para hist√≥rico
	if (ModeController.isInterviewMode() && !isCurrent) {
		const existingAnswer = findAnswerByQuestionId(questionId);
		if (existingAnswer) {
			// emitUIChange('onAnswerAdd', {
			// 	questionId,
			// 	action: 'showExisting',
			// });
			updateStatusMessage('üìå Essa pergunta j√° foi respondida');
			return;
		}
	}

	// limpa destaque
	// emitUIChange('onAnswerAdd', {
	// 	questionId,
	// 	action: 'clearActive',
	// });

	// log temporario para testar a aplica√ß√£o s√≥ remover depois
	console.log('ü§ñ askGpt chamado | questionId:', selectedQuestionId);
	console.log('üß™ GPT RECEBERIA:', text);

	console.log('üßæ askGpt diagn√≥stico', {
		textLength: text.length,
		selectedQuestionId,
		questionId_variable: questionId, // üî• DEBUG: mostrar a vari√°vel questionId
		isInterviewMode: ModeController.isInterviewMode(),
		interviewTurnId,
		gptAnsweredTurnId,
	});

	// marca que este turno teve uma requisi√ß√£o ao GPT (apenas para CURRENT)
	if (isCurrent) {
		gptRequestedTurnId = interviewTurnId;
		gptRequestedQuestionId = CURRENT_QUESTION_ID; // üî• [IMPORTANTE] Rastreia qual pergunta foi solicitada
		lastAskedQuestionNormalized = normalizedText;
		console.log('‚ÑπÔ∏è gptRequestedTurnId definido para turno', gptRequestedTurnId);
		console.log('‚ÑπÔ∏è gptRequestedQuestionId definido para:', gptRequestedQuestionId);
		lastSentQuestionText = text.trim();
		console.log('‚ÑπÔ∏è lastSentQuestionText definido:', lastSentQuestionText);
	}

	// Inicia medi√ß√£o do GPT
	transcriptionMetrics.gptStartTime = Date.now();

	// ÔøΩ MODO ENTREVISTA ‚Äî STREAMING
	if (ModeController.isInterviewMode()) {
		const gptStartAt = ENABLE_INTERVIEW_TIMING_DEBUG ? Date.now() : null;
		let streamedText = '';

		console.log('‚è≥ enviando para o GPT via stream...');

		// üî• N√£o preparar bloco antes - deixar o primeiro token criar (mais r√°pido!)

		ipcRenderer
			.invoke('ask-gpt-stream', [
				{ role: 'system', content: SYSTEM_PROMPT },
				{ role: 'user', content: text },
			])
			.catch(err => {
				console.error('‚ùå Erro ao chamar ask-gpt-stream:', err);
				updateStatusMessage('‚ùå Erro ao enviar para GPT');
			});

		const onChunk = (_, token) => {
			streamedText += token;

			// üî• PROTE√á√ÉO: Valida se o questionId ainda √© v√°lido
			// (evita renderizar em question ID antigo/inv√°lido)
			if (
				!questionId ||
				(isCurrent && gptRequestedQuestionId !== CURRENT_QUESTION_ID) ||
				(!isCurrent && !questionsHistory.find(q => q.id === questionId))
			) {
				console.warn('üö® onChunk: questionId inv√°lido ou desatualizado, ignorando token:', {
					questionId,
					isCurrent,
					gptRequestedQuestionId,
					token,
				});
				return;
			}

			// üî• DEBUG: Log para rastrear qual questionId est√° sendo enviado
			if (streamedText.length <= 50) {
				console.log('üé¨ [onChunk] Enviando para onAnswerStreamChunk:', {
					questionId,
					gptRequestedQuestionId,
					token,
					accumLength: streamedText.length,
				});
			}

			emitUIChange('onAnswerStreamChunk', {
				questionId,
				token,
				accum: streamedText,
			});
			console.log('üü¢ GPT_STREAM_CHUNK recebido (token parcial)', token);
		};

		const onEnd = () => {
			console.log('‚úÖ GPT_STREAM_END recebido (stream finalizado)');
			ipcRenderer.removeListener('GPT_STREAM_CHUNK', onChunk);
			ipcRenderer.removeListener('GPT_STREAM_END', onEnd);

			// Finaliza medi√ß√µes
			transcriptionMetrics.gptEndTime = Date.now();
			transcriptionMetrics.totalTime = Date.now() - transcriptionMetrics.audioStartTime;

			// Log m√©tricas
			logTranscriptionMetrics();

			let finalText = streamedText;
			if (ENABLE_INTERVIEW_TIMING_DEBUG && gptStartAt) {
				const endAt = Date.now();
				const elapsed = endAt - gptStartAt;

				const startTime = new Date(gptStartAt).toLocaleTimeString();
				const endTime = new Date(endAt).toLocaleTimeString();

				finalText +=
					`\n\n‚è±Ô∏è GPT iniciou: ${startTime}` + `\n‚è±Ô∏è GPT finalizou: ${endTime}` + `\n‚è±Ô∏è Resposta em ${elapsed}ms`;
			}

			// garante que o turno foi realmente fechado
			const wasRequestedForThisTurn = gptRequestedTurnId === interviewTurnId;
			const requestedQuestionId = gptRequestedQuestionId; // üî• Qual pergunta foi REALMENTE solicitada

			gptAnsweredTurnId = interviewTurnId;
			gptRequestedTurnId = null;
			gptRequestedQuestionId = null; // üî• Limpa ap√≥s usar

			// üîí RENDERIZAR A RESPOSTA COM O ID CORRETO
			if (requestedQuestionId) {
				// const finalHtml = marked.parse(finalText); // Resposta j√° renderizada via streaming

				console.log('‚úÖ GPT_STREAM_END: Renderizando resposta para pergunta solicitada:', {
					requestedQuestionId,
					wasRequestedForThisTurn,
				});

				// Se a pergunta solicitada foi CURRENT, promover para history ANTES de renderizar
				if (requestedQuestionId === CURRENT_QUESTION_ID && currentQuestion.text) {
					console.log('üîÑ GPT_STREAM_END: Promovendo CURRENT para history antes de renderizar resposta');
					promoteCurrentToHistory(currentQuestion.text);

					// Pega a pergunta rec√©m-promovida
					const promotedQuestion = questionsHistory[questionsHistory.length - 1];
					if (promotedQuestion) {
						// Renderiza com o ID da pergunta promovida
						// renderGptAnswer(promotedQuestion.id, finalHtml); // Resposta j√° renderizada via streaming
						promotedQuestion.answered = true;
						answeredQuestions.add(promotedQuestion.id);
						renderQuestionsHistory();
						console.log('‚úÖ Resposta renderizada para pergunta promovida:', promotedQuestion.id);
					} else {
						console.warn('‚ö†Ô∏è Pergunta promovida n√£o encontrada');
						// renderGptAnswer(requestedQuestionId, finalHtml); // Resposta j√° renderizada via streaming
					}
				} else {
					// Para perguntas do hist√≥rico, renderiza com o ID recebido
					// renderGptAnswer(requestedQuestionId, finalHtml); // Resposta j√° renderizada via streaming
					answeredQuestions.add(requestedQuestionId);

					// Se for do hist√≥rico, atualiza o flag tamb√©m
					if (requestedQuestionId !== CURRENT_QUESTION_ID) {
						try {
							const q = questionsHistory.find(x => x.id === requestedQuestionId);
							if (q) {
								q.answered = true;
								renderQuestionsHistory();
							}
						} catch (err) {
							console.warn('‚ö†Ô∏è falha ao marcar pergunta como respondida:', err);
						}
					}
				}

				resetInterviewTurnState();
			} else {
				// üî• Nenhuma pergunta foi rastreada como solicitada
				console.warn('‚ö†Ô∏è GPT_STREAM_END mas nenhuma pergunta solicitada foi encontrada');
				resetInterviewTurnState();
			}

			// üî• Notificar config-manager que stream terminou (para limpar info de streaming)
			globalThis.RendererAPI?.emitUIChange?.('onAnswerStreamEnd', {});
		};

		ipcRenderer.on('GPT_STREAM_CHUNK', onChunk);
		ipcRenderer.once('GPT_STREAM_END', onEnd);
		return;
	}

	// üîµ MODO NORMAL ‚Äî BATCH
	console.log('‚è≥ enviando para o GPT (batch)...');
	const res = await ipcRenderer.invoke('ask-gpt', [
		{ role: 'system', content: SYSTEM_PROMPT },
		{ role: 'user', content: text },
	]);

	console.log('‚úÖ resposta do GPT recebida (batch)');

	// Finaliza medi√ß√µes
	transcriptionMetrics.gptEndTime = Date.now();
	transcriptionMetrics.totalTime = Date.now() - transcriptionMetrics.audioStartTime;

	// Log m√©tricas
	logTranscriptionMetrics();

	// üî• COMENTADO: renderGptAnswer(questionId, res);
	// Apenas streaming ser√° exibido

	const wasRequestedForThisTurn = gptRequestedTurnId === interviewTurnId;

	console.log(
		'‚ÑπÔ∏è gptRequestedTurnId antes do batch:',
		gptRequestedTurnId,
		'wasRequestedForThisTurn:',
		wasRequestedForThisTurn,
	);

	// üîí FECHAMENTO AT√îMICO DO CICLO
	if (isCurrent && wasRequestedForThisTurn) {
		promoteCurrentToHistory(text);
		// ap√≥s promover para o hist√≥rico, a pergunta j√° est√° no hist√≥rico e resposta vinculada
		try {
			// Encontra a √∫ltima pergunta adicionada (que acabamos de promover)
			const q = questionsHistory[questionsHistory.length - 1];
			if (q) {
				q.answered = true;
				renderQuestionsHistory();
			}
		} catch (err) {
			console.warn('‚ö†Ô∏è falha ao marcar pergunta como respondida (batch):', err);
		}
	}

	// marca que o GPT respondeu esse turno (batch)
	gptAnsweredTurnId = interviewTurnId;
	gptRequestedTurnId = null;

	debugLogRenderer('Fim da fun√ß√£o: "askGpt"');
}

/* ===============================
   UI (RENDER / SELE√á√ÉO / SCROLL)
=============================== */

function addTranscript(author, text, time, elementId = null) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "addTranscript"');
	let timeStr;
	if (time) {
		if (typeof time === 'number') timeStr = new Date(time).toLocaleTimeString();
		else if (time instanceof Date) timeStr = time.toLocaleTimeString();
		else timeStr = String(time);
	} else {
		timeStr = new Date().toLocaleTimeString();
	}

	// üî• Apenas EMITE o evento com os dados
	// config-manager.js √© respons√°vel por adicionar ao DOM
	const transcriptData = {
		author,
		text,
		timeStr,
		elementId: 'conversation',
		placeholderId: elementId, // üî• PASSAR ID PARA SER ATRIBU√çDO AO ELEMENTO REAL
	};

	emitUIChange('onTranscriptAdd', transcriptData);

	// Retorna um objeto proxy que simula um elemento DOM para compatibilidade
	// Usado quando a transcri√ß√£o √© um placeholder que ser√° atualizado depois
	const placeholderProxy = {
		dataset: {
			startAt: typeof time === 'number' ? time : Date.now(),
			stopAt: null,
		},
		// Permite que c√≥digo posterior trate como elemento DOM
		classList: {
			add: () => {},
			remove: () => {},
			contains: () => false,
			toggle: () => false,
		},
	};

	debugLogRenderer('Fim da fun√ß√£o: "addTranscript"');
	return placeholderProxy;
}

function renderCurrentQuestion() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "renderCurrentQuestion"');

	// Desabilitado temporariamente (teste)
	if (DESABILITADO_TEMPORARIAMENTE) {
		debugLogRenderer('Fim da fun√ß√£o: "renderCurrentQuestion" üîí DESABILITADO TEMPORARIAMENTE');
		return;
	}

	if (!currentQuestion.text) {
		emitUIChange('onCurrentQuestionUpdate', { text: '', isSelected: false });
		return;
	}

	let label = currentQuestion.text;

	if (ENABLE_INTERVIEW_TIMING_DEBUG && currentQuestion.lastUpdateTime) {
		const time = new Date(currentQuestion.lastUpdateTime).toLocaleTimeString();
		label = `‚è±Ô∏è ${time} ‚Äî ${label}`;
	}

	// üî• Apenas EMITE dados - config-manager aplica ao DOM
	const questionData = {
		text: label,
		isSelected: selectedQuestionId === CURRENT_QUESTION_ID,
		rawText: currentQuestion.text,
		createdAt: currentQuestion.createdAt,
		lastUpdateTime: currentQuestion.lastUpdateTime,
	};

	console.log(`üì§ renderCurrentQuestion: emitindo onCurrentQuestionUpdate`, {
		label,
		isSelected: selectedQuestionId === CURRENT_QUESTION_ID,
	});
	emitUIChange('onCurrentQuestionUpdate', questionData);

	debugLogRenderer('Fim da fun√ß√£o: "renderCurrentQuestion"');
}

function renderQuestionsHistory() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "renderQuestionsHistory"');

	// Desabilitado temporariamente (teste)
	if (DESABILITADO_TEMPORARIAMENTE) {
		debugLogRenderer('Fim da fun√ß√£o: "renderCurrentQuestion" üîí DESABILITADO TEMPORARIAMENTE');
		return;
	}

	// üî• Gera dados estruturados - config-manager renderiza no DOM
	const historyData = [...questionsHistory].reverse().map(q => {
		let label = q.text;
		if (ENABLE_INTERVIEW_TIMING_DEBUG && q.lastUpdateTime) {
			const time = new Date(q.lastUpdateTime).toLocaleTimeString();
			label = `‚è±Ô∏è ${time} ‚Äî ${label}`;
		}

		return {
			id: q.id,
			text: label,
			isIncomplete: q.incomplete,
			isAnswered: q.answered,
			isSelected: q.id === selectedQuestionId,
		};
	});

	emitUIChange('onQuestionsHistoryUpdate', historyData);

	scrollToSelectedQuestion();

	debugLogRenderer('Fim da fun√ß√£o: "renderQuestionsHistory"');
}

function clearAllSelections() {
	// Emite evento para o controller limpar as sele√ß√µes visuais
	emitUIChange('onClearAllSelections', {});
}

function scrollToSelectedQuestion() {
	emitUIChange('onScrollToQuestion', {
		questionId: selectedQuestionId,
	});
}

function getSelectedQuestionText() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "getSelectedQuestionText"');
	// 1Ô∏è‚É£ Se existe sele√ß√£o expl√≠cita
	if (selectedQuestionId === CURRENT_QUESTION_ID) {
		return currentQuestion.text;
	}

	if (selectedQuestionId) {
		const q = questionsHistory.find(q => q.id === selectedQuestionId);
		if (q?.text) return q.text;
	}

	// 2Ô∏è‚É£ Fallback: CURRENT (se tiver texto)
	if (currentQuestion.text && currentQuestion.text.trim().length > 0) {
		return currentQuestion.text;
	}

	debugLogRenderer('Fim da fun√ß√£o: "getSelectedQuestionText"');
	return '';
}

// üî• NOVO: Verifica se existe um modelo de IA ativo e retorna o nome do modelo
function hasActiveModel() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "hasActiveModel"');
	if (!window.configManager) {
		console.warn('‚ö†Ô∏è ConfigManager n√£o inicializado ainda');
		return { active: false, model: null };
	}

	const config = window.configManager.config;
	if (!config || !config.api) {
		console.warn('‚ö†Ô∏è Config ou api n√£o dispon√≠vel');
		return { active: false, model: null };
	}

	// Verifica se algum modelo est√° ativo e retorna o nome
	const providers = ['openai', 'google', 'openrouter', 'custom'];
	for (const provider of providers) {
		if (config.api[provider] && config.api[provider].enabled === true) {
			console.log(`‚úÖ Modelo ativo encontrado: ${provider}`);
			return { active: true, model: provider };
		}
	}

	console.warn('‚ö†Ô∏è Nenhum modelo ativo encontrado');

	debugLogRenderer('Fim da fun√ß√£o: "hasActiveModel"');
	return { active: false, model: null };
}

// Fun√ß√£o principal para o bot√£o de iniciar/parar escuta (Come√ßar a Ouvir... (Ctrl+d))
async function listenToggleBtn() {
	debugLogRenderer('In√≠cio da fun√ß√£o: "listenToggleBtn"');

	if (!isRunning) {
		console.log('üé§ listenToggleBtn: Tentando INICIAR escuta...');

		// üî• VALIDA√á√ÉO 1: Modelo de IA ativo
		const { active: hasModel, model: activeModel } = hasActiveModel();
		console.log(`üìä DEBUG: hasModel = ${hasModel}, activeModel = ${activeModel}`);

		if (!hasModel) {
			const errorMsg = 'Ative um modelo de IA antes de come√ßar a ouvir';
			console.warn(`‚ö†Ô∏è ${errorMsg}`);
			console.log('üì° DEBUG: Emitindo onError:', errorMsg);
			emitUIChange('onError', errorMsg);
			return;
		}

		// üî• VALIDA√á√ÉO 2: Dispositivo de √°udio de SA√çDA (obrigat√≥rio para ouvir a reuni√£o)
		const hasOutputDevice = UIElements.outputSelect?.value;
		console.log(`üìä DEBUG: hasOutputDevice = ${hasOutputDevice}`);

		if (!hasOutputDevice) {
			const errorMsg = 'Selecione um dispositivo de √°udio (output) para ouvir a reuni√£o';
			console.warn(`‚ö†Ô∏è ${errorMsg}`);
			console.log('üì° DEBUG: Emitindo onError:', errorMsg);
			emitUIChange('onError', errorMsg);
			return;
		}
	}

	// Inverte o estado de isRunning
	isRunning = !isRunning;
	const buttonText = isRunning ? 'Parar a Escuta... (Ctrl+d)' : 'Come√ßar a Ouvir... (Ctrl+d)';
	const statusMsg = isRunning ? 'Status: ouvindo...' : 'Status: parado';

	// Emite o evento 'onListenButtonToggle' para atualizar o bot√£o de escuta
	emitUIChange('onListenButtonToggle', {
		isRunning,
		buttonText,
	});

	// Atualiza o status da escuta na tela
	updateStatusMessage(statusMsg);

	console.log(`üé§ Listen toggle: ${isRunning ? 'INICIANDO' : 'PARANDO'}`);
	await (isRunning ? startAudio() : stopAudio());

	debugLogRenderer('Fim da fun√ß√£o: "listenToggleBtn"');
}

function handleQuestionClick(questionId) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "handleQuestionClick"');
	selectedQuestionId = questionId;
	clearAllSelections();
	renderQuestionsHistory();
	renderCurrentQuestion();

	// ‚ö†Ô∏è CURRENT nunca bloqueia resposta
	if (questionId !== CURRENT_QUESTION_ID) {
		const existingAnswer = findAnswerByQuestionId(questionId);

		if (existingAnswer) {
			emitUIChange('onAnswerSelected', {
				questionId: questionId,
				shouldScroll: true,
			});

			updateStatusMessage('üìå Essa pergunta j√° foi respondida');
			return;
		}
	}

	// Se for uma pergunta do hist√≥rico marcada como incompleta, n√£o enviar automaticamente ao GPT
	if (questionId !== CURRENT_QUESTION_ID) {
		const q = questionsHistory.find(q => q.id === questionId);
		if (q && q.incomplete) {
			updateStatusMessage('‚ö†Ô∏è Pergunta incompleta ‚Äî pressione o bot√£o de responder para enviar ao GPT');
			console.log('‚ÑπÔ∏è pergunta incompleta selecionada ‚Äî aguarda envio manual:', q.text);
			return;
		}
	}

	if (
		ModeController.isInterviewMode() &&
		selectedQuestionId === CURRENT_QUESTION_ID &&
		gptAnsweredTurnId === interviewTurnId
	) {
		updateStatusMessage('‚õî GPT j√° respondeu esse turno');
		console.log('‚õî GPT j√° respondeu esse turno');
		return;
	}

	// ‚ùì Ainda n√£o respondida ‚Üí chama GPT (click ou atalho)
	//console.error('closeCurrentQuestion: askGpt() 2978; üîí COMENTADA at√© transcri√ß√£o em tempo real funcionar');
	askGpt(); // üîí COMENTADA at√© transcri√ß√£o em tempo real funcionar

	debugLogRenderer('Fim da fun√ß√£o: "handleQuestionClick"');
}

function applyOpacity(value) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "applyOpacity"');
	const appOpacity = parseFloat(value);

	// aplica opacidade no conte√∫do geral
	document.documentElement.style.setProperty('--app-opacity', appOpacity.toFixed(2));

	// topBar nunca abaixo de 0.75
	const topbarOpacity = Math.max(appOpacity, 0.75);
	document.documentElement.style.setProperty('--app-opacity-75', topbarOpacity.toFixed(2));

	localStorage.setItem('overlayOpacity', appOpacity);

	// logs tempor√°rios para debug
	console.log('üéöÔ∏è Opacity change | app:', value, '| topBar:', topbarOpacity);

	debugLogRenderer('Fim da fun√ß√£o: "applyOpacity"');
}

// üî• Novo: atualizar status sem tocar em DOM
function updateStatusMessage(message) {
	debugLogRenderer('In√≠cio da fun√ß√£o: "updateStatusMessage"');
	emitUIChange('onStatusUpdate', { message });
	debugLogRenderer('Fim da fun√ß√£o: "updateStatusMessage"');
}

/* ===============================
   SCREENSHOT CAPTURE - FUN√á√ïES
=============================== */

/**
 * Captura screenshot discretamente e armazena em mem√≥ria
 */
async function captureScreenshot() {
	if (isCapturing) {
		console.log('‚è≥ Captura j√° em andamento...');
		return;
	}

	isCapturing = true;
	updateStatusMessage('üì∏ Capturando tela...');

	try {
		const result = await ipcRenderer.invoke('CAPTURE_SCREENSHOT');

		if (!result.success) {
			console.warn('‚ö†Ô∏è Falha na captura:', result.error);
			updateStatusMessage(`‚ùå ${result.error}`);
			emitUIChange('onScreenshotBadgeUpdate', {
				count: capturedScreenshots.length,
				visible: capturedScreenshots.length > 0,
			});
			return;
		}

		// ‚úÖ Armazena refer√™ncia do screenshot
		capturedScreenshots.push({
			filepath: result.filepath,
			filename: result.filename,
			timestamp: result.timestamp,
			size: result.size,
		});

		console.log(`‚úÖ Screenshot capturado: ${result.filename}`);
		console.log(`üì¶ Total em mem√≥ria: ${capturedScreenshots.length}`);

		// Atualiza UI
		updateStatusMessage(`‚úÖ ${capturedScreenshots.length} screenshot(s) capturado(s)`);
		emitUIChange('onScreenshotBadgeUpdate', {
			count: capturedScreenshots.length,
			visible: true,
		});
	} catch (error) {
		console.error('‚ùå Erro ao capturar screenshot:', error);
		updateStatusMessage('‚ùå Erro na captura');
	} finally {
		isCapturing = false;
	}
}

/**
 * Envia screenshots para an√°lise com OpenAI Vision
 */
async function analyzeScreenshots() {
	if (isAnalyzing) {
		console.log('‚è≥ An√°lise j√° em andamento...');
		return;
	}

	if (capturedScreenshots.length === 0) {
		console.warn('‚ö†Ô∏è Nenhum screenshot para analisar');
		updateStatusMessage('‚ö†Ô∏è Nenhum screenshot para analisar (capture com Ctrl+Shift+F)');
		return;
	}

	isAnalyzing = true;
	updateStatusMessage(`üîç Analisando ${capturedScreenshots.length} screenshot(s)...`);

	try {
		// Extrai caminhos dos arquivos
		const filepaths = capturedScreenshots.map(s => s.filepath);

		console.log('üöÄ Enviando para an√°lise:', filepaths);

		// Envia para main.js
		const result = await ipcRenderer.invoke('ANALYZE_SCREENSHOTS', filepaths);

		if (!result.success) {
			console.error('‚ùå Falha na an√°lise:', result.error);
			updateStatusMessage(`‚ùå ${result.error}`);
			return;
		}

		// ‚úÖ Renderiza resposta do GPT
		const questionText = `üì∏ An√°lise de ${capturedScreenshots.length} screenshot(s)`;
		// üî¢ USA ID SEQUENCIAL COMO AS PERGUNTAS NORMAIS (n√£o UUID)
		const questionId = String(questionsHistory.length + 1);

		// Adiciona "pergunta" ao hist√≥rico ANTES de renderizar respostas
		questionsHistory.push({
			id: questionId,
			text: questionText,
			createdAt: Date.now(),
			lastUpdateTime: Date.now(),
			answered: true,
		});

		// ‚úÖ MARCA COMO RESPONDIDA (importante para clique n√£o gerar duplicata)
		answeredQuestions.add(questionId);

		renderQuestionsHistory();

		// ‚úÖ RENDERIZA VIA STREAMING (fluxo real) - usa onAnswerStreamChunk como GPT normal
		// Divide an√°lise em tokens e emite como se fosse stream
		const analysisText = result.analysis;
		const tokens = analysisText.split(/(\s+|[.,!?;:\-\(\)\[\]{}\n])/g).filter(t => t.length > 0);

		console.log(`üì∏ [AN√ÅLISE] Simulando stream: ${tokens.length} tokens`);

		// Emite tokens assim como o GPT faz (permite UI renderizar em tempo real)
		let accumulated = '';
		for (const token of tokens) {
			accumulated += token;

			// ‚úÖ USA O MESMO EVENTO onAnswerStreamChunk (fluxo real)
			emitUIChange('onAnswerStreamChunk', {
				questionId: questionId,
				token: token,
				accum: accumulated,
			});

			// Pequeno delay entre tokens para simular streaming real
			await new Promise(resolve => setTimeout(resolve, 2));
		}

		console.log('‚úÖ An√°lise conclu√≠da e renderizada');
		updateStatusMessage('‚úÖ An√°lise conclu√≠da');

		// üóëÔ∏è Limpa screenshots ap√≥s an√°lise
		console.log(`üóëÔ∏è Limpando ${capturedScreenshots.length} screenshot(s) da mem√≥ria...`);
		capturedScreenshots = [];

		// Atualiza badge
		emitUIChange('onScreenshotBadgeUpdate', {
			count: 0,
			visible: false,
		});

		// For√ßa limpeza no sistema
		await ipcRenderer.invoke('CLEANUP_SCREENSHOTS');
	} catch (error) {
		console.error('‚ùå Erro ao analisar screenshots:', error);
		updateStatusMessage('‚ùå Erro na an√°lise');
	} finally {
		isAnalyzing = false;
	}
}

/**
 * Limpa todos os screenshots armazenados
 */
function clearScreenshots() {
	if (capturedScreenshots.length === 0) return;

	console.log(`üóëÔ∏è Limpando ${capturedScreenshots.length} screenshot(s)...`);
	capturedScreenshots = [];

	updateStatusMessage('‚úÖ Screenshots limpos');
	emitUIChange('onScreenshotBadgeUpdate', {
		count: 0,
		visible: false,
	});

	// For√ßa limpeza no sistema
	ipcRenderer.invoke('CLEANUP_SCREENSHOTS').catch(err => {
		console.warn('‚ö†Ô∏è Erro na limpeza:', err);
	});
}

/* ===============================
   BOOT
=============================== */

marked.setOptions({
	html: true, // üî• Permite renderiza√ß√£o de HTML (n√£o escapa entidades)
	breaks: true,
	gfm: true, // GitHub Flavored Markdown
	highlight: function (code, lang) {
		if (lang && hljs.getLanguage(lang)) {
			return hljs.highlight(code, { language: lang }).value;
		}
		return hljs.highlightAuto(code).value;
	},
});

// Exporta fun√ß√µes p√∫blicas que o controller pode chamar
const RendererAPI = {
	// √Åudio - Grava√ß√£o
	startInput,
	stopInput: stopInputMonitor,
	listenToggleBtn,
	askGpt,
	startOutput,
	stopOutput: stopOutputMonitor,
	restartAudioPipeline,

	// √Åudio - Monitoramento de volume
	startInputVolumeMonitoring,
	startOutputVolumeMonitoring,
	stopInputVolumeMonitoring,
	stopOutputVolumeMonitoring,
	// Entrevista - Reset (centralizado em resetAppState)
	resetAppState,

	// Modo
	changeMode: mode => {
		CURRENT_MODE = mode;
	},
	getMode: () => CURRENT_MODE,

	// Questions
	handleCurrentQuestion,
	handleQuestionClick,
	closeCurrentQuestion,

	// UI
	applyOpacity,
	updateMockBadge: show => {
		emitUIChange('onMockBadgeUpdate', { visible: show });
	},
	setMockToggle: checked => {
		if (UIElements.mockToggle) {
			UIElements.mockToggle.checked = checked;
		}
		APP_CONFIG.MODE_DEBUG = checked;
	},
	setModeSelect: mode => {
		emitUIChange('onModeSelectUpdate', { mode });
	},

	// Drag
	initDragHandle: (dragHandle, documentElement) => {
		if (!dragHandle) return;
		const doc = documentElement || document; // fallback para document global
		dragHandle.addEventListener('pointerdown', async event => {
			console.log('ü™ü Drag iniciado (pointerdown)');
			isDraggingWindow = true;
			dragHandle.classList.add('drag-active');

			const _pid = event.pointerId;
			try {
				dragHandle.setPointerCapture && dragHandle.setPointerCapture(_pid);
			} catch (err) {
				console.warn('setPointerCapture falhou:', err);
			}

			setTimeout(() => ipcRenderer.send('START_WINDOW_DRAG'), 40);

			const startBounds = (await ipcRenderer.invoke('GET_WINDOW_BOUNDS')) || {
				x: 0,
				y: 0,
			};
			const startCursor = { x: event.screenX, y: event.screenY };
			let lastAnimation = 0;

			function onPointerMove(ev) {
				const now = performance.now();
				if (now - lastAnimation < 16) return;
				lastAnimation = now;

				const dx = ev.screenX - startCursor.x;
				const dy = ev.screenY - startCursor.y;

				ipcRenderer.send('MOVE_WINDOW_TO', {
					x: startBounds.x + dx,
					y: startBounds.y + dy,
				});
			}

			function onPointerUp(ev) {
				try {
					dragHandle.removeEventListener('pointermove', onPointerMove);
					dragHandle.removeEventListener('pointerup', onPointerUp);
				} catch (err) {}

				if (dragHandle.classList.contains('drag-active')) {
					dragHandle.classList.remove('drag-active');
				}

				try {
					dragHandle.releasePointerCapture && dragHandle.releasePointerCapture(_pid);
				} catch (err) {}

				isDraggingWindow = false;
			}

			dragHandle.addEventListener('pointermove', onPointerMove);
			dragHandle.addEventListener('pointerup', onPointerUp, { once: true });
			event.stopPropagation();
		});

		doc.addEventListener('pointerup', () => {
			if (!dragHandle.classList.contains('drag-active')) return;
			console.log('ü™ü Drag finalizado (pointerup)');
			dragHandle.classList.remove('drag-active');
			isDraggingWindow = false;
		});

		dragHandle.addEventListener('pointercancel', () => {
			if (dragHandle.classList.contains('drag-active')) {
				dragHandle.classList.remove('drag-active');
				isDraggingWindow = false;
			}
		});
	},

	// Click-through
	setClickThrough: enabled => {
		ipcRenderer.send('SET_CLICK_THROUGH', enabled);
	},
	updateClickThroughButton: (enabled, btnToggle) => {
		if (!btnToggle) return;
		btnToggle.style.opacity = enabled ? '0.5' : '1';
		btnToggle.title = enabled
			? 'Click-through ATIVO (clique para desativar)'
			: 'Click-through INATIVO (clique para ativar)';
		console.log('üé® Bot√£o atualizado - opacity:', btnToggle.style.opacity);
	},

	// UI Registration
	registerUIElements: elements => {
		registerUIElements(elements);
	},
	onUIChange: (eventName, callback) => {
		onUIChange(eventName, callback);
	},

	// API Key
	setAppConfig: config => {
		APP_CONFIG = config;
	},
	getAppConfig: () => APP_CONFIG,

	// Navegacao de perguntas (Ctrl+Shift+ArrowUp/Down via globalShortcut IPC)
	navigateQuestions: direction => {
		const all = getNavigableQuestionIds();
		if (all.length === 0) return;

		let index = all.indexOf(selectedQuestionId);
		if (index === -1) {
			index = direction === 'up' ? all.length - 1 : 0;
		} else {
			index += direction === 'up' ? -1 : 1;
			index = Math.max(0, Math.min(index, all.length - 1));
		}

		selectedQuestionId = all[index];
		clearAllSelections();
		renderQuestionsHistory();
		renderCurrentQuestion();

		if (APP_CONFIG.MODE_DEBUG) {
			const msg = direction === 'up' ? 'üß™ Ctrl+ArrowUp detectado (teste)' : 'üß™ Ctrl+ArrowDown detectado (teste)';
			updateStatusMessage(msg);
			console.log('üìå Atalho Selecionou:', selectedQuestionId);
		}
	},

	// IPC Listeners
	onApiKeyUpdated: callback => {
		ipcRenderer.on('API_KEY_UPDATED', callback);
	},
	onToggleAudio: callback => {
		ipcRenderer.on('CMD_TOGGLE_AUDIO', callback);
	},
	onAskGpt: callback => {
		ipcRenderer.on('CMD_ASK_GPT', callback);
	},
	onGptStreamChunk: callback => {
		ipcRenderer.on('GPT_STREAM_CHUNK', callback);
	},
	onGptStreamEnd: callback => {
		ipcRenderer.on('GPT_STREAM_END', callback);
	},
	sendRendererError: error => {
		try {
			console.error('RENDERER ERROR', error.error || error.message || error);
			ipcRenderer.send('RENDERER_ERROR', {
				message: String(error.message || error),
				stack: error.error?.stack || null,
			});
		} catch (err) {
			console.error('Falha ao enviar RENDERER_ERROR', err);
		}
	},

	// üì∏ NOVO: Screenshot functions
	captureScreenshot,
	analyzeScreenshots,
	clearScreenshots,
	getScreenshotCount: () => capturedScreenshots.length,

	// üì∏ NOVO: Screenshot shortcuts
	onCaptureScreenshot: callback => {
		ipcRenderer.on('CMD_CAPTURE_SCREENSHOT', callback);
	},
	onAnalyzeScreenshots: callback => {
		ipcRenderer.on('CMD_ANALYZE_SCREENSHOTS', callback);
	},
	// Navegacao de perguntas (Ctrl+Shift+ArrowUp/Down via globalShortcut)
	onNavigateQuestions: callback => {
		ipcRenderer.on('CMD_NAVIGATE_QUESTIONS', (_, direction) => {
			callback(direction);
		});
	},

	// Emit UI changes (para config-manager enviar eventos para renderer)
	emitUIChange,
};

if (typeof module !== 'undefined' && module.exports) {
	module.exports = RendererAPI;
}

// üî• Expor globalmente para que config-manager possa acessar
if (typeof globalThis !== 'undefined') {
	globalThis.RendererAPI = RendererAPI;
	globalThis.runMockAutoPlay = runMockAutoPlay; // üé≠ Exportar Mock autoplay
	globalThis.mockScenarioIndex = 0; // üé≠ √çndice global para cen√°rios
	globalThis.mockAutoPlayActive = false; // üé≠ Flag global para evitar m√∫ltiplas execu√ß√µes
}
function debugLogRenderer(msg) {
	console.log('%cü™≤ ‚ùØ‚ùØ‚ùØ‚ùØ Debug: ' + msg + ' em renderer.js', 'color: brown; font-weight: bold;');
}

/* ===============================
   FUN√á√ÉO PARA LOGAR M√âTRICAS
=============================== */

function logTranscriptionMetrics() {
	if (!transcriptionMetrics.audioStartTime) return;

	const gptTime = transcriptionMetrics.gptEndTime - transcriptionMetrics.gptStartTime;
	const totalTime = transcriptionMetrics.totalTime;

	console.log(`üìä ================================`);
	console.log(`üìä M√âTRICAS DE TEMPO DETALHADAS:`);
	console.log(`üìä ================================`);
	console.log(`üìä TAMANHO √ÅUDIO: ${transcriptionMetrics.audioSize} bytes`);
	console.log(`üìä GPT: ${gptTime}ms`);
	console.log(`üìä TOTAL: ${totalTime}ms`);
	console.log(`üìä GPT % DO TOTAL: ${Math.round((gptTime / totalTime) * 100)}%`);
	console.log(`üìä ================================`);

	// Reset para pr√≥xima medi√ß√£o
	transcriptionMetrics = {
		audioStartTime: null,
		gptStartTime: null,
		gptEndTime: null,
		totalTime: null,
		audioSize: 0,
	};
}

/* ===============================
   RESET COMPLETO (TEMPOR√ÅRIO PARA TESTES)
=============================== */

/**
 * üîÑ Limpa tudo na se√ß√£o home como se o app tivesse aberto agora
 * Funcionalidade TEMPOR√ÅRIA para facilitar testes sem fechar a aplica√ß√£o
 */
function resetHomeSection() {
	console.log('\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê');
	console.log('üîÑ RESET COMPLETO ACIONADO PELO BOT√ÉO resetHomeBtn');
	console.log('‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê');

	// üî• Usar a fun√ß√£o centralizada de reset
	resetAppState().then(success => {
		if (success) {
			console.log('‚úÖ Reset via resetAppState() conclu√≠do com sucesso!');
		} else {
			console.error('‚ùå Erro ao executar resetAppState()');
		}
		console.log('‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n');
	});
}

// üî• LISTENER DO BOT√ÉO RESET
document.addEventListener('DOMContentLoaded', () => {
	// üî• Registrar listener para eventos de transcri√ß√£o completa (STT)
	onSTTEvent('transcriptionComplete', data => {
		if (!ModeController.isInterviewMode()) {
			console.log('‚è≠Ô∏è STT Event: modo normal (n√£o entrevista), ignorando auto-ask');
			return;
		}

		console.log('üîä STT Event: transcriptionComplete recebido');
		console.log('   ‚Üí Texto:', data.text?.substring(0, 50) + '...');
		console.log('   ‚Üí Speaker:', data.speaker);
		console.log('   ‚Üí Modelo:', data.model);

		// üî• Removido: AUTO_CLOSE_QUESTION_TIMEOUT ‚Äî agora usamos apenas o sil√™ncio para Deepgram
	});

	const resetBtn = document.getElementById('resetHomeBtn');
	if (resetBtn) {
		resetBtn.addEventListener('click', () => {
			const confirmed = confirm('‚ö†Ô∏è Isso vai limpar toda transcri√ß√£o, hist√≥rico e respostas.\n\nTem certeza?');
			if (confirmed) {
				resetHomeSection();
			}
		});
		console.log('‚úÖ Listener do bot√£o reset instalado');
	} else {
		console.warn('‚ö†Ô∏è Bot√£o reset n√£o encontrado no DOM');
	}
});

/* ===============================
   MOCK / DEBUG
=============================== */

/* ===============================
   üé≠ MOCK SYSTEM - Intercepta ipcRenderer
   Quando MODE_DEBUG=true, substitui respostas reais por mocks
=============================== */

// üîç Respostas mockadas por pergunta
const MOCK_RESPONSES = {
	'Mock - O que √© JVM e para que serve?':
		'Mock - A JVM (Java Virtual Machine) √© uma m√°quina virtual que executa bytecode Java. Ela permite que programas Java rodem em qualquer plataforma sem modifica√ß√£o. A JVM gerencia mem√≥ria, garbage collection e fornece um ambiente isolado e seguro para execu√ß√£o de c√≥digo.',
	'Mock - Qual a diferen√ßa entre JDK e JRE?':
		'Mock - JDK (Java Development Kit) √© o kit completo para desenvolvimento, incluindo compilador, ferramentas e bibliotecas. JRE (Java Runtime Environment) cont√©m apenas o necess√°rio para executar aplica√ß√µes Java compiladas. Todo desenvolvedor precisa do JDK, mas usu√°rios finais precisam apenas da JRE.',
	'Mock - O que √© uma classe em Java?':
		'Mock - Uma classe √© o molde ou blueprint para criar objetos. Define atributos (propriedades) e m√©todos (comportamentos). As classes s√£o fundamentais na programa√ß√£o orientada a objetos. Por exemplo, uma classe Carro pode ter atributos como cor e velocidade, e m√©todos como acelerar e frear.',
	'Mock - Explique sobre heran√ßa em Java':
		'Mock - Heran√ßa permite que uma classe herde propriedades e m√©todos de outra classe. A classe filha estende a classe pai usando a palavra-chave extends. Isso promove reutiliza√ß√£o de c√≥digo e cria uma hierarquia de classes. Por exemplo, a classe Bicicleta pode herdar de Veiculo.',
	'Mock - Como funciona polimorfismo?':
		'Mock - Polimorfismo significa muitas formas. Permite que objetos de diferentes tipos respondam a mesma chamada de m√©todo de forma diferente. Pode ser atrav√©s de sobrescrita de m√©todos (heran√ßa) ou interface. Exemplo: diferentes animais implementam o m√©todo fazer_som() diferentemente.',
	'Mock - O que √© encapsulamento?':
		'Mock - Encapsulamento √© o princ√≠pio de ocultar detalhes internos da implementa√ß√£o. Usa modificadores de acesso como private, protected e public. Protege dados e m√©todos cr√≠ticos, permitindo controle sobre como s√£o acessados. √â uma pilar da seguran√ßa e manuten√ß√£o do c√≥digo orientado a objetos.',
};

// üé¨ Cen√°rios autom√°ticos para teste
// screenshotsCount: 0 = sem screenshot, 1 = tira 1 foto, 2 = tira 2 fotos, etc
const MOCK_SCENARIOS = [
	{ question: 'Mock - O que √© JVM e para que serve?', screenshotsCount: 1 },
	{ question: 'Mock - Qual a diferen√ßa entre JDK e JRE?', screenshotsCount: 0 },
	{ question: 'Mock - O que √© uma classe em Java?', screenshotsCount: 0 },
	{ question: 'Mock - Explique sobre heran√ßa em Java', screenshotsCount: 2 },
	{ question: 'Mock - Como funciona polimorfismo?', screenshotsCount: 0 },
	{ question: 'Mock - O que √© encapsulamento?', screenshotsCount: 0 },
];

let mockScenarioIndex = 0;
let mockAutoPlayActive = false;

/**
 * üé≠ Retorna resposta mockada para pergunta (busca exata ou parcial)
 */
function getMockResponse(question) {
	// Match exato
	if (MOCK_RESPONSES[question]) {
		return MOCK_RESPONSES[question];
	}

	// Match parcial
	for (const [key, value] of Object.entries(MOCK_RESPONSES)) {
		if (question.toLowerCase().includes(key.toLowerCase())) {
			return value;
		}
	}

	// Fallback
	return `Resposta mockada para: "${question}"\n\nEste √© um teste do sistema em modo Mock.`;
}

/**
 * üé≠ Intercepta ipcRenderer.invoke para mockar 'ask-gpt-stream'
 * Emite eventos com pequenos delays para permitir processamento
 */
const originalInvoke = ipcRenderer.invoke;
ipcRenderer.invoke = function (channel, ...args) {
	// Intercepta an√°lise de screenshots quando MODE_DEBUG
	// IMPORTANTE: CAPTURE_SCREENSHOT √© REAL (tira foto mesmo), ANALYZE_SCREENSHOTS √© MOCK (simula resposta)
	if (channel === 'ANALYZE_SCREENSHOTS' && APP_CONFIG.MODE_DEBUG) {
		console.log('üì∏ [MOCK] Interceptando ANALYZE_SCREENSHOTS...');
		const filepaths = args[0] || [];
		const screenshotCount = filepaths.length;

		// Retorna an√°lise mockada
		const mockAnalysis = `
		## üì∏ An√°lise de ${screenshotCount} Screenshot(s) - MOCK

		### Esta √© uma resposta simulada para o teste do sistema.

		Para resolver o problema apresentado na captura de tela, que √© o "Remove Element" do LeetCode, vamos implementar uma fun√ß√£o em Java que remove todas as ocorr√™ncias de um valor espec√≠fico de um array. A fun√ß√£o deve modificar o array in-place e retornar o novo comprimento do array.

		Resumo do Problema
		Entrada: Um array de inteiros nums e um inteiro val que queremos remover.
		Sa√≠da: O novo comprimento do array ap√≥s remover todas as ocorr√™ncias de val.
		Passos para a Solu√ß√£o
		Iterar pelo array: Vamos percorrer o array e verificar cada elemento.
		Manter um √≠ndice: Usaremos um √≠ndice para rastrear a posi√ß√£o onde devemos colocar os elementos que n√£o s√£o iguais a val.
		Modificar o array in-place: Sempre que encontrarmos um elemento que n√£o √© igual a val, colocamos esse elemento na posi√ß√£o do √≠ndice e incrementamos o √≠ndice.
		Retornar o comprimento: No final, o √≠ndice representar√° o novo comprimento do array.
		Implementa√ß√£o do C√≥digo
		Aqui est√° a implementa√ß√£o em Java:

		class Solution {
			public int removeElement(int[] nums, int val) {
				// Inicializa um √≠ndice para rastrear a nova posi√ß√£o
				int index = 0;

				// Percorre todos os elementos do array
				for (int i = 0; i &lt; nums.length; i++) {
					// Se o elemento atual n√£o √© igual a val
					if (nums[i] != val) {
						// Coloca o elemento na posi√ß√£o do √≠ndice
						nums[index] = nums[i];
						// Incrementa o √≠ndice
						index++;
					}
				}

				// Retorna o novo comprimento do array
				return index;
			}
		}

		Explica√ß√£o do C√≥digo
		Classe e M√©todo: Criamos uma classe chamada Solution e um m√©todo removeElement que recebe um array de inteiros nums e um inteiro val.
		√çndice Inicial: Inicializamos uma vari√°vel index em 0.
		`;

		return Promise.resolve({
			success: true,
			analysis: mockAnalysis,
			filesAnalyzed: screenshotCount,
			timestamp: Date.now(),
		});
	}

	// Intercepta ask-gpt-stream quando MODE_DEBUG
	if (channel === 'ask-gpt-stream' && APP_CONFIG.MODE_DEBUG) {
		console.log('üé≠ [MOCK] Interceptando ask-gpt-stream...');

		// Obt√©m a pergunta do primeiro argumento (array de mensagens)
		const messages = args[0] || [];
		const userMessage = messages.find(m => m.role === 'user');
		const questionText = userMessage ? userMessage.content : 'Pergunta desconhecida';

		// Busca resposta mockada
		const mockResponse = getMockResponse(questionText);

		// Divide em tokens (remove vazios)
		const tokens = mockResponse.split(/(\s+|[.,!?;:\-\(\)\[\]{}\n])/g).filter(t => t.length > 0);

		console.log(`üé≠ [MOCK] Emitindo ${tokens.length} tokens para pergunta: "${questionText.substring(0, 50)}..."`);

		// Fun√ß√£o para emitir tokens com pequeno delay entre eles
		async function emitTokens() {
			let accumulated = '';
			for (let i = 0; i < tokens.length; i++) {
				const token = tokens[i];
				accumulated += token;

				// Emite o evento com delay m√≠nimo
				await new Promise(resolve => {
					setTimeout(() => {
						// ‚úÖ CORRETO: Emite apenas o token como 2¬∫ argumento
						ipcRenderer.emit('GPT_STREAM_CHUNK', null, token);
						resolve();
					}, 5); // 5ms entre tokens
				});
			}

			// Sinaliza fim do stream ap√≥s todos os tokens
			await new Promise(resolve => {
				setTimeout(() => {
					ipcRenderer.emit('GPT_STREAM_END');
					resolve();
				}, 10);
			});
		}

		// Inicia emiss√£o de tokens de forma ass√≠ncrona
		emitTokens().catch(err => {
			console.error('‚ùå Erro ao emitir tokens mock:', err);
		});

		// Retorna promise resolvida imediatamente (esperado pela API)
		return Promise.resolve({ success: true });
	}

	// Todas as outras chamadas passam para o invoke real
	return originalInvoke.call(this, channel, ...args);
};

/**
 * üé≠ Executa cen√°rios de entrevista mock automaticamente
 */
async function runMockAutoPlay() {
	if (mockAutoPlayActive) return;
	mockAutoPlayActive = true;

	while (mockScenarioIndex < MOCK_SCENARIOS.length && APP_CONFIG.MODE_DEBUG && mockAutoPlayActive) {
		const scenario = MOCK_SCENARIOS[mockScenarioIndex];
		console.log(
			`\nüé¨ ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nüé¨ MOCK CEN√ÅRIO ${mockScenarioIndex + 1}/${
				MOCK_SCENARIOS.length
			}\nüé¨ ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê`,
		);

		// FASE 1: Simula captura de √°udio (2-4s)
		console.log(`üé§ [FASE-1] Capturando √°udio da pergunta...`);
		const audioStartTime = Date.now();
		const placeholderId = `placeholder-${audioStartTime}-${Math.random()}`;

		// Emite placeholder
		emitUIChange('onTranscriptAdd', {
			author: 'Outros',
			text: '...',
			timeStr: new Date().toLocaleTimeString(),
			elementId: 'conversation',
			placeholderId: placeholderId,
		});

		// Aguarda captura
		await new Promise(resolve => setTimeout(resolve, 2000 + Math.random() * 2000));

		// üî• CHECK: Se modo debug foi desativado, para imediatamente
		if (!APP_CONFIG.MODE_DEBUG || !mockAutoPlayActive) {
			console.log('üõë [PARADA] Modo debug desativado - parando mock autoplay');
			break;
		}

		const audioEndTime = Date.now();
		console.log(`‚úÖ [FASE-1] √Åudio capturado`);

		// Calcula lat√™ncia (arredonda para inteiro - sem casas decimais)
		const latencyMs = Math.round(800 + Math.random() * 400);
		const totalMs = audioEndTime - audioStartTime + latencyMs;

		// Atualiza placeholder com texto real
		emitUIChange('onPlaceholderFulfill', {
			speaker: 'Outros',
			text: scenario.question,
			startStr: new Date(audioStartTime).toLocaleTimeString(),
			stopStr: new Date(audioEndTime).toLocaleTimeString(),
			recordingDuration: audioEndTime - audioStartTime,
			latency: latencyMs,
			total: totalMs,
			placeholderId: placeholderId,
		});

		// FASE 2: Processa pergunta (handleSpeech + closeCurrentQuestion)
		console.log(`üìù [FASE-2] Processando pergunta...`);
		handleSpeech(OTHER, scenario.question, { skipAddToUI: true });

		// Aguarda consolida√ß√£o (800ms para garantir que pergunta saia do CURRENT)
		await new Promise(resolve => setTimeout(resolve, 800));

		// üî• CHECK: Se modo debug foi desativado, para imediatamente
		if (!APP_CONFIG.MODE_DEBUG || !mockAutoPlayActive) {
			console.log('üõë [PARADA] Modo debug desativado - parando mock autoplay');
			break;
		}

		// Simula sil√™ncio e fecha pergunta
		console.log(`üîá [FASE-2] Sil√™ncio detectado, fechando pergunta...`);
		closeCurrentQuestion();

		// FASE 3: askGpt ser√° acionado automaticamente, o interceptor (ask-gpt-stream) que ir√° mockar
		console.log(`ü§ñ [FASE-3] askGpt acionado - mock stream ser√° emitido pelo interceptor`);

		// Aguarda stream terminar (~30ms por token)
		const mockResponse = getMockResponse(scenario.question);
		const estimatedTime = mockResponse.length * 30;
		await new Promise(resolve => setTimeout(resolve, estimatedTime + 1000));

		// üî• CHECK: Se modo debug foi desativado, para imediatamente SEM TIRAR SCREENSHOT
		if (!APP_CONFIG.MODE_DEBUG || !mockAutoPlayActive) {
			console.log('üõë [PARADA] Modo debug desativado - parando sem capturar screenshot');
			break;
		}

		// FASE 4 (Opcional): Captura N screenshots REAIS e depois aciona an√°lise
		if (scenario.screenshotsCount && scenario.screenshotsCount > 0) {
			// FASE 4A: Captura m√∫ltiplos screenshots
			for (let i = 1; i <= scenario.screenshotsCount; i++) {
				// üî• CHECK: Verifica antes de cada screenshot
				if (!APP_CONFIG.MODE_DEBUG || !mockAutoPlayActive) {
					console.log(
						`üõë [PARADA] Modo debug desativado - cancelando captura de screenshot ${i}/${scenario.screenshotsCount}`,
					);
					break;
				}

				console.log(`üì∏ [FASE-4A] Capturando screenshot ${i}/${scenario.screenshotsCount} REAL da resposta...`);
				await captureScreenshot();

				// Delay entre m√∫ltiplas capturas para respeitar cooldown de 2s do main.js
				if (i < scenario.screenshotsCount) {
					console.log(`   ‚è≥ Aguardando 2200ms antes da pr√≥xima captura (cooldown CAPTURE_COOLDOWN)...`);
					await new Promise(resolve => setTimeout(resolve, 2200));
				}
			}

			// üî• CHECK: Verifica antes de an√°lise
			if (!APP_CONFIG.MODE_DEBUG || !mockAutoPlayActive) {
				console.log('üõë [PARADA] Modo debug desativado - cancelando an√°lise de screenshots');
				break;
			}

			// Log de valida√ß√£o: quantas fotos tem antes de analisar
			console.log(
				`üì∏ [PR√â-AN√ÅLISE] Total de screenshots em mem√≥ria: ${capturedScreenshots.length}/${scenario.screenshotsCount}`,
			);

			// FASE 4B: An√°lise dos screenshots capturados
			console.log(`üì∏ [FASE-4B] Analisando ${scenario.screenshotsCount} screenshot(s)...`);
			await analyzeScreenshots();
		}

		mockScenarioIndex++;

		if (mockScenarioIndex < MOCK_SCENARIOS.length) {
			console.log(`\n‚è≥ Aguardando 1s antes do pr√≥ximo cen√°rio...\n`);
			await new Promise(resolve => setTimeout(resolve, 1000));
		}
	}

	console.log('‚úÖ Mock autoplay finalizado');
	mockAutoPlayActive = false;
}

//console.log('üöÄ Entrou no renderer.js');
