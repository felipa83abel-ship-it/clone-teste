/* ===============================
   IMPORTS
=============================== */
const { ipcRenderer } = require('electron');
const { marked } = require('marked');
const hljs = require('highlight.js');

// ğŸ”’ DESABILITADO TEMPORARIAMENTE
const DESABILITADO_TEMPORARIAMENTE = false;

/* ===============================
   ğŸ” PROTEÃ‡ÃƒO CONTRA CAPTURA DE TELA EXTERNA
   Desabilita/limita APIs usadas por Zoom, Teams, Meet, OBS, Discord, Snipping Tool, etc.
=============================== */
(function protectAgainstScreenCapture() {
	// âœ… Desabilita getDisplayMedia (usado por Zoom, Meet, Teams para capturar)
	if (navigator && navigator.mediaDevices && navigator.mediaDevices.getDisplayMedia) {
		const originalGetDisplayMedia = navigator.mediaDevices.getDisplayMedia.bind(navigator.mediaDevices);
		navigator.mediaDevices.getDisplayMedia = async function (...args) {
			console.warn('ğŸ” BLOQUEADO: Tentativa de usar getDisplayMedia (captura de tela externa)');
			throw new Error('Screen capture not available in this window');
		};
	}

	// âœ… Desabilita captureStream (usado para captura de janela)
	if (window.HTMLCanvasElement && window.HTMLCanvasElement.prototype.captureStream) {
		Object.defineProperty(window.HTMLCanvasElement.prototype, 'captureStream', {
			value: function () {
				console.warn('ğŸ” BLOQUEADO: Tentativa de usar Canvas.captureStream()');
				throw new Error('Capture stream not available');
			},
			writable: false,
			configurable: false,
		});
	}

	// âœ… Intercepta getUserMedia para avisar sobre tentativas de captura de Ã¡udio (pode ser usado em combo com vÃ­deo)
	if (navigator && navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
		const originalGetUserMedia = navigator.mediaDevices.getUserMedia.bind(navigator.mediaDevices);
		navigator.mediaDevices.getUserMedia = async function (constraints) {
			if (constraints && constraints.video) {
				console.warn('ğŸ” AVISO: Tentativa de usar getUserMedia com vÃ­deo detectada');
				// Ainda permite Ã¡udio, mas bloqueia vÃ­deo para captura
				if (constraints.video) {
					delete constraints.video;
				}
			}
			return originalGetUserMedia(constraints);
		};
	}

	console.log('âœ… ProteÃ§Ã£o contra captura externa ativada');
})();

/* ===============================
   CONSTANTES
=============================== */

const YOU = 'VocÃª';
const OTHER = 'Outros';

const ENABLE_INTERVIEW_TIMING_DEBUG = true; // â† desligar depois = false
const QUESTION_IDLE_TIMEOUT = 300; // Tempo de espera para a pergunta ser considerada inativa = 300
const CURRENT_QUESTION_ID = 'CURRENT'; // ID da pergunta atual

const INPUT_SPEECH_THRESHOLD = 20; // Valor limite (threshold) para detectar fala mais cedo = 20
const INPUT_SILENCE_TIMEOUT = 100; // Tempo de espera para silÃªncio = 100
const MIN_INPUT_AUDIO_SIZE = 1000; // Valor mÃ­nimo de tamanho de Ã¡udio para a normal = 1000
const MIN_INPUT_AUDIO_SIZE_INTERVIEW = 350; // Valor mÃ­nimo de tamanho de Ã¡udio para a entrevista = 350

const OUTPUT_SPEECH_THRESHOLD = 20; // Valor limite (threshold) para detectar fala mais cedo = 8
const OUTPUT_SILENCE_TIMEOUT = 100; // ğŸ”¥ Aumentado de 100ms para 500ms para evitar cortar palavras no fim (pausas naturais)
const MIN_OUTPUT_AUDIO_SIZE = 1000; // Valor mÃ­nimo de tamanho de Ã¡udio para a normal = 2500
const MIN_OUTPUT_AUDIO_SIZE_INTERVIEW = 350; // Valor mÃ­nimo para enviar parcial (~3-4 chunks, ~3KB)
// controla intervalo mÃ­nimo entre requisiÃ§Ãµes STT parciais (ms) - mantÃ©m rate-limit para nÃ£o sobrecarregar API
const PARTIAL_MIN_INTERVAL_MS = 3000;

const OUTPUT_ENDING_PHRASES = ['tchau', 'tchau tchau', 'obrigado', 'valeu', 'falou', 'beleza', 'ok']; // Palavras finais para detectar o fim da fala

const SYSTEM_PROMPT = `
VocÃª Ã© um assistente para entrevistas tÃ©cnicas de Java. Responda como candidato.
Regras de resposta (priorize sempre estas):
- Seja natural e conciso: responda em no mÃ¡ximo 1â€“2 frases curtas.
- Use linguagem coloquial e direta, como alguÃ©m explicando rapidamente verbalmente.
- Evite listas longas, exemplos extensos ou parÃ¡grafos detalhados.
- NÃ£o comece com cumprimentos ou palavras de preenchimento (ex.: "Claro", "Ok").
- Quando necessÃ¡rio, entregue um exemplo mÃ­nimo de 1 linha apenas.
`;

/* ===============================
   SCREENSHOT CAPTURE - ESTADO E CONTROLE
=============================== */

let capturedScreenshots = []; // Array de { filepath, filename, timestamp }
let isCapturing = false;
let isAnalyzing = false;

/* ===============================
   ESTADO GLOBAL
=============================== */

let APP_CONFIG = {
	MODE_DEBUG: false, // â† alterado via config-manager.js (true = modo mock)
};

// ğŸªŸ Estado do Drag and Drop da janela
let isDraggingWindow = false;

let isRunning = false;
let audioContext;
// let mockInterviewRunning = false;

// ğŸ”¥ MODIFICADO: STT model vem da config agora (removido USE_LOCAL_WHISPER)
let transcriptionMetrics = {
	audioStartTime: null,
	whisperStartTime: null,
	whisperEndTime: null,
	gptStartTime: null,
	gptEndTime: null,
	totalTime: null,
	audioSize: 0,
};

/* ğŸ¤ INPUT (VOCÃŠ) */
let inputStream;
let inputAnalyser;
let inputData;
let inputRecorder;
let inputChunks = [];
let inputSpeaking = false;
let inputSilenceTimer = null;
let inputPartialChunks = [];
let inputPartialTimer = null;

/* ğŸ”Š OUTPUT (OUTROS) */
let outputStream;
let outputAnalyser;
let outputData;
let outputRecorder;
let outputChunks = [];
let outputSpeaking = false;
let outputSilenceTimer = null;
let outputPartialChunks = [];
let outputPartialTimer = null;
let outputPartialText = '';

// ğŸ”¥ NOVO: IDs para rastrear e parar os loops de animation
let inputVolumeAnimationId = null;
let outputVolumeAnimationId = null;

/* ğŸ§  PERGUNTAS */
let currentQuestion = { text: '', lastUpdate: 0, finalized: false, lastUpdateTime: null, createdAt: null };
let questionsHistory = [];
const answeredQuestions = new Set(); // ğŸ”’ Armazena respostas jÃ¡ geradas (questionId -> true)
let selectedQuestionId = null;
let interviewTurnId = 0;
let gptAnsweredTurnId = null;
let gptRequestedTurnId = null;
let gptRequestedQuestionId = null; // ğŸ”¥ [IMPORTANTE] Rastreia QUAL pergunta foi realmente solicitada ao GPT
let lastSentQuestionText = '';
let autoCloseQuestionTimer = null;
let lastInputStartAt = null;
let lastInputStopAt = null;
let lastOutputStartAt = null;
let lastOutputStopAt = null;
let lastInputPlaceholderEl = null;
let lastOutputPlaceholderEl = null;
let lastAskedQuestionNormalized = null;
let lastPartialSttAt = null;
let lastOutputPlaceholderId = null; // ğŸ”¥ ID Ãºnico para rastrear qual placeholder atualizar

// ğŸ”¥ VariÃ¡veis temporÃ¡rias para transcriÃ§Ã£o atual (imunes a race conditions)
// Armazenam os timestamps capturados NO MOMENTO de onstop() para uso exclusivo por transcribeOutput()
let pendingOutputStartAt = null;
let pendingOutputStopAt = null;

/* ===============================
   CALLBACKS / OBSERVERS SYSTEM
   renderer.js Ã© "cego" para DOM
   config-manager.js se inscreve em mudanÃ§as
=============================== */

const UICallbacks = {
	onError: null, // ğŸ”¥ NOVO: Para mostrar erros de validaÃ§Ã£o
	onTranscriptAdd: null,
	onCurrentQuestionUpdate: null,
	onQuestionsHistoryUpdate: null,
	onAnswerAdd: null,
	onStatusUpdate: null, // â† Adicionado: Para atualizar status na UI
	onInputVolumeUpdate: null,
	onOutputVolumeUpdate: null,
	onMockBadgeUpdate: null,
	onDOMElementsReady: null, // callback para pedir elementos ao config-manager
	onListenButtonToggle: null,
	onAnswerSelected: null,
	onClearAllSelections: null,
	onScrollToQuestion: null,
	onTranscriptionCleared: null,
	onAnswersCleared: null,
	onAnswerStreamChunk: null,
	onAnswerIdUpdate: null,
	onModeSelectUpdate: null,
	onPlaceholderFulfill: null,
	onPlaceholderUpdate: null,
	onScreenshotBadgeUpdate: null,
};

// FunÃ§Ã£o para config-manager se inscrever em eventos
function onUIChange(eventName, callback) {
	if (UICallbacks.hasOwnProperty(eventName)) {
		UICallbacks[eventName] = callback;
		console.log(`ğŸ“¡ UI callback registrado em renderer.js: ${eventName}`);
	}
}

// FunÃ§Ã£o para emitir/enviar eventos para config-manager
function emitUIChange(eventName, data) {
	if (UICallbacks[eventName] && typeof UICallbacks[eventName] === 'function') {
		UICallbacks[eventName](data);
	} else {
		console.warn(`âš ï¸ DEBUG: Nenhum callback registrado para '${eventName}'`);
	}
}

/* ===============================
   ELEMENTOS UI - Solicitado por callback
   (config-manager.js fornece esses elementos)
=============================== */

let UIElements = {
	inputSelect: null,
	outputSelect: null,
	listenBtn: null,
	statusText: null,
	transcriptionBox: null, // Mantido para compatibilidade, mas pode receber 'conversation'
	currentQuestionBox: null,
	currentQuestionTextBox: null,
	questionsHistoryBox: null,
	answersHistoryBox: null,
	askBtn: null,
	inputVu: null,
	outputVu: null,
	inputVuHome: null,
	outputVuHome: null,
	mockToggle: null,
	mockBadge: null,
	interviewModeSelect: null,
	btnClose: null,
	btnToggleClick: null,
	dragHandle: null,
	darkToggle: null,
	opacitySlider: null,
};

// config-manager.js chama isso para registrar elementos
function registerUIElements(elements) {
	UIElements = { ...UIElements, ...elements };
	console.log('âœ… UI Elements registrados no renderer.js');
}

/* ===============================
   MODO / ORQUESTRADOR
=============================== */

const MODES = {
	NORMAL: 'NORMAL',
	INTERVIEW: 'INTERVIEW',
};

// ğŸ”„ modo atual (default = comportamento atual)
let CURRENT_MODE = MODES.NORMAL;

// ğŸ¼ controlador central de estratÃ©gia
const ModeController = {
	isInterviewMode() {
		return CURRENT_MODE === MODES.INTERVIEW;
	},

	// â±ï¸ MediaRecorder.start(timeslice)
	mediaRecorderTimeslice() {
		if (!this.isInterviewMode()) return null;

		// OUTPUT pode ser mais agressivo que INPUT
		return 60; // reduzido para janelas parciais mais responsivas
	},

	// ğŸ¤– GPT streaming
	allowGptStreaming() {
		return this.isInterviewMode();
	},

	// ğŸ“¦ tamanho mÃ­nimo de Ã¡udio aceito
	minInputAudioSize(defaultSize) {
		return this.isInterviewMode() ? Math.min(400, defaultSize) : defaultSize;
	},
};

/* ===============================
   HELPERS PUROS
=============================== */

function finalizeQuestion(t) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "finalizeQuestion"');
	debugLogRenderer('Fim da funÃ§Ã£o: "finalizeQuestion"');
	return t.trim().endsWith('?') ? t.trim() : t.trim() + '?';
}

function normalizeForCompare(t) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "normalizeForCompare"');
	debugLogRenderer('Fim da funÃ§Ã£o: "normalizeForCompare"');
	return (t || '')
		.toLowerCase()
		.replace(/[?!.\n\r]/g, '')
		.replace(/\s+/g, ' ')
		.trim();
}

function looksLikeQuestion(t) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "looksLikeQuestion"');
	const s = t.toLowerCase().trim();

	// precisa ter ? OU comeÃ§ar com palavra tÃ­pica de pergunta
	const questionStarters = [
		'o que',
		'por que',
		'porque',
		'como',
		'qual',
		'quais',
		'quando',
		'onde',
		'fale',
		'me fale',
		'me explica',
		'me explique',
		'me diga',
		'diga',
		'vocÃª',
		'explique',
		'descreva',
		'jÃ¡',
		'tu jÃ¡',
	];

	debugLogRenderer('Fim da funÃ§Ã£o: "looksLikeQuestion"');
	return s.includes('?') || questionStarters.some(q => s.startsWith(q));
}

function isGarbageSentence(t) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "isGarbageSentence"');
	const s = t.toLowerCase();
	debugLogRenderer('Fim da funÃ§Ã£o: "isGarbageSentence"');
	return ['obrigado', 'atÃ© a prÃ³xima', 'finalizando'].some(w => s.includes(w));
}

// Encurta uma resposta em markdown para atÃ© `maxSentences` sentenÃ§as.
function shortenAnswer(markdownText, maxSentences = 2) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "shortenAnswer"');
	if (!markdownText) return markdownText;

	// remove blocos de cÃ³digo temporariamente para evitar cortes ruins
	const codeBlocks = [];
	const withoutCode = markdownText.replace(/```[\s\S]*?```/g, match => {
		codeBlocks.push(match);
		return `__CODEBLOCK_${codeBlocks.length - 1}__`;
	});

	// remove inline code
	const tmp = withoutCode.replace(/`([^`]*)`/g, '$1');

	// extrai sentenÃ§as por pontuaÃ§Ã£o final
	const parts = tmp.split(/(?<=[\.\?!])\s+/);

	const take = parts.slice(0, maxSentences).join(' ').trim();

	// restaura blocos de cÃ³digo, caso existam (apendados ao final)
	let result = take;
	if (codeBlocks.length) {
		result += '\n\n' + codeBlocks.join('\n\n');
	}

	// garante pontuaÃ§Ã£o final
	if (!/[\.\?!]$/.test(result)) result = result + '.';

	debugLogRenderer('Fim da funÃ§Ã£o: "shortenAnswer"');
	return result;
}

function isIncompleteQuestion(t) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "isIncompleteQuestion"');
	if (!t) return false;
	const s = t.trim();
	// casos Ã³bvios: contÃ©m reticÃªncias (..., â€¦) â€” normalmente placeholders ou cortes
	if (s.includes('...') || s.includes('â€¦')) return true;

	// termina com fragmento muito curto seguido de pontuaÃ§Ã£o (ex: "O que Ã© a...")
	// ou termina com apenas 1-3 letras antes do fim (sinal de corte)
	if (/\b\w{1,3}[\.]{0,3}$/.test(s) && /\.\.{1,3}$/.test(s)) return true;

	// termina com palavra muito curta e sem contexto (ex: endsWith ' a' )
	if (/\b[a-z]{1,2}$/.test(s.toLowerCase())) return true;

	debugLogRenderer('Fim da funÃ§Ã£o: "isIncompleteQuestion"');
	return false;
}

function getNavigableQuestionIds() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "getNavigableQuestionIds"');
	const ids = [];

	// CURRENT sÃ³ entra se tiver texto
	if (currentQuestion.text && currentQuestion.text.trim().length > 0) {
		ids.push(CURRENT_QUESTION_ID);
	}

	// HistÃ³rico (mais recente primeiro)
	ids.push(
		...questionsHistory
			.slice()
			.reverse()
			.map(q => q.id),
	);

	debugLogRenderer('Fim da funÃ§Ã£o: "getNavigableQuestionIds"');
	return ids;
}

function findAnswerByQuestionId(questionId) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "findAnswerByQuestionId"');

	if (!questionId) {
		// ID invÃ¡lido
		debugLogRenderer('Fim da funÃ§Ã£o: "findAnswerByQuestionId"');
		return false;
	}

	debugLogRenderer('Fim da funÃ§Ã£o: "findAnswerByQuestionId"');
	return answeredQuestions.has(questionId);
}

function promoteCurrentToHistory(text) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "promoteCurrentToHistory"');
	console.log('ğŸ“š promovendo pergunta para histÃ³rico:', text);

	// evita duplicaÃ§Ã£o no histÃ³rico: se a Ãºltima entrada Ã© igual (normalizada), nÃ£o adiciona
	const last = questionsHistory.length ? questionsHistory[questionsHistory.length - 1] : null;
	if (last && normalizeForCompare(last.text) === normalizeForCompare(text)) {
		console.log('ğŸ”• pergunta igual jÃ¡ presente no histÃ³rico â€” pulando promoÃ§Ã£o');

		// limpa CURRENT mas preserva seleÃ§Ã£o conforme antes
		const prevSelected = selectedQuestionId;
		currentQuestion = { text: '', lastUpdate: 0, finalized: false, lastUpdateTime: null, createdAt: null };
		if (prevSelected === null || prevSelected === CURRENT_QUESTION_ID) {
			selectedQuestionId = CURRENT_QUESTION_ID;
		} else {
			selectedQuestionId = prevSelected;
		}

		renderQuestionsHistory();
		renderCurrentQuestion();
		return;
	}

	const newId = String(questionsHistory.length + 1);

	questionsHistory.push({
		id: newId,
		text,
		createdAt: currentQuestion.createdAt || Date.now(),
		lastUpdateTime: currentQuestion.lastUpdateTime || currentQuestion.createdAt || Date.now(),
	});

	// ğŸ”¥ [IMPORTANTE] Migrar resposta de CURRENT para o novo ID no history
	if (answeredQuestions.has(CURRENT_QUESTION_ID)) {
		answeredQuestions.delete(CURRENT_QUESTION_ID);
		answeredQuestions.add(newId);
		console.log('ğŸ”„ [IMPORTANTE] Migrada resposta de CURRENT para newId:', newId);
	}

	// ğŸ”¥ [CRÃTICO] Atualizar o ID do bloco de resposta no DOM se ele foi criado com CURRENT
	console.log('ğŸ”„ [IMPORTANTE] Emitindo onAnswerIdUpdate para atualizar bloco de resposta: CURRENT â†’ ', newId);
	emitUIChange('onAnswerIdUpdate', {
		oldId: CURRENT_QUESTION_ID,
		newId: newId,
	});

	// ğŸ”¥ [IMPORTANTE] Se uma pergunta CURRENT foi solicitada ao GPT,
	// atualizar o rastreamento para apontar para o novo ID promovido
	if (gptRequestedQuestionId === CURRENT_QUESTION_ID) {
		gptRequestedQuestionId = newId;
		console.log('ğŸ”„ [IMPORTANTE] gptRequestedQuestionId atualizado de CURRENT para newId:', newId);
	}

	// preserva seleÃ§Ã£o do usuÃ¡rio: se nÃ£o havia seleÃ§Ã£o explÃ­cita ou estava no CURRENT,
	// mantÃ©m a seleÃ§Ã£o no CURRENT para que o novo CURRENT seja principal.
	const prevSelected = selectedQuestionId;

	currentQuestion = { text: '', lastUpdate: 0, finalized: false, lastUpdateTime: null, createdAt: null };

	if (prevSelected === null || prevSelected === CURRENT_QUESTION_ID) {
		selectedQuestionId = CURRENT_QUESTION_ID;
	} else {
		// usuÃ¡rio tinha selecionado algo no histÃ³rico â€” preserva essa seleÃ§Ã£o
		selectedQuestionId = prevSelected;
	}

	renderQuestionsHistory();
	renderCurrentQuestion();

	debugLogRenderer('Fim da funÃ§Ã£o: "promoteCurrentToHistory"');
}

function isQuestionReady(text) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "isQuestionReady"');
	if (!ModeController.isInterviewMode()) return true;

	const trimmed = text.trim();

	// ğŸ”¥ entrevistas podem ter perguntas curtas ("O que Ã© POO")
	if (trimmed.length < 10) return false;

	// ignora despedidas
	if (isEndingPhrase(trimmed)) return false;

	// heurÃ­stica simples de pergunta
	const questionIndicators = [
		'o que',
		'por que',
		'porque',
		'como',
		'qual',
		'quais',
		'quando',
		'onde',
		'fale',
		'me fale',
		'me explica',
		'me explique',
		'me diga',
		'diga',
		'vocÃª',
		'explique',
		'descreva',
		'jÃ¡',
		'tu jÃ¡',
	];

	const lower = trimmed.toLowerCase();

	const hasIndicator = questionIndicators.some(q => lower.includes(q));

	const hasQuestionMark = trimmed.includes('?');

	debugLogRenderer('Fim da funÃ§Ã£o: "isQuestionReady"'); // sÃ³ dispara se houver indÃ­cio real
	return hasIndicator || hasQuestionMark;
}

function isEndingPhrase(text) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "isEndingPhrase"');
	const normalized = text.toLowerCase().trim();

	debugLogRenderer('Fim da funÃ§Ã£o: "isEndingPhrase"');
	return OUTPUT_ENDING_PHRASES.some(p => normalized === p);
}

/* ===============================
   ğŸ”¥ RESET COMPLETO DO APP
   FunÃ§Ã£o centralizada e reutilizÃ¡vel para limpar tudo
   Pode ser chamada por: mock toggle, resetHomeBtn, ou qualquer outro
=============================== */

/**
 * ğŸ§¹ Reseta o aplicativo completamente para estado inicial
 * - Substitui resetInterviewState() e resetHomeSection()
 * - Centraliza TODA lÃ³gica de limpeza em um Ãºnico lugar
 * - Pode ser reutilizada por qualquer botÃ£o/controle
 *
 * Uso:
 *   await resetAppState(); // Completo e seguro
 */
async function resetAppState() {
	console.log('ğŸ§¹ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
	console.log('ğŸ§¹ INICIANDO RESET COMPLETO DO APP');
	console.log('ğŸ§¹ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');

	try {
		// 1ï¸âƒ£ PARAR AUTOPLAY DO MOCK (prevent async operations)
		mockAutoPlayActive = false;
		mockScenarioIndex = 0;
		console.log('âœ… Autoplay do mock parado');

		// 2ï¸âƒ£ PARAR ÃUDIO IMEDIATAMENTE (input/output)
		if (isRunning) {
			console.log('ğŸ¤ Parando captura de Ã¡udio...');
			await stopInput();
			await stopOutput();
			isRunning = false;
		}

		// 3ï¸âƒ£ RESET DE ESTADO DE ÃUDIO
		inputSpeaking = false;
		outputSpeaking = false;
		console.log('âœ… Estado de Ã¡udio resetado');

		// 4ï¸âƒ£ LIMPAR CHUNKS DE ÃUDIO
		inputChunks = [];
		outputChunks = [];
		inputPartialChunks = [];
		outputPartialChunks = [];
		outputPartialText = '';
		voskAccumulatedText = '';
		console.log('âœ… Chunks de Ã¡udio limpos');

		// 5ï¸âƒ£ LIMPAR TIMERS DE ÃUDIO
		if (inputSilenceTimer) {
			clearTimeout(inputSilenceTimer);
			inputSilenceTimer = null;
		}
		if (outputSilenceTimer) {
			clearTimeout(outputSilenceTimer);
			outputSilenceTimer = null;
		}
		if (inputPartialTimer) {
			clearTimeout(inputPartialTimer);
			inputPartialTimer = null;
		}
		if (outputPartialTimer) {
			clearTimeout(outputPartialTimer);
			outputPartialTimer = null;
		}
		if (voskPartialTimer) {
			clearTimeout(voskPartialTimer);
			voskPartialTimer = null;
		}
		if (autoCloseQuestionTimer) {
			clearTimeout(autoCloseQuestionTimer);
			autoCloseQuestionTimer = null;
		}
		console.log('âœ… Timers limpos');

		// 6ï¸âƒ£ LIMPAR PERGUNTAS E RESPOSTAS
		currentQuestion = { text: '', lastUpdate: 0, finalized: false, lastUpdateTime: null, createdAt: null };
		questionsHistory = [];
		answeredQuestions.clear();
		selectedQuestionId = null;
		lastSentQuestionText = '';
		lastAskedQuestionNormalized = null;
		console.log('âœ… Perguntas e respostas limpas');

		// 7ï¸âƒ£ LIMPAR ESTADO GPT/ENTREVISTA
		interviewTurnId = 0;
		gptAnsweredTurnId = null;
		gptRequestedTurnId = null;
		gptRequestedQuestionId = null;
		console.log('âœ… Estado de entrevista resetado');

		// 8ï¸âƒ£ LIMPAR PLACEHOLDERS
		lastInputStartAt = null;
		lastInputStopAt = null;
		lastOutputStartAt = null;
		lastOutputStopAt = null;
		pendingOutputStartAt = null;
		pendingOutputStopAt = null;
		lastPartialSttAt = null;
		lastOutputPlaceholderId = null;
		lastInputPlaceholderEl = null;
		lastOutputPlaceholderEl = null;
		console.log('âœ… Placeholders limpos');

		// 9ï¸âƒ£ RESETAR MÃ‰TRICAS
		transcriptionMetrics = {
			audioStartTime: null,
			whisperStartTime: null,
			whisperEndTime: null,
			gptStartTime: null,
			gptEndTime: null,
			totalTime: null,
			audioSize: 0,
		};
		console.log('âœ… MÃ©tricas resetadas');

		// ğŸ”Ÿ LIMPAR SCREENSHOTS (sem chamar API!)
		if (capturedScreenshots.length > 0) {
			console.log(`ğŸ—‘ï¸ Limpando ${capturedScreenshots.length} screenshot(s)...`);
			capturedScreenshots = [];
			emitUIChange('onScreenshotBadgeUpdate', {
				count: 0,
				visible: false,
			});
			// ForÃ§a limpeza no sistema
			try {
				await ipcRenderer.invoke('CLEANUP_SCREENSHOTS');
			} catch (err) {
				console.warn('âš ï¸ Erro ao limpar screenshots no sistema:', err);
			}
		}
		console.log('âœ… Screenshots limpos');

		// 1ï¸âƒ£1ï¸âƒ£ LIMPAR FLAGS
		isCapturing = false;
		isAnalyzing = false;
		console.log('âœ… Flags resetadas');

		// 1ï¸âƒ£2ï¸âƒ£ ATUALIZAR UI - PERGUNTAS
		emitUIChange('onCurrentQuestionUpdate', {
			text: '',
			isSelected: false,
		});
		emitUIChange('onQuestionsHistoryUpdate', []);
		console.log('âœ… Perguntas UI limpa');

		// 1ï¸âƒ£3ï¸âƒ£ ATUALIZAR UI - TRANSCRIÃ‡Ã•ES E RESPOSTAS
		emitUIChange('onTranscriptionCleared');
		emitUIChange('onAnswersCleared');
		console.log('âœ… TranscriÃ§Ãµes e respostas UI limpas');

		// 1ï¸âƒ£4ï¸âƒ£ ATUALIZAR UI - BOTÃƒO LISTEN
		emitUIChange('onListenButtonToggle', {
			isRunning: false,
			buttonText: 'ğŸ¤ ComeÃ§ar a Ouvir... (Ctrl+D)',
		});
		console.log('âœ… BotÃ£o listen resetado');

		// 1ï¸âƒ£5ï¸âƒ£ ATUALIZAR UI - STATUS
		emitUIChange('onStatusUpdate', {
			status: 'ready',
			message: 'âœ… Pronto',
		});
		console.log('âœ… Status atualizado');

		// 1ï¸âƒ£6ï¸âƒ£ LIMPAR SELEÃ‡Ã•ES
		clearAllSelections();
		console.log('âœ… SeleÃ§Ãµes limpas');

		// 1ï¸âƒ£7ï¸âƒ£ LOG FINAL
		console.log('âœ… â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
		console.log('âœ… RESET COMPLETO CONCLUÃDO COM SUCESSO');
		console.log('âœ… â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');

		return true;
	} catch (error) {
		console.error('âŒ Erro ao resetar app:', error);
		return false;
	}
}

/**
 * ğŸ”§ Limpa parcialmente o estado de uma volta de entrevista (turn)
 * Usado internamente durante streaming para nÃ£o perder contexto
 * NÃƒO substitui resetAppState() - Ã© um helper minor
 */
function resetInterviewTurnState() {
	// Limpa apenas o output parcial desta volta especÃ­fica
	outputPartialText = '';
	outputPartialChunks = [];
	// NÃ£o limpa lastAskedQuestionNormalized aqui - mantÃ©m para evitar duplicatas
}

/* ===============================
   TRANSCRIÃ‡ÃƒO (STT) - MODELO DINÃ‚MICO
=============================== */

/**
 * ObtÃ©m o modelo STT configurado para o provider ativo
 * @returns {string} 'vosk-local' | 'whisper-1' | 'google-stt' etc
 */
function getConfiguredSTTModel() {
	try {
		if (!window.configManager || !window.configManager.config) {
			console.warn('âš ï¸ configManager nÃ£o disponÃ­vel, usando padrÃ£o: whisper-1');
			return 'whisper-1';
		}

		const config = window.configManager.config;
		const activeProvider = config.api?.activeProvider || 'openai';
		const sttModel = config.api?.[activeProvider]?.selectedSTTModel;

		if (!sttModel) {
			console.warn(`âš ï¸ Modelo STT nÃ£o configurado para ${activeProvider}, usando padrÃ£o: whisper-1`);
			return 'whisper-1';
		}

		console.log(`ğŸ¤ STT Model selecionado: ${sttModel} (provider: ${activeProvider})`);
		console.log(`   [DEBUG] config.api.${activeProvider}.selectedSTTModel = "${sttModel}"`);
		console.log(
			`   [DEBUG] select#${activeProvider}-stt-model.value = "${
				document.getElementById(activeProvider + '-stt-model')?.value
			}"`,
		);
		return sttModel;
	} catch (err) {
		console.error('âŒ Erro ao obter modelo STT da config:', err);
		return 'whisper-1'; // fallback
	}
}

/**
 * Roteia transcriÃ§Ã£o de Ã¡udio para o modelo STT configurado
 *
 * Modelos suportados (via config-manager):
 * 1. vosk-local          â†’ main.js handlers: vosk-transcribe + vosk-finalize
 * 2. whisper-cpp-local   â†’ main.js handler: transcribe-local (alta precisÃ£o)
 * 3. whisper-1           â†’ main.js handler: transcribe-audio (online, OpenAI)
 *
 * Retorna: texto transcrito ou erro
 */
async function transcribeAudio(blob) {
	transcriptionMetrics.audioStartTime = Date.now();
	transcriptionMetrics.audioSize = blob.size;

	const buffer = Buffer.from(await blob.arrayBuffer());
	const sttModel = getConfiguredSTTModel();
	console.log(`ğŸ¤ TranscriÃ§Ã£o (${sttModel}): ${blob.size} bytes`);
	console.log(
		`â±ï¸ InÃ­cio: ${new Date(transcriptionMetrics.audioStartTime).toLocaleTimeString()}.${
			transcriptionMetrics.audioStartTime % 1000
		}`,
	);

	// Roteia para o modelo configurado
	if (sttModel === 'vosk-local') {
		// Modelo: Vosk local
		// Vantagem: RÃ¡pido (500-1000ms), offline, leve
		// Desvantagem: Menor precisÃ£o (modelo pequeno ~50MB)
		// Handlers: main.js â†’ vosk-transcribe (envia), vosk-finalize (recupera resultado acumulado)
		// Processo: WebM â†’ Buffer â†’ Vosk server Python â†’ Texto
		try {
			console.log(`ğŸš€ Enviando para Vosk (local)...`);
			transcriptionMetrics.whisperStartTime = Date.now();

			// Primeiro envia o Ã¡udio para processar
			await ipcRenderer.invoke('vosk-transcribe', buffer);

			// Depois finaliza para obter o resultado final acumulado
			const finalResult = await ipcRenderer.invoke('vosk-finalize');

			transcriptionMetrics.whisperEndTime = Date.now();
			const whisperTime = transcriptionMetrics.whisperEndTime - transcriptionMetrics.whisperStartTime;

			console.log(`âœ… Vosk concluÃ­do em ${whisperTime}ms`);

			// Vosk retorna um objeto: { final: string, partial: string, isFinal: boolean }
			// Extrai o texto final
			let transcribedText = '';
			if (typeof finalResult === 'string') {
				transcribedText = finalResult;
			} else if (typeof finalResult === 'object' && finalResult !== null) {
				// Usa final (que agora contÃ©m o resultado acumulado)
				transcribedText = finalResult.final || '';
			}

			console.log(`ğŸ“ Resultado (${transcribedText.length} chars): "${transcribedText.substring(0, 80)}..."`);

			return transcribedText;
		} catch (error) {
			console.error('âŒ Vosk falhou:', error.message);
			// Fallback para OpenAI
			try {
				console.log('ğŸ”„ Fallback para OpenAI...');
				return await ipcRenderer.invoke('transcribe-audio', buffer);
			} catch (openaiError) {
				throw new Error(`Falha na transcriÃ§Ã£o: ${openaiError.message}`);
			}
		}
	} else if (sttModel === 'whisper-cpp-local') {
		// Modelo: Whisper.cpp local
		// Vantagem: Alta precisÃ£o (modelo maior), offline
		// Desvantagem: Mais lento (2-4s), requer arquivos locais
		// Handler: main.js â†’ transcribe-local (buffer enviado via IPC)
		// Processo: WebM â†’ WAV â†’ Whisper.cpp CLI â†’ Texto
		try {
			console.log(`ğŸš€ Enviando para Whisper.cpp (local, alta precisÃ£o)...`);
			transcriptionMetrics.whisperStartTime = Date.now();

			const result = await ipcRenderer.invoke('transcribe-local', buffer);

			transcriptionMetrics.whisperEndTime = Date.now();
			const whisperTime = transcriptionMetrics.whisperEndTime - transcriptionMetrics.whisperStartTime;

			console.log(`âœ… Whisper.cpp concluÃ­do em ${whisperTime}ms`);
			console.log(`ğŸ“ Resultado (${result.length} chars): "${result.substring(0, 80)}..."`);

			return result;
		} catch (error) {
			console.error('âŒ Whisper.cpp falhou:', error.message);
			// Fallback para OpenAI
			try {
				console.log('ğŸ”„ Fallback para OpenAI...');
				return await ipcRenderer.invoke('transcribe-audio', buffer);
			} catch (openaiError) {
				throw new Error(`Falha na transcriÃ§Ã£o: ${openaiError.message}`);
			}
		}
	} else if (sttModel === 'whisper-1') {
		// Modelo: OpenAI Whisper-1 (online)
		// Vantagem: Melhor precisÃ£o (modelo grande), multilÃ­ngue
		// Desvantagem: Requer conexÃ£o, custo ($0.02/min), latÃªncia
		// Handler: main.js â†’ transcribe-audio (requer OpenAI API key configurada)
		// Processo: WebM â†’ Arquivo temp â†’ OpenAI API â†’ Texto
		transcriptionMetrics.whisperStartTime = Date.now();
		const result = await ipcRenderer.invoke('transcribe-audio', buffer);
		transcriptionMetrics.whisperEndTime = Date.now();

		const whisperTime = transcriptionMetrics.whisperEndTime - transcriptionMetrics.whisperStartTime;
		console.log(`âœ… Whisper-1 concluÃ­do em ${whisperTime}ms`);

		return result;
	} else {
		// Modelo desconhecido - tenta OpenAI como fallback
		console.warn(`âš ï¸ Modelo STT desconhecido: ${sttModel}, usando OpenAI`);
		transcriptionMetrics.whisperStartTime = Date.now();
		const result = await ipcRenderer.invoke('transcribe-audio', buffer);
		transcriptionMetrics.whisperEndTime = Date.now();
		return result;
	}
}

async function transcribeAudioPartial(blob) {
	const buffer = Buffer.from(await blob.arrayBuffer());
	const sttModel = getConfiguredSTTModel();

	if (sttModel === 'vosk-local') {
		// âš ï¸ Para Vosk, nÃ£o fazemos transcriÃ§Ã£o parcial em tempo real
		// Vosk acumula e retorna parciais, mas nÃ£o queremos enviÃ¡-las para a UI
		// A transcriÃ§Ã£o real serÃ¡ feita em transcribeAudio() quando a gravaÃ§Ã£o terminar
		return '';
	} else if (sttModel === 'whisper-1') {
		try {
			return await ipcRenderer.invoke('transcribe-audio-partial', buffer);
		} catch (error) {
			console.warn('âš ï¸ Whisper-1 parcial falhou:', error.message);
			return '';
		}
	} else {
		// Modelo desconhecido - tenta OpenAI como fallback
		try {
			return await ipcRenderer.invoke('transcribe-audio-partial', buffer);
		} catch (error) {
			console.warn('âš ï¸ TranscriÃ§Ã£o parcial falhou:', error.message);
			return '';
		}
	}
}

/* ===============================
   TRANSCRIÃ‡ÃƒO VOSK (MODO ENTREVISTA)
=============================== */

let voskAccumulatedText = ''; // Acumula resultado parcial do Vosk
let voskPartialTimer = null;
let voskScriptProcessor = null; // ScriptProcessorNode para capturar PCM bruto
let voskAudioBuffer = []; // Acumula PCM entre envios

/**
 * Converte array de floats PCM para Int16Array
 */
function floatToPCM16(floatArray) {
	const pcm16 = new Int16Array(floatArray.length);
	for (let i = 0; i < floatArray.length; i++) {
		pcm16[i] = Math.max(-1, Math.min(1, floatArray[i])) * 0x7fff;
	}
	return pcm16;
}

/**
 * Inicia captura de PCM bruto do Ã¡udio (substitui MediaRecorder para Vosk)
 * @param {MediaStreamAudioSourceNode} source - Source do Ã¡udio da stream
 * @deprecated Usar MediaRecorder com timeslice ao invÃ©s de ScriptProcessorNode
 */
function startVoskPcmCapture(source) {
	console.warn('âš ï¸ startVoskPcmCapture deprecated - use MediaRecorder timeslice instead');
}

/**
 * Para captura de PCM bruto do Vosk
 */
function stopVoskPcmCapture() {
	try {
		if (voskScriptProcessor) {
			voskScriptProcessor.disconnect();
			voskScriptProcessor.onaudioprocess = null;
			voskScriptProcessor = null;
		}
		voskAudioBuffer = [];
		console.log('âœ… Captura PCM para Vosk parada');
	} catch (error) {
		console.error('âŒ Erro ao parar captura PCM:', error);
	}
}

/**
 * Transcreve chunk de blob com Vosk (modo entrevista - padrÃ£o Deepgram)
 * Envia blobs WebM diretamente para Vosk via IPC
 */
/**
 * ğŸš« DEPRECADO: Vosk nÃ£o funciona com chunks WebM fragmentados do MediaRecorder
 * MediaRecorder gera blobs WebM incompletos que ffmpeg/Vosk rejeitam
 * SoluÃ§Ã£o: usar apenas Whisper para OUTPUT (funciona bem com WebM fragmentado)
 * @deprecated
 */
async function voskTranscribeChunkFromBlob(blob) {
	console.warn('âš ï¸ voskTranscribeChunkFromBlob deprecado - usar Whisper ao invÃ©s');
	// FunÃ§Ã£o removida - ver transcribeOutput() para transcriÃ§Ã£o final de saÃ­da
}

/**
 * Inicia captura de PCM bruto do Ã¡udio (substitui MediaRecorder para Vosk)
 * @param {MediaStreamAudioSourceNode} source - Source do Ã¡udio da stream
 * @deprecated Usar MediaRecorder com timeslice ao invÃ©s de ScriptProcessorNode
 */
function startVoskPcmCapture(source) {
	console.warn('âš ï¸ startVoskPcmCapture deprecated - usar MediaRecorder com timeslice ao invÃ©s de ScriptProcessorNode');
	// FunÃ§Ã£o deprecada mantida para compatibilidade reversa
}

/* ===============================
   DISPOSITIVOS / CONTROLE DE ÃUDIO
=============================== */

async function startAudio() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "startAudio"');

	// Se houver dispositivo de entrada selecionado, inicia a captura de Ã¡udio
	if (UIElements.inputSelect?.value) await startInput();
	// Se houver dispositivo de saÃ­da selecionado, inicia a captura de Ã¡udio
	if (UIElements.outputSelect?.value) await startOutput();

	debugLogRenderer('Fim da funÃ§Ã£o: "startAudio"');
}

async function stopAudio() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "stopAudio"');

	if (currentQuestion.text) closeCurrentQuestionForced();

	inputRecorder?.state === 'recording' && inputRecorder.stop();
	outputRecorder?.state === 'recording' && outputRecorder.stop();

	// ğŸ†• VOSK: Reset do estado
	if (ModeController.isInterviewMode()) {
		voskAccumulatedText = '';
		if (voskPartialTimer) {
			clearTimeout(voskPartialTimer);
			voskPartialTimer = null;
		}
	}

	stopInputMonitor();
	stopOutputMonitor();

	debugLogRenderer('Fim da funÃ§Ã£o: "stopAudio"');
}

async function restartAudioPipeline() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "restartAudioPipeline"');

	stopAudio();

	debugLogRenderer('Fim da funÃ§Ã£o: "restartAudioPipeline"');
}

/* ===============================
   AUDIO - VOLUME MONITORING
=============================== */

// Inicia apenas monitoramento de volume (sem gravar)
async function startInputVolumeMonitoring() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "startInputVolumeMonitoring"');

	if (APP_CONFIG.MODE_DEBUG) {
		console.log('ğŸ¤ Monitoramento de volume entrada (modo teste)...');
		return;
	}

	if (!UIElements.inputSelect?.value) {
		console.log('âš ï¸ Nenhum dispositivo input selecionado');
		return;
	}

	if (!audioContext) {
		audioContext = new AudioContext();
	}

	// ğŸ”¥ NOVO: Se jÃ¡ tem stream ativa, nÃ£o faz nada
	if (inputStream && inputAnalyser) {
		console.log('â„¹ï¸ Monitoramento de volume de entrada jÃ¡ ativo');
		return;
	}

	try {
		// Verificar se isRunning Ã© false antes de iniciar o stream
		if (!isRunning) {
			console.log('ğŸ”„ Iniciando stream de Ã¡udio (input)...');

			inputStream = await navigator.mediaDevices.getUserMedia({
				audio: { deviceId: { exact: UIElements.inputSelect.value } },
			});

			const source = audioContext.createMediaStreamSource(inputStream);

			inputAnalyser = audioContext.createAnalyser();
			inputAnalyser.fftSize = 256;
			inputData = new Uint8Array(inputAnalyser.frequencyBinCount);
			source.connect(inputAnalyser);

			console.log('âœ… Monitoramento de volume de entrada iniciado com sucesso');
			updateInputVolume(); // ğŸ”¥ Inicia o loop de atualizaÃ§Ã£o
		}
	} catch (error) {
		console.error('âŒ Erro ao iniciar monitoramento de volume de entrada:', error);
		inputStream = null;
		inputAnalyser = null;
	}

	debugLogRenderer('Fim da funÃ§Ã£o: "startInputVolumeMonitoring"');
}

// Inicia apenas monitoramento de volume para output (sem gravar)
async function startOutputVolumeMonitoring() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "startOutputVolumeMonitoring"');

	// Se o modo de debug estiver ativo, retorna
	if (APP_CONFIG.MODE_DEBUG) {
		console.log('ğŸ”Š Monitoramento de volume saÃ­da (modo teste)...');
		return;
	}

	// Se nÃ£o houver dispositivo de saÃ­da selecionado, retorna
	if (!UIElements.outputSelect?.value) {
		console.log('âš ï¸ Nenhum dispositivo output selecionado');
		return;
	}

	// Se nÃ£o houver contexto de Ã¡udio, cria um novo
	if (!audioContext) {
		audioContext = new AudioContext();
	}

	// Se jÃ¡ houver stream e analisador de frequÃªncia ativos, retorna
	if (outputStream && outputAnalyser) {
		console.log('â„¹ï¸ Monitoramento de volume de saÃ­da jÃ¡ ativo');
		return;
	}

	try {
		// Se isRunning for false, inicia o stream de Ã¡udio (output)
		if (!isRunning) {
			console.log('ğŸ”„ Iniciando stream de Ã¡udio (output)...');

			// Cria a stream de Ã¡udio (outputStream)
			await createOutputStream();

			// Inicia o loop de atualizaÃ§Ã£o do volume de saÃ­da
			updateOutputVolume();
		}

		debugLogRenderer('Fim da funÃ§Ã£o: "startOutputVolumeMonitoring"');
	} catch (error) {
		console.error('âŒ Erro ao iniciar monitoramento de volume de saÃ­da:', error);

		// Limpa a stream e o analisador de frequÃªncia (outputStream e outputAnalyser)
		outputStream = null;
		outputAnalyser = null;
	}
}

function stopInputVolumeMonitoring() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "stopInputVolumeMonitoring"');

	// Se isRunning true, nÃ£o para o monitoramento
	if (isRunning) {
		console.log('â„¹ï¸ Monitoramento de volume de entrada em execuÃ§Ã£o, isRunning = true â€” pulando parada');

		debugLogRenderer('Fim da funÃ§Ã£o: "stopInputVolumeMonitoring"');
		return;
	}

	// 1. Para o loop de animaÃ§Ã£o
	if (inputVolumeAnimationId) {
		cancelAnimationFrame(inputVolumeAnimationId);
		inputVolumeAnimationId = null;
	}

	// 2. Para as tracks de Ã¡udio para economizar energia/recurso
	if (inputStream) {
		inputStream.getTracks().forEach(track => track.stop());
		inputStream = null;
	}

	inputAnalyser = null;
	inputData = null;

	// 3. Zera a UI
	emitUIChange('onInputVolumeUpdate', { percent: 0 });

	console.log('ğŸ›‘ Monitoramento de volume de entrada parado');

	debugLogRenderer('Fim da funÃ§Ã£o: "stopInputVolumeMonitoring"');
}

function stopOutputVolumeMonitoring() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "stopOutputVolumeMonitoring"');

	// Se isRunning true, nÃ£o para o monitoramento
	if (isRunning) {
		console.log('â„¹ï¸ Monitoramento de volume de saÃ­da em execuÃ§Ã£o, isRunning = true â€” pulando parada');

		debugLogRenderer('Fim da funÃ§Ã£o: "stopOutputVolumeMonitoring"');
		return;
	}

	// 1. Para o loop de animaÃ§Ã£o
	if (outputVolumeAnimationId) {
		cancelAnimationFrame(outputVolumeAnimationId);
		outputVolumeAnimationId = null;
	}

	// 2.Para as tracks de Ã¡udio para economizar energia/recurso
	if (outputStream) {
		outputStream.getTracks().forEach(track => track.stop());
		outputStream = null;
	}

	outputAnalyser = null;
	outputData = null;

	// 3. Zera a UI
	emitUIChange('onOutputVolumeUpdate', { percent: 0 });

	console.log('ğŸ›‘ Monitoramento de volume de saÃ­da parado');

	debugLogRenderer('Fim da funÃ§Ã£o: "stopOutputVolumeMonitoring"');
}

/* ===============================
   AUDIO - INPUT (VOCÃŠ)
=============================== */

async function startInput() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "startInput"');

	if (APP_CONFIG.MODE_DEBUG) {
		const text = 'Iniciando monitoramento de entrada de Ã¡udio (modo teste)...';
		addTranscript(YOU, text);
		return;
	}

	if (!UIElements.inputSelect?.value) return;

	if (!audioContext) {
		audioContext = new AudioContext();
	}

	// CRÃTICO: Evita recriar recorder E stream se jÃ¡ existem
	if (inputRecorder && inputRecorder.state !== 'inactive') {
		console.log('â„¹ï¸ inputRecorder jÃ¡ existe e estÃ¡ ativo, pulando reconfiguraÃ§Ã£o');
		return;
	}

	// Se jÃ¡ existe stream mas precisa reconfigurar, limpa primeiro
	if (inputStream) {
		console.log('ğŸ§¹ Limpando stream de entrada anterior antes de recriar');
		inputStream.getTracks().forEach(t => t.stop());
		inputStream = null;
	}

	try {
		inputStream = await navigator.mediaDevices.getUserMedia({
			audio: { deviceId: { exact: UIElements.inputSelect.value } },
		});

		const source = audioContext.createMediaStreamSource(inputStream);

		inputAnalyser = audioContext.createAnalyser();
		inputAnalyser.fftSize = 256;
		inputData = new Uint8Array(inputAnalyser.frequencyBinCount);
		source.connect(inputAnalyser);

		// recorder SEMPRE existe
		inputRecorder = new MediaRecorder(inputStream, {
			mimeType: 'audio/webm;codecs=opus',
		});

		inputRecorder.ondataavailable = e => {
			console.log('ğŸ”¥ input.ondataavailable - chunk tamanho:', e.data?.size || e.data?.byteLength || 'n/a');

			inputChunks.push(e.data);

			// MODO ENTREVISTA â€“ permite transcriÃ§Ã£o incremental
			if (ModeController.isInterviewMode()) {
				console.log('ğŸ§© handlePartialInputChunk chamado (input)');
				handlePartialInputChunk(e.data);
			}
		};

		inputRecorder.onstop = () => {
			console.log('â¹ï¸ inputRecorder.onstop chamado');

			// marca o momento exato em que a gravaÃ§Ã£o parou
			lastInputStopAt = Date.now();

			// PROTEÃ‡ÃƒO CRÃTICA: Se lastInputStartAt for null/undefined, usar stopAt como fallback
			// MAS nÃ£o usar para calcular duration (isso causaria grav 0ms)
			const actualStartTime =
				lastInputStartAt !== null && lastInputStartAt !== undefined ? lastInputStartAt : lastInputStopAt;

			const recordingDuration = lastInputStopAt - actualStartTime;

			// Logs detalhados para debug
			console.log('â±ï¸ Parada:', new Date(lastInputStopAt).toLocaleTimeString());
			if (lastInputStartAt !== null && lastInputStartAt !== undefined) {
				console.log('â±ï¸ InÃ­cio:', new Date(lastInputStartAt).toLocaleTimeString());
			} else {
				console.warn('âš ï¸ AVISO: lastInputStartAt Ã© null/undefined! Usando lastInputStopAt como fallback.');
				lastInputStartAt = lastInputStopAt;
			}
			console.log('â±ï¸ DuraÃ§Ã£o da gravaÃ§Ã£o:', recordingDuration, 'ms');

			// Cancela qualquer timer pendente de transcriÃ§Ã£o parcial
			// Isso evita que handlePartialInputChunk processe chunks apÃ³s onstop
			if (inputPartialTimer) {
				clearTimeout(inputPartialTimer);
				inputPartialTimer = null;
				console.log('â±ï¸ Cancelado timer de transcriÃ§Ã£o parcial (inputPartialTimer)');
			}

			// Limpa chunks parciais acumulados para evitar duplicaÃ§Ã£o
			inputPartialChunks = [];
			console.log('ğŸ—‘ï¸ Limpos chunks parciais acumulados (inputPartialChunks)');

			// adiciona placeholder visual para indicar que estamos aguardando a transcriÃ§Ã£o
			// usa startAt se disponÃ­vel para mostrar o horÃ¡rio inicial enquanto aguarda
			const timeForPlaceholder = lastInputStartAt || lastInputStopAt;
			lastInputPlaceholderEl = addTranscript(YOU, '...', timeForPlaceholder);
			if (lastInputPlaceholderEl) {
				lastInputPlaceholderEl.dataset.stopAt = lastInputStopAt;
				// SEMPRE salvar startAt se estiver disponÃ­vel (atÃ© que 0 Ã© vÃ¡lido, nÃ£o null)
				if (lastInputStartAt !== null && lastInputStartAt !== undefined) {
					lastInputPlaceholderEl.dataset.startAt = lastInputStartAt;
				} else {
					// Se startAt nÃ£o foi setado corretamente, usar stopAt como fallback
					lastInputPlaceholderEl.dataset.startAt = lastInputStopAt;
				}
			}

			// âœ… CHAMADA CRÃTICA: Transcreve o Ã¡udio capturado
			transcribeInput();
		};

		// Inicia loop de volume apenas se nÃ£o estiver rodando
		if (!inputVolumeAnimationId) {
			updateInputVolume();
		}

		console.log('âœ… startInput: Configurado com sucesso');
	} catch (error) {
		console.error('âŒ Erro em startInput:', error);
		inputStream = null;
		inputRecorder = null;
		throw error;
	}

	debugLogRenderer('Fim da funÃ§Ã£o: "startInput"');
}

function updateInputVolume() {
	//debugLogRenderer('InÃ­cio da funÃ§Ã£o: "updateInputVolume"');

	// CRÃTICO: Verifica se deve continuar ANTES de fazer qualquer processamento
	if (!inputAnalyser || !inputData) {
		console.log('âš ï¸ updateInputVolume: analyser ou data nÃ£o disponÃ­vel, parando loop');
		if (inputVolumeAnimationId) {
			cancelAnimationFrame(inputVolumeAnimationId);
			inputVolumeAnimationId = null;
		}
		emitUIChange('onInputVolumeUpdate', { percent: 0 });
		return;
	}

	try {
		inputAnalyser.getByteFrequencyData(inputData);
		const avg = inputData.reduce((a, b) => a + b, 0) / inputData.length;
		const percent = Math.min(100, Math.round((avg / 80) * 100));

		// Emite evento em vez de atualizar DOM diretamente
		emitUIChange('onInputVolumeUpdate', { percent });

		if (avg > INPUT_SPEECH_THRESHOLD && inputRecorder && isRunning) {
			if (!inputSpeaking) {
				inputSpeaking = true;
				inputChunks = [];

				const slice = ModeController.mediaRecorderTimeslice();
				lastInputStartAt = Date.now();
				console.log(
					'ğŸ™ï¸ iniciando gravaÃ§Ã£o de entrada (inputRecorder.start) - startAt',
					new Date(lastInputStartAt).toLocaleTimeString(),
					'| inputSpeaking =',
					inputSpeaking,
				);
				slice ? inputRecorder.start(slice) : inputRecorder.start();
			}
			if (inputSilenceTimer) {
				clearTimeout(inputSilenceTimer);
				inputSilenceTimer = null;
			}
		} else if (inputSpeaking && !inputSilenceTimer && inputRecorder) {
			inputSilenceTimer = setTimeout(() => {
				inputSpeaking = false;
				inputSilenceTimer = null;
				console.log(
					'â¹ï¸ parando gravaÃ§Ã£o de entrada por silÃªncio (inputRecorder.stop) | lastInputStartAt =',
					lastInputStartAt ? new Date(lastInputStartAt).toLocaleTimeString() : 'NULL',
				);
				if (inputRecorder && inputRecorder.state === 'recording') {
					inputRecorder.stop();
				}
			}, INPUT_SILENCE_TIMEOUT);
		}
	} catch (error) {
		console.error('âŒ Erro em updateInputVolume:', error);
		if (inputVolumeAnimationId) {
			cancelAnimationFrame(inputVolumeAnimationId);
			inputVolumeAnimationId = null;
		}
		emitUIChange('onInputVolumeUpdate', { percent: 0 });
		return;
	}

	// Continua o loop apenas se tudo estiver OK
	inputVolumeAnimationId = requestAnimationFrame(updateInputVolume);

	//debugLogRenderer('Fim da funÃ§Ã£o: "updateInputVolume"');
}

function stopInputMonitor() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "stopInputMonitor"');

	// 1. Para o loop de animation PRIMEIRO
	if (inputVolumeAnimationId) {
		cancelAnimationFrame(inputVolumeAnimationId);
		inputVolumeAnimationId = null;
		console.log('âœ… Loop de animaÃ§Ã£o de entrada cancelado');
	}

	// 2. Para o recorder se estiver gravando
	if (inputRecorder) {
		if (inputRecorder.state === 'recording') {
			console.log('â¹ï¸ Parando recorder de entrada...');
			inputRecorder.stop();
		}
		inputRecorder = null;
	}

	// 3. Fecha a stream
	if (inputStream) {
		inputStream.getTracks().forEach(t => {
			t.stop();
			console.log('âœ… Track de entrada parada:', t.label);
		});
		inputStream = null;
	}

	// 4. Limpa analyser e dados
	inputAnalyser = null;
	inputData = null;

	// 5. Reseta estado
	inputSpeaking = false;
	if (inputSilenceTimer) {
		clearTimeout(inputSilenceTimer);
		inputSilenceTimer = null;
	}

	// 6. Atualiza UI
	emitUIChange('onInputVolumeUpdate', { percent: 0 });

	debugLogRenderer('Fim da funÃ§Ã£o: "stopInputMonitor"');
	return Promise.resolve();
}

/* ===============================
   AUDIO - OUTPUT (OUTROS) - VIA VOICEMEETER
=============================== */

async function createOutputStream() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "createOutputStream"');

	// Cria a stream de Ã¡udio (outputStream)
	outputStream = await navigator.mediaDevices.getUserMedia({
		audio: { deviceId: { exact: UIElements.outputSelect.value } },
	});

	// Cria o source de Ã¡udio (source)
	const source = audioContext.createMediaStreamSource(outputStream);

	// Cria o analisador de frequÃªncia (outputAnalyser)
	outputAnalyser = audioContext.createAnalyser();
	// Define o tamanho do FFT (fftSize) como 256
	outputAnalyser.fftSize = 256;
	// Cria os dados (outputData)
	outputData = new Uint8Array(outputAnalyser.frequencyBinCount);
	// Conecta o source ao analisador de frequÃªncia
	source.connect(outputAnalyser);

	debugLogRenderer('Fim da funÃ§Ã£o: "createOutputStream"');

	return source;
}

async function startOutput() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "startOutput"');

	// Se o modo de debug estiver ativo, retorna
	if (APP_CONFIG.MODE_DEBUG) {
		const text = 'Iniciando monitoramento de saÃ­da de Ã¡udio (modo teste)...';
		addTranscript(OTHER, text);
		return;
	}

	// Se nÃ£o houver dispositivo de saÃ­da selecionado, retorna
	if (!UIElements.outputSelect?.value) {
		console.log('âš ï¸ Nenhum dispositivo output selecionado');
		return;
	}

	// Se nÃ£o houver contexto de Ã¡udio, cria um novo
	if (!audioContext) {
		audioContext = new AudioContext();
	}

	// Se jÃ¡ houver outputRecorder e ele estiver ativo, retorna
	if (outputRecorder && outputRecorder.state !== 'inactive') {
		console.log('â„¹ï¸ outputRecorder jÃ¡ existe e estÃ¡ ativo, pulando reconfiguraÃ§Ã£o');
		return;
	}

	// Se jÃ¡ houver outputStream, limpa primeiro
	if (outputStream) {
		console.log('ğŸ§¹ Limpando stream de saÃ­da anterior antes de recriar');
		outputStream.getTracks().forEach(t => t.stop());
		outputStream = null;
	}

	try {
		console.log('ğŸ”„ startOutput: Configurando monitoramento de saÃ­da de Ã¡udio...');

		// Cria a stream de Ã¡udio (outputStream)
		await createOutputStream();

		// Cria o recorder (outputRecorder), recorder SEMPRE existe
		outputRecorder = new MediaRecorder(outputStream, {
			mimeType: 'audio/webm;codecs=opus',
		});

		// Define o callback para quando houver dados disponÃ­veis no outputRecorder, acionado ao chamar outputRecorder.start()
		outputRecorder.ondataavailable = e => {
			console.log(
				'ğŸ”¥ outputRecorder.ondataavailable chamado - chunk tamanho:',
				e.data?.size || e.data?.byteLength || 'n/a',
			);

			// Adiciona o chunk (pedaÃ§os de dados) ao array de chunks de saÃ­da
			outputChunks.push(e.data);

			// MODO ENTREVISTA â€“ permite transcriÃ§Ã£o incremental
			if (ModeController.isInterviewMode()) {
				console.log('ğŸ§© handlePartialOutputChunk chamado (output)');
				handlePartialOutputChunk(e.data);
			}
		};

		// Define o callback para quando o outputRecorder for parado, acionado ao chamar outputRecorder.stop()
		outputRecorder.onstop = () => {
			console.log('â¹ï¸ outputRecorder.onstop chamado');

			// Marca o momento exato em que a gravaÃ§Ã£o parou
			lastOutputStopAt = Date.now();

			// ğŸ”¥ CRÃTICO: Capturar timestamps AGORA em variÃ¡veis temporÃ¡rias
			// Essas variÃ¡veis sÃ£o isoladas e NÃƒO serÃ£o sobrescritas por updateOutputVolume()
			pendingOutputStartAt = lastOutputStartAt;
			pendingOutputStopAt = lastOutputStopAt;

			// Debug: Verificar valores de lastOutputStartAt
			console.log('ğŸ” DEBUG outputRecorder.onstop:');
			console.log('  â†’ lastOutputStartAt:', lastOutputStartAt, `(tipo: ${typeof lastOutputStartAt})`);
			console.log('  â†’ lastOutputStopAt:', lastOutputStopAt, `(tipo: ${typeof lastOutputStopAt})`);
			console.log('  â†’ ğŸ”¥ Capturado em pending: start=', pendingOutputStartAt, 'stop=', pendingOutputStopAt);

			// Calcula duraÃ§Ã£o com proteÃ§Ã£o contra valores invÃ¡lidos
			let recordingDuration = 0;
			if (lastOutputStartAt !== null && lastOutputStartAt !== undefined && typeof lastOutputStartAt === 'number') {
				recordingDuration = lastOutputStopAt - lastOutputStartAt;
			} else {
				console.warn('âš ï¸ AVISO: lastOutputStartAt Ã© invÃ¡lido, usando 0 como duraÃ§Ã£o');
				recordingDuration = 0;
			}

			console.log('â±ï¸ Parada: ' + new Date(lastOutputStopAt).toLocaleTimeString());
			console.log('â±ï¸ DuraÃ§Ã£o da gravaÃ§Ã£o:', recordingDuration, 'ms');

			// Cancela qualquer timer pendente de transcriÃ§Ã£o parcial
			// Isso evita que transcribeOutputPartial processe chunks apÃ³s onstop
			if (outputPartialTimer) {
				clearTimeout(outputPartialTimer);
				outputPartialTimer = null;
				console.log('â±ï¸ Cancelado timer de transcriÃ§Ã£o parcial (outputPartialTimer)');
			}

			// Limpa chunks parciais acumulados para evitar duplicaÃ§Ã£o
			outputPartialChunks = [];
			console.log('ğŸ—‘ï¸ Limpos chunks parciais acumulados (outputPartialChunks)');

			// Inicia a transcriÃ§Ã£o do Ã¡udio de saÃ­da (Vosk)
			// âš ï¸ O placeholder serÃ¡ criado direto no transcribeOutput() com as mÃ©tricas corretas
			transcribeOutput();
		};

		// Inicia o loop de atualizaÃ§Ã£o do volume de saÃ­da, se nÃ£o estiver rodando
		if (!outputVolumeAnimationId) {
			updateOutputVolume();
		}

		console.log('âœ… startOutput: Monitoramento de saÃ­da de Ã¡udio configurado com sucesso');
	} catch (error) {
		console.error('âŒ Erro em startOutput:', error);

		outputStream = null;
		outputRecorder = null;
		throw error;
	}

	debugLogRenderer('Fim da funÃ§Ã£o: "startOutput"');
}

function updateOutputVolume() {
	//debugLogRenderer('InÃ­cio da funÃ§Ã£o: "updateOutputVolume"');

	// CrÃ­tico: Verifica se o analisador de frequÃªncia (outputAnalyser) e os dados (outputData)
	// estÃ£o disponÃ­veis antes de continuar o loop de animaÃ§Ã£o
	if (!outputAnalyser || !outputData) {
		console.log('âš ï¸ updateOutputVolume: outputAnalyser ou outputData nÃ£o disponÃ­vel, parando loop de animaÃ§Ã£o');

		// Se o loop de animaÃ§Ã£o (outputVolumeAnimationId) estiver definido, limpa o loop de animaÃ§Ã£o
		if (outputVolumeAnimationId) {
			// Para o loop de animaÃ§Ã£o
			cancelAnimationFrame(outputVolumeAnimationId);
			// Limpa o loop de animaÃ§Ã£o
			outputVolumeAnimationId = null;
		}

		// Emite o evento 'onOutputVolumeUpdate' para atualizar o volume de saÃ­da
		emitUIChange('onOutputVolumeUpdate', { percent: 0 });

		return;
	}

	try {
		// ObtÃ©m os dados do analisador de frequÃªncia (outputAnalyser)
		outputAnalyser.getByteFrequencyData(outputData);
		// Calcula o volume mÃ©dio (avg) dos dados do analisador de frequÃªncia (outputData)
		const avg = outputData.reduce((a, b) => a + b, 0) / outputData.length;
		// Calcula o percentual de volume (percent) dos dados do analisador de frequÃªncia (outputData)
		const percent = Math.min(100, Math.round((avg / 60) * 100));

		// Emite o evento 'onOutputVolumeUpdate' para atualizar o volume de saÃ­da
		emitUIChange('onOutputVolumeUpdate', { percent });

		// Se o volume mÃ©dio (avg) estiver acima do limite (OUTPUT_SPEECH_THRESHOLD)
		// e o recorder (outputRecorder) estiver rodando e o isRunning for true, inicia a gravaÃ§Ã£o de saÃ­da
		if (avg > OUTPUT_SPEECH_THRESHOLD && outputRecorder && isRunning) {
			// Se o outputSpeaking for false, inicia a gravaÃ§Ã£o de saÃ­da
			if (!outputSpeaking) {
				// RESET: Limpa valores da frase anterior ANTES de iniciar nova frase
				lastOutputPlaceholderEl = null;
				lastOutputStopAt = null;
				// Nota: lastOutputStartAt serÃ¡ atualizado abaixo
				console.log('ğŸ§¹ LIMPAR: Resetando lastOutputPlaceholderEl e lastOutputStopAt ANTES de nova frase');

				// Define o estado de outputSpeaking como true
				outputSpeaking = true;
				// Limpa o array de chunks de saÃ­da
				outputChunks = [];

				// Define o momento exato em que a gravaÃ§Ã£o de saÃ­da foi iniciada
				lastOutputStartAt = Date.now();

				console.log('ğŸ™ï¸ InÃ­cio: ' + new Date(lastOutputStartAt).toLocaleTimeString());
				console.log('ğŸ“Š lastOutputStartAt definido para:', lastOutputStartAt);

				// ğŸ”¥ PASSO 1: Criar placeholder IMEDIATAMENTE quando fala inicia
				// Isso garante que "Outros: ..." apareÃ§a na tela assim que detecta fala
				try {
					// ğŸ”¥ Gerar ID ANTES de criar o placeholder
					lastOutputPlaceholderId = 'placeholder-' + lastOutputStartAt + '-' + Math.random();
					// ğŸ”¥ Passar o ID para ser atribuÃ­do ao elemento real no DOM
					lastOutputPlaceholderEl = addTranscript(OTHER, '...', lastOutputStartAt, lastOutputPlaceholderId);
					if (lastOutputPlaceholderEl && lastOutputPlaceholderEl.dataset) {
						lastOutputPlaceholderEl.dataset.startAt = lastOutputStartAt;
						lastOutputPlaceholderEl.dataset.stopAt = lastOutputStartAt; // provisÃ³rio, serÃ¡ atualizado
					}
					console.log('âœ¨ Placeholder criado no inÃ­cio da fala para "Outros" (id=' + lastOutputPlaceholderId + ')');
				} catch (err) {
					console.warn('âš ï¸ Falha ao criar placeholder no inÃ­cio:', err);
				}

				// Usar o mesmo timeslice que INPUT para manter consistÃªncia
				const slice = ModeController.mediaRecorderTimeslice();
				slice ? outputRecorder.start(slice) : outputRecorder.start();
			}
			if (outputSilenceTimer) {
				clearTimeout(outputSilenceTimer);
				outputSilenceTimer = null;
			}
		} else if (outputSpeaking && !outputSilenceTimer && outputRecorder) {
			// Define o timer de silÃªncio (outputSilenceTimer)
			outputSilenceTimer = setTimeout(() => {
				// Define o estado de outputSpeaking como false
				outputSpeaking = false;
				// Limpa o timer de silÃªncio (outputSilenceTimer)
				outputSilenceTimer = null;

				console.log('â¹ï¸ parando gravaÃ§Ã£o de saÃ­da por silÃªncio (outputRecorder.stop)');

				// Se o recorder (outputRecorder) estiver rodando, para a gravaÃ§Ã£o de saÃ­da
				if (outputRecorder && outputRecorder.state === 'recording') {
					// Para a gravaÃ§Ã£o de saÃ­da
					outputRecorder.stop();
				}
			}, OUTPUT_SILENCE_TIMEOUT); // Tempo de espera para silÃªncio
		}
	} catch (error) {
		console.error('âŒ Erro em updateOutputVolume:', error);
		// Se o loop de animaÃ§Ã£o (outputVolumeAnimationId) estiver definido, limpa o loop de animaÃ§Ã£o
		if (outputVolumeAnimationId) {
			// Para o loop de animaÃ§Ã£o
			cancelAnimationFrame(outputVolumeAnimationId);
			// Limpa o loop de animaÃ§Ã£o
			outputVolumeAnimationId = null;
		}
		// Emite o evento 'onOutputVolumeUpdate' para atualizar o volume de saÃ­da
		emitUIChange('onOutputVolumeUpdate', { percent: 0 });
		return;
	}

	// Continua o loop de animaÃ§Ã£o apenas se tudo estiver OK
	outputVolumeAnimationId = requestAnimationFrame(updateOutputVolume);

	//debugLogRenderer('Fim da funÃ§Ã£o: "updateOutputVolume"');
}

function stopOutputMonitor() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "stopOutputMonitor"');

	// 1. Para o loop de animation PRIMEIRO
	if (outputVolumeAnimationId) {
		cancelAnimationFrame(outputVolumeAnimationId);
		outputVolumeAnimationId = null;
		console.log('âœ… Loop de animaÃ§Ã£o de saÃ­da cancelado');
	}

	// 2. Para o recorder se estiver gravando
	if (outputRecorder) {
		if (outputRecorder.state === 'recording') {
			console.log('â¹ï¸ Parando recorder de saÃ­da...');
			outputRecorder.stop();
		}
		outputRecorder = null;
	}

	// 3. Fecha a stream
	if (outputStream) {
		outputStream.getTracks().forEach(t => {
			t.stop();
			console.log('âœ… Track de saÃ­da parada:', t.label);
		});
		outputStream = null;
	}

	// 4. Limpa analyser e dados
	outputAnalyser = null;
	outputData = null;

	// 5. Reseta estado
	outputSpeaking = false;
	if (outputSilenceTimer) {
		clearTimeout(outputSilenceTimer);
		outputSilenceTimer = null;
	}

	// 6. Atualiza UI
	emitUIChange('onOutputVolumeUpdate', { percent: 0 });

	debugLogRenderer('Fim da funÃ§Ã£o: "stopOutputMonitor"');
	return Promise.resolve();
}

/* ===============================
   MODO ENTREVISTA - TRANSCRIÃ‡ÃƒO PARCIAL
=============================== */

async function handlePartialInputChunk(blobChunk) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "handlePartialInputChunk"');
	if (!ModeController.isInterviewMode()) return;

	// ignora ruÃ­do
	if (blobChunk.size < 200) return;

	inputPartialChunks.push(blobChunk);

	if (inputPartialTimer) clearTimeout(inputPartialTimer);

	inputPartialTimer = setTimeout(async () => {
		if (!inputPartialChunks.length) return;

		const blob = new Blob(inputPartialChunks, { type: 'audio/webm' });
		inputPartialChunks = [];

		try {
			const buffer = Buffer.from(await blob.arrayBuffer());
			const partialText = (await transcribeAudioPartial(blob))?.trim();

			if (partialText && !isGarbageSentence(partialText)) {
				addTranscript(YOU, partialText);
				handleSpeech(YOU, partialText);
			}
		} catch (err) {
			console.warn('âš ï¸ erro na transcriÃ§Ã£o parcial (INPUT)', err);
		}
	}, 180); // janela curta (reduzida de 250 -> 180)

	debugLogRenderer('Fim da funÃ§Ã£o:  "handlePartialInputChunk"');
}

async function handlePartialOutputChunk(blobChunk) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "handlePartialOutputChunk"');
	if (!ModeController.isInterviewMode()) return;

	// ignora ruÃ­do
	if (blobChunk.size < 200) return;

	outputPartialChunks.push(blobChunk);

	if (outputPartialTimer) clearTimeout(outputPartialTimer);

	outputPartialTimer = setTimeout(async () => {
		if (!outputPartialChunks.length) return;

		const blob = new Blob(outputPartialChunks, { type: 'audio/webm' });
		outputPartialChunks = [];

		try {
			const buffer = Buffer.from(await blob.arrayBuffer());
			const partialText = (await transcribeAudioPartial(blob))?.trim();

			if (partialText && !isGarbageSentence(partialText)) {
				addTranscript(OTHER, partialText);
				// NÃƒO chamar handleSpeech aqui - evita consolidaÃ§Ã£o nas parciais
				// consolidaÃ§Ã£o sÃ³ acontece em transcribeOutput() para o Ã¡udio final
			}
		} catch (err) {
			console.warn('âš ï¸ erro na transcriÃ§Ã£o parcial (OUTPUT)', err);
		}
	}, 180); // janela curta (reduzida de 250 -> 180)

	debugLogRenderer('Fim da funÃ§Ã£o:  "handlePartialOutputChunk"');
}

function transcribeOutputPartial(blobChunk) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "transcribeOutputPartial"');

	// Se nÃ£o estiver no modo entrevista, retorna
	if (!ModeController.isInterviewMode()) {
		console.log('â„¹ï¸ transcribeOutputPartial: retornando, modo entrevista nÃ£o ativo');

		debugLogRenderer('Fim da funÃ§Ã£o: "transcribeOutputPartial"');
		return;
	}

	// Desabilitado temporariamente (teste)
	if (DESABILITADO_TEMPORARIAMENTE) {
		debugLogRenderer('Fim da funÃ§Ã£o: "transcribeOutputPartial" ğŸ”’ DESABILITADO TEMPORARIAMENTE');
		return;
	}

	// MODO ENTREVISTA â€“ permite transcriÃ§Ã£o incremental

	// Ignora ruÃ­do, evita blobs pequenos demais
	if (blobChunk.size < MIN_OUTPUT_AUDIO_SIZE_INTERVIEW) {
		console.log('âš ï¸ Ignorando blobChunk pequeno demais para transcriÃ§Ã£o parcial (OUTPUT) - size:', blobChunk.size);

		debugLogRenderer('Fim da funÃ§Ã£o: "transcribeOutputPartial"');
		return;
	}

	// Adiciona o chunk ao array de chunks parciais de saÃ­da
	outputPartialChunks.push(blobChunk);
	console.log('ğŸ“¦ Chunk acumulado:', blobChunk.size, 'bytes | Total chunks:', outputPartialChunks.length);

	// Reinicia o timer para processar o chunk parcial apÃ³s um curto perÃ­odo
	if (outputPartialTimer) clearTimeout(outputPartialTimer);

	// calcula delay respeitando um intervalo mÃ­nimo entre requisiÃ§Ãµes STT parciais
	const now = Date.now();
	const elapsedSinceLast = typeof lastPartialSttAt === 'number' ? now - lastPartialSttAt : Infinity;
	let intendedDelay = 120; // janela base para agrupar chunks
	if (elapsedSinceLast < PARTIAL_MIN_INTERVAL_MS) {
		intendedDelay = PARTIAL_MIN_INTERVAL_MS - elapsedSinceLast + 50; // pequeno buffer extra
		console.log('â±ï¸ Ajustando delay parcial para respeitar rate-limit (ms):', intendedDelay);
	}

	// Define um timer para processar o chunk parcial apÃ³s X(ms)
	// Timeout curto (300ms) para agrupar ~5-8 chunks e enviar rÃ¡pido para STT
	outputPartialTimer = setTimeout(async () => {
		// Se nÃ£o houver chunks parciais de saÃ­da, retorna
		if (!outputPartialChunks.length) {
			console.log('âš ï¸ Nenhum chunk parcial para processar');
			return;
		}

		// Cria um blob a partir dos chunks parciais de saÃ­da
		const blob = new Blob(outputPartialChunks, { type: 'audio/webm' });

		// Loga o tamanho total do blob parcial
		const totalSize = outputPartialChunks.reduce((acc, chunk) => acc + chunk.size, 0);
		console.log('ğŸµ Processando blob parcial:', totalSize, 'bytes de', outputPartialChunks.length, 'chunks');

		// Limpa o array de chunks parciais de saÃ­da apÃ³s criar blob
		outputPartialChunks = [];

		try {
			// Envia para transcriÃ§Ã£o o blob parcial de saÃ­da
			const partialText = await transcribeAudioPartial(blob);
			// marca Ãºltimo envio parcial
			lastPartialSttAt = Date.now();
			console.log('ğŸ“ transcribeOutputPartial: TranscriÃ§Ã£o recebida: ', partialText);

			// Ignora transcriÃ§Ã£o vazia
			if (!partialText || partialText.trim().length === 0) {
				console.log('âš ï¸ TranscriÃ§Ã£o vazia - ignorando');
				return;
			}

			// Ignora sentenÃ§as garbage
			if (isGarbageSentence(partialText)) {
				console.log('ğŸ—‘ï¸ SentenÃ§a descartada (garbage):', partialText);
				return;
			}

			// acumula texto parcial
			outputPartialText += ' ' + partialText;
			outputPartialText = outputPartialText.trim();
			console.log('ğŸ“‹ Texto acumulado:', outputPartialText);

			// Atualiza UI com transcriÃ§Ã£o parcial imediatamente (usa placeholder incremental)
			try {
				// cria placeholder se ainda nÃ£o existe (usa startAt se disponÃ­vel)
				if (!lastOutputPlaceholderEl) {
					const placeholderTime = lastOutputStartAt || Date.now();
					lastOutputPlaceholderEl = addTranscript(OTHER, '...', placeholderTime);
					if (lastOutputPlaceholderEl && lastOutputPlaceholderEl.dataset) {
						lastOutputPlaceholderEl.dataset.startAt = placeholderTime;
						// marca um stop provisÃ³rio para o UI mostrar intervalo dinÃ¢mico
						lastOutputPlaceholderEl.dataset.stopAt = Date.now();
					}
				} else if (lastOutputPlaceholderEl && lastOutputPlaceholderEl.dataset) {
					// atualiza stop provisÃ³rio a cada parcial
					lastOutputPlaceholderEl.dataset.stopAt = Date.now();
				}

				// solicita ao config-manager atualizaÃ§Ã£o parcial do placeholder (inclui mÃ©tricas provisÃ³rias)
				emitUIChange('onPlaceholderUpdate', {
					speaker: OTHER,
					text: outputPartialText,
					timeStr: new Date(lastOutputStartAt || Date.now()).toLocaleTimeString(),
					startStr: new Date(lastOutputStartAt || Date.now()).toLocaleTimeString(),
					stopStr: new Date().toLocaleTimeString(),
					recordingDuration: Date.now() - (lastOutputStartAt || Date.now()),
					latency: 0,
					total: Date.now() - (lastOutputStartAt || Date.now()),
					provisional: true,
				});

				// atualiza currentQuestion para refletir texto parcial
				if (
					!currentQuestion.text ||
					normalizeForCompare(currentQuestion.text) !== normalizeForCompare(outputPartialText)
				) {
					currentQuestion.text = outputPartialText;
					currentQuestion.lastUpdate = Date.now();
					currentQuestion.lastUpdateTime = Date.now();
					currentQuestion.finalized = false;
					selectedQuestionId = CURRENT_QUESTION_ID;
					renderCurrentQuestion();
				}
			} catch (err) {
				console.warn('âš ï¸ falha ao atualizar UI com transcriÃ§Ã£o parcial:', err);
			}

			// verifica se a pergunta estÃ¡ "pronta" (heurÃ­stica)
			if (isQuestionReady(outputPartialText)) {
				console.log('â“ Pergunta detectada (parcial):', outputPartialText);

				// limpa texto parcial acumulado
				const newText = outputPartialText.trim();

				// verifica se o novo texto Ã© igual ao texto atual da pergunta, se sim, ignora
				if (newText === currentQuestion.text) {
					// ğŸŸ¡ No modo entrevista, se a pergunta ainda NÃƒO foi fechada,
					// permitimos seguir para fechamento e chamada do GPT
					if (!currentQuestion.finalized) {
						console.log('ğŸŸ¡ Pergunta repetida, mas vÃ¡lida no modo entrevista â€” permitindo fechamento');
					} else {
						console.log('ğŸ”• Ignorando nova transcriÃ§Ã£o igual Ã  currentQuestion');
						return;
					}
				}

				// se currentQuestion ainda nÃ£o tinha texto, marca como um novo turno
				if (!currentQuestion.text) {
					currentQuestion.createdAt = Date.now();
					interviewTurnId++; // novo turno detectado
					console.log('ğŸ†• Novo turno iniciado:', interviewTurnId);
				}

				// atualiza a pergunta atual com o novo texto parcial
				currentQuestion.text = newText;
				// atualiza timestamp de Ãºltima modificaÃ§Ã£o
				currentQuestion.lastUpdate = Date.now();
				currentQuestion.lastUpdateTime = Date.now();
				// marca como nÃ£o finalizada
				currentQuestion.finalized = false;

				// atualiza UI
				selectedQuestionId = CURRENT_QUESTION_ID;
				renderCurrentQuestion();

				console.log('ğŸ§  currentQuestion (parcial):', currentQuestion.text);
				console.log('ğŸ¯ interviewTurnId:', interviewTurnId);
				console.log('ğŸ¤– gptAnsweredTurnId:', gptAnsweredTurnId);

				// reseta o timer de auto fechamento
				if (autoCloseQuestionTimer) {
					clearTimeout(autoCloseQuestionTimer);
				}

				// â±ï¸ agenda timer para auto fechamento da pergunta apÃ³s perÃ­odo ocioso
				autoCloseQuestionTimer = setTimeout(() => {
					console.log('â±ï¸ Auto close question disparado (timeout)');

					if (
						ModeController.isInterviewMode() &&
						currentQuestion.text &&
						!currentQuestion.finalized &&
						gptAnsweredTurnId !== interviewTurnId
					) {
						// fecha a pergunta atual automaticamente
						closeCurrentQuestion();
					}
				}, QUESTION_IDLE_TIMEOUT);

				console.log('â²ï¸ Timer de auto-fechamento agendado para', QUESTION_IDLE_TIMEOUT, 'ms');
			} else {
				console.log('â³ Aguardando mais texto para formar pergunta completa');
			}
		} catch (err) {
			console.error('âŒ Erro na transcriÃ§Ã£o parcial (OUTPUT):', err);
		}
	}, 300); // Janela de 300ms para mÃ¡xima responsividade - envia ~5-8 chunks a cada 3s (rate-limit)

	debugLogRenderer('Fim da funÃ§Ã£o: "transcribeOutputPartial"');
}

/* ===============================
   MODO NORMAL - TRANSCRIÃ‡ÃƒO
=============================== */

async function transcribeInput() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "transcribeInput"');
	if (!inputChunks.length) return;

	const blob = new Blob(inputChunks, { type: 'audio/webm' });
	console.log('ğŸ” transcrever entrada - blob.size:', blob.size); // diagnÃ³stico

	// ignora ruÃ­do / respiraÃ§Ã£o
	const minSize = ModeController.isInterviewMode() ? MIN_INPUT_AUDIO_SIZE_INTERVIEW : MIN_INPUT_AUDIO_SIZE;

	if (blob.size < minSize) return;

	inputChunks = [];

	// medir tempo de conversÃ£o blob -> buffer
	const tBlobToBuffer = Date.now();
	const buffer = Buffer.from(await blob.arrayBuffer());
	console.log('timing: bufferConv', Date.now() - tBlobToBuffer, 'ms, size', buffer.length);

	// medir tempo IPC + STT (roundtrip)
	const tSend = Date.now();
	const text = (await transcribeAudio(blob))?.trim();
	console.log('timing: ipc_stt_roundtrip', Date.now() - tSend, 'ms');
	if (!text || isGarbageSentence(text)) return;

	// Se existia um placeholder (timestamp do stop), calcula mÃ©tricas e emite evento para atualizar
	if (lastInputPlaceholderEl && lastInputPlaceholderEl.dataset) {
		// Extrai timestamps do dataset (sempre como nÃºmeros, nunca null)
		const stop = lastInputPlaceholderEl.dataset.stopAt
			? Number(lastInputPlaceholderEl.dataset.stopAt)
			: lastInputStopAt;

		// Para startAt, SEMPRE preferir dataset (mesmo que seja 0), nunca deixar undefined
		const start =
			lastInputPlaceholderEl.dataset.startAt !== undefined
				? Number(lastInputPlaceholderEl.dataset.startAt)
				: lastInputStartAt !== null && lastInputStartAt !== undefined
				? lastInputStartAt
				: stop;

		const now = Date.now();
		const recordingDuration = stop - start;
		const latency = now - stop;
		const total = now - start;
		const startStr = new Date(start).toLocaleTimeString();
		const stopStr = new Date(stop).toLocaleTimeString();
		const displayStr = new Date(now).toLocaleTimeString();

		// Log detalhado de timing
		console.log('â±ï¸ TIMING COMPLETO:');
		console.log(`  âœ… InÃ­cio: ${startStr}`);
		console.log(`  â¹ï¸ Parada: ${stopStr}`);
		console.log(`  ğŸ“º ExibiÃ§Ã£o: ${displayStr}`);
		console.log(`  ğŸ“Š DuraÃ§Ã£o gravaÃ§Ã£o: ${recordingDuration}ms | LatÃªncia: ${latency}ms | Total: ${total}ms`);

		// Emite para config-manager atualizar o placeholder com texto final e mÃ©tricas
		emitUIChange('onPlaceholderFulfill', {
			speaker: YOU,
			text,
			stopStr,
			startStr,
			recordingDuration,
			latency,
			total,
		});

		lastInputPlaceholderEl = null;
		lastInputStopAt = null;
		console.log('ğŸ—‘ï¸ Resetando timestamps: lastInputStartAt = null, lastInputStopAt = null');
		lastInputStartAt = null;
	} else {
		addTranscript(YOU, text);
	}

	handleSpeech(YOU, text);

	debugLogRenderer('Fim da funÃ§Ã£o: "transcribeInput"');
}

async function transcribeOutput() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "transcribeOutput"');

	// Desabilitado temporariamente (teste)
	if (DESABILITADO_TEMPORARIAMENTE) {
		debugLogRenderer('Fim da funÃ§Ã£o: "transcribeOutput" ğŸ”’ DESABILITADO TEMPORARIAMENTE');
		return;
	}

	// Se nÃ£o houver chunks de saÃ­da, retorna
	if (!outputChunks.length) {
		console.log('âš ï¸ transcribeOutput: nenhum chunk de saÃ­da disponÃ­vel');

		debugLogRenderer('Fim da funÃ§Ã£o: "transcribeOutput"');
		return;
	}

	// Cria um blob a partir dos chunks de saÃ­da
	const blob = new Blob(outputChunks, { type: 'audio/webm' });
	console.log('ğŸµ transcribeOutput: blob.size =', blob.size, 'bytes | chunks =', outputChunks.length);

	// Valida tamanho mÃ­nimo dependendo do modo (evita ruÃ­do / respiraÃ§Ã£o)
	const minSize = ModeController.isInterviewMode() ? MIN_OUTPUT_AUDIO_SIZE_INTERVIEW : MIN_OUTPUT_AUDIO_SIZE;
	if (blob.size < minSize) {
		console.log('âš ï¸ transcribeOutput: Blob muito pequeno (', blob.size, '/', minSize, ') - ignorando');

		debugLogRenderer('Fim da funÃ§Ã£o: "transcribeOutput"');
		return;
	}

	// Limpa o array de chunks de saÃ­da
	outputChunks = [];

	try {
		// Envia para transcriÃ§Ã£o o blob de saÃ­da
		const text = await transcribeAudio(blob);
		console.log('ğŸ“ transcribeOutput: TranscriÃ§Ã£o recebida: ', text);

		// Ignora transcriÃ§Ã£o vazia
		if (!text || text.trim().length === 0) {
			console.log('âš ï¸ transcribeOutput: TranscriÃ§Ã£o vazia - ignorando');
			return;
		}

		// Ignora sentenÃ§as garbage
		if (isGarbageSentence(text)) {
			console.log('ğŸ—‘ï¸ transcribeOutput: SentenÃ§a descartada (garbage):', text);
			return;
		}

		// Se existia um placeholder (timestamp do stop), atualiza esse placeholder com o texto final e latÃªncia
		if (lastOutputPlaceholderEl && lastOutputPlaceholderEl.dataset) {
			console.log('ğŸ”„ Atualizando placeholder com transcriÃ§Ã£o final...');

			// ğŸ”¥ USAR VARIÃVEIS PENDENTES (imunes a race condition)
			// Essas variÃ¡veis foram capturadas em onstop() e nÃ£o foram sobrescritas por updateOutputVolume()
			const stop = pendingOutputStopAt || lastOutputStopAt;
			const start = pendingOutputStartAt || lastOutputStartAt || stop;

			// Debug: verificar se pending* foi usada
			console.log(
				'ğŸ”¥ DEBUG transcribeOutput: pendingOutputStopAt=' +
					pendingOutputStopAt +
					', pendingOutputStartAt=' +
					pendingOutputStartAt,
			);

			// calcula mÃ©tricas
			const now = Date.now();
			const recordingDuration = stop - start;
			const latency = now - stop;
			const total = now - start;
			const startStr = new Date(start).toLocaleTimeString();
			const stopStr = new Date(stop).toLocaleTimeString();
			const displayStr = new Date(now).toLocaleTimeString();

			// Log detalhado de timing
			console.log('â±ï¸ TIMING COMPLETO (Output):');
			console.log(`  âœ… InÃ­cio: ${startStr}`);
			console.log(`  â¹ï¸ Parada: ${stopStr}`);
			console.log(`  ğŸ“º ExibiÃ§Ã£o: ${displayStr}`);
			console.log(`  ğŸ“Š DuraÃ§Ã£o gravaÃ§Ã£o: ${recordingDuration}ms | LatÃªncia: ${latency}ms | Total: ${total}ms`);

			// Emite atualizaÃ§Ã£o de UI ao placeholder com texto final e mÃ©tricas
			// ğŸ”¥ PASSA O ID DO PLACEHOLDER para que config-manager atualize o elemento CORRETO
			emitUIChange('onPlaceholderFulfill', {
				speaker: OTHER,
				text,
				stopStr,
				startStr,
				recordingDuration,
				latency,
				total,
				placeholderId: lastOutputPlaceholderId, // ğŸ”¥ ESSENCIAL para encontrar o placeholder correto
			});

			// reseta variÃ¡veis de placeholder
			lastOutputPlaceholderEl = null;
			// NÃƒO resetar lastOutputStopAt e lastOutputStartAt aqui!
			// Eles serÃ£o preservados para timing correto da prÃ³xima frase
			// SerÃ£o resetados apenas quando uma NOVA frase inicia em updateOutputVolume()
			console.log(
				'ğŸ§¹ RESET #1: lastOutputPlaceholderEl resetado | lastOutputStartAt/StopAt PRESERVADOS para prÃ³xima frase',
			);

			// processa a fala transcrita (consolidaÃ§Ã£o de perguntas)
			// Usa Date.now() para pegar o tempo exato que chegou no renderer
			handleSpeech(OTHER, text);
		} else {
			// Sem placeholder - cria placeholder e emite fulfill para garantir mÃ©tricas
			console.log('â• Nenhum placeholder existente - criando e preenchendo com mÃ©tricas');
			// ğŸ”¥ USAR VARIÃVEIS PENDENTES (imunes a race condition)
			const stop = pendingOutputStopAt || lastOutputStopAt || Date.now();
			const start = pendingOutputStartAt || lastOutputStartAt || stop;

			// Debug: verificar se pending* foi usada
			console.log(
				'ğŸ”¥ DEBUG transcribeOutput: pendingOutputStopAt=' +
					pendingOutputStopAt +
					', pendingOutputStartAt=' +
					pendingOutputStartAt,
			);

			const now = Date.now();
			const recordingDuration = stop - start;
			const latency = now - stop;
			const total = now - start;
			const startStr = new Date(start).toLocaleTimeString();
			const stopStr = new Date(stop).toLocaleTimeString();
			const displayStr = new Date(now).toLocaleTimeString();

			// Log detalhado de timing
			console.log('â±ï¸ TIMING COMPLETO (Output):');
			console.log(`  âœ… InÃ­cio: ${startStr}`);
			console.log(`  â¹ï¸ Parada: ${stopStr}`);
			console.log(`  ğŸ“º ExibiÃ§Ã£o: ${displayStr}`);
			console.log(`  ğŸ“Š DuraÃ§Ã£o gravaÃ§Ã£o: ${recordingDuration}ms | LatÃªncia: ${latency}ms | Total: ${total}ms`);

			// cria um placeholder visÃ­vel antes de preencher (garante consistÃªncia com fluxo parcial)
			const elIdForFallback = 'placeholder-' + start + '-' + Math.random();
			const placeholderEl = addTranscript(OTHER, '...', start, elIdForFallback);

			if (placeholderEl && placeholderEl.dataset) {
				placeholderEl.dataset.startAt = start;
				placeholderEl.dataset.stopAt = stop;
			}

			// Emite atualizaÃ§Ã£o final para preencher o placeholder com texto e mÃ©tricas
			// ğŸ”¥ PASSA O ID DO PLACEHOLDER para que config-manager atualize o elemento CORRETO
			emitUIChange('onPlaceholderFulfill', {
				speaker: OTHER,
				text,
				stopStr,
				startStr,
				recordingDuration,
				latency,
				total,
				placeholderId: elIdForFallback, // ğŸ”¥ ESSENCIAL para encontrar o placeholder correto
			});

			// reseta variÃ¡veis de placeholder
			console.log(
				'ğŸ§¹ RESET #2: lastOutputPlaceholderEl resetado | lastOutputStartAt/StopAt PRESERVADOS para prÃ³xima frase',
			);
			lastOutputPlaceholderEl = null;
			// NÃƒO resetar lastOutputStopAt e lastOutputStartAt aqui!
			// Eles serÃ£o preservados para timing correto da prÃ³xima frase

			// processa a fala transcrita (consolidaÃ§Ã£o de perguntas)
			// Usa Date.now() para pegar o tempo exato que chegou no renderer
			handleSpeech(OTHER, text);
		}

		// ğŸ”¥ Limpar variÃ¡veis pendentes apÃ³s transcriÃ§Ã£o completa
		// Elas jÃ¡ foram usadas para calcular mÃ©tricas, agora podem ser limpas
		console.log('ğŸ§¹ RESET #3: Limpando pendingOutputStartAt e pendingOutputStopAt');
		pendingOutputStartAt = null;
		pendingOutputStopAt = null;

		// MODO ENTREVISTA: Se a transcriÃ§Ã£o final indicar claramente uma pergunta, fechar e enviar ao GPT imediatamente
		// if (ModeController.isInterviewMode() && isQuestionReady(text)) {
		// 	console.log('ğŸ”” transcribeOutput: TranscriÃ§Ã£o final forma pergunta vÃ¡lida');
		// 	console.log('   â†’ Fechando pergunta e chamando GPT agora');

		// 	// limpa estado parcial e cancela o temporizador automÃ¡tico para evitar duplicatas
		// 	outputPartialText = '';

		// 	// cancela o temporizador automÃ¡tico para evitar duplicatas
		// 	if (autoCloseQuestionTimer) {
		// 		clearTimeout(autoCloseQuestionTimer);
		// 		autoCloseQuestionTimer = null;
		// 		console.log('   â†’ Timer automÃ¡tico cancelado');
		// 	}

		// 	// fecha a pergunta atual imediatamente
		// 	closeCurrentQuestion();
		// }
	} catch (err) {
		console.warn('âš ï¸ erro na transcriÃ§Ã£o (OUTPUT)', err);
	}

	debugLogRenderer('Fim da funÃ§Ã£o: "transcribeOutput"');
}

/* ===============================
   CONSOLIDAÃ‡ÃƒO DE PERGUNTAS
=============================== */

function handleSpeech(author, text) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "handleSpeech"');
	const cleaned = text.replace(/ÃŠ+|hum|ahn/gi, '').trim();
	console.log('ğŸ”Š handleSpeech', { author, raw: text, cleaned });
	if (cleaned.length < 3) return;

	// Usa o tempo exato que chegou no renderer (Date.now)
	const now = Date.now();

	if (author === OTHER) {
		// ğŸ‘‰ Se jÃ¡ existe uma pergunta finalizada,
		//    significa que uma NOVA pergunta comeÃ§ou
		if (currentQuestion.finalized) {
			console.log(
				'â„¹ï¸ QuestÃ£o anterior finalizada â€” promovendo para a histÃ³ria e continuando a processar o novo discurso.',
			);
			promoteCurrentToHistory(currentQuestion.text);
		}

		// ğŸ§  Detecta inÃ­cio de NOVA pergunta e fecha a anterior
		// âš ï¸ IMPORTANTE: Consolida ANTES de fechar, para evitar perder falas intermidiÃ¡rias
		if (
			currentQuestion.text &&
			looksLikeQuestion(cleaned) &&
			now - currentQuestion.lastUpdate > 500 &&
			!currentQuestion.finalized
		) {
			// ğŸ”€ CONSOLIDAÃ‡ÃƒO: Adiciona a fala atual antes de fechar a pergunta anterior
			// Isso garante que "explique o que Ã©... Y" seja parte da pergunta "Vou comeÃ§ar... X"
			console.log('ğŸ”€ [IMPORTANTE] Consolidando nova fala com pergunta atual antes de fechar:', {
				current: currentQuestion.text,
				new: cleaned,
				currentLength: currentQuestion.text.length,
				newLength: cleaned.length,
			});
			const beforeConsolidate = currentQuestion.text;
			currentQuestion.text += (currentQuestion.text ? ' ' : '') + cleaned;
			currentQuestion.lastUpdateTime = now;
			currentQuestion.lastUpdate = now;
			console.log('ğŸ”€ [IMPORTANTE] ApÃ³s consolidaÃ§Ã£o:', {
				before: beforeConsolidate,
				after: currentQuestion.text,
				finalLength: currentQuestion.text.length,
			});

			closeCurrentQuestion();

			// ğŸ›‘ Retorna para evitar processar a mesma fala novamente abaixo
			renderCurrentQuestion();
			debugLogRenderer('Fim da funÃ§Ã£o: "handleSpeech"');
			return;
		}

		if (!currentQuestion.text) {
			// evita criar novo turno se a transcriÃ§Ã£o final for igual Ã  Ãºltima pergunta jÃ¡ enviada
			if (lastSentQuestionText && cleaned.trim() === lastSentQuestionText) {
				console.log('ğŸ”• transcriÃ§Ã£o igual Ã  Ãºltima pergunta enviada â€” ignorando novo turno');
				return;
			}
			currentQuestion.createdAt = Date.now();
			currentQuestion.lastUpdateTime = Date.now();
			interviewTurnId++; // ğŸ”¥ novo turno
		}

		// evita duplicaÃ§Ã£o quando a mesma frase parcial/final chega novamente
		if (currentQuestion.text && normalizeForCompare(currentQuestion.text) === normalizeForCompare(cleaned)) {
			console.log('ğŸ” speech igual ao currentQuestion â€” ignorando concatenaÃ§Ã£o');
		} else {
			currentQuestion.text += (currentQuestion.text ? ' ' : '') + cleaned;
			currentQuestion.lastUpdateTime = now;
		}
		currentQuestion.lastUpdate = now;

		// ğŸŸ¦ CURRENT vira seleÃ§Ã£o padrÃ£o ao receber fala
		if (!selectedQuestionId) {
			selectedQuestionId = CURRENT_QUESTION_ID;
			clearAllSelections();
		}

		renderCurrentQuestion();
	}

	debugLogRenderer('Fim da funÃ§Ã£o: "handleSpeech"');
}

/* ===============================
   FECHAMENTO DE PERGUNTAS
=============================== */

function closeCurrentQuestion() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "closeCurrentQuestion"');

	// ğŸ”’ GUARDA ABSOLUTA:
	// Se a pergunta jÃ¡ foi finalizada, NÃƒO faÃ§a nada.
	if (currentQuestion.finalized) {
		console.log('â›” closeCurrentQuestion ignorado â€” pergunta jÃ¡ finalizada');
		return;
	}

	// Garante que lastUpdateTime seja definido quando se tenta fechar
	if (!currentQuestion.lastUpdateTime && currentQuestion.text) {
		currentQuestion.lastUpdateTime = Date.now();
	}

	console.log('ğŸšª closeCurrentQuestion called', {
		interviewTurnId,
		gptAnsweredTurnId,
		currentQuestionText: currentQuestion.text,
	});

	// trata perguntas incompletas
	if (isIncompleteQuestion(currentQuestion.text)) {
		console.log('âš ï¸ pergunta incompleta detectada â€” promovendo ao histÃ³rico como incompleta:', currentQuestion.text);

		const newId = String(questionsHistory.length + 1);
		questionsHistory.push({
			id: newId,
			text: currentQuestion.text,
			createdAt: currentQuestion.createdAt || Date.now(),
			lastUpdateTime: currentQuestion.lastUpdateTime || currentQuestion.createdAt || Date.now(),
			incomplete: true,
		});

		selectedQuestionId = newId;

		currentQuestion.text = '';
		currentQuestion.lastUpdateTime = null;
		currentQuestion.createdAt = null;
		currentQuestion.finalized = false;

		renderQuestionsHistory();
		renderCurrentQuestion();
		return;
	}

	if (!looksLikeQuestion(currentQuestion.text)) {
		// âš ï¸ No modo entrevista, NÃƒO abortar o fechamento
		if (ModeController.isInterviewMode()) {
			console.log('âš ï¸ looksLikeQuestion=false, mas modo entrevista ativo â€” forÃ§ando fechamento');

			currentQuestion.text = finalizeQuestion(currentQuestion.text);
			currentQuestion.lastUpdateTime = Date.now();
			currentQuestion.finalized = true;

			// garante seleÃ§Ã£o lÃ³gica
			selectedQuestionId = CURRENT_QUESTION_ID;

			// chama GPT automaticamente se ainda nÃ£o respondeu este turno
			if (gptRequestedTurnId !== interviewTurnId && gptAnsweredTurnId !== interviewTurnId) {
				console.log('â¡ï¸ closeCurrentQuestion (fallback) chamou askGpt', {
					interviewTurnId,
					gptRequestedTurnId,
					gptAnsweredTurnId,
				});

				//console.error('closeCurrentQuestion: askGpt() 2281; ğŸ”’ COMENTADA atÃ© transcriÃ§Ã£o em tempo real funcionar');
				askGpt(); // ğŸ”’ COMENTADA atÃ© transcriÃ§Ã£o em tempo real funcionar
			}

			return;
		}

		// modo normal mantÃ©m comportamento atual
		currentQuestion.text = '';
		currentQuestion.lastUpdateTime = null;
		currentQuestion.createdAt = null;
		currentQuestion.finalized = false;
		renderCurrentQuestion();
		return;
	}

	// âœ… consolida a pergunta
	currentQuestion.text = finalizeQuestion(currentQuestion.text);
	currentQuestion.lastUpdateTime = Date.now();
	currentQuestion.finalized = true;

	// âš ï¸ NUNCA renderizar aqui no modo entrevista
	if (!ModeController.isInterviewMode()) {
		renderCurrentQuestion();
	}

	// ğŸ”¥ COMPORTAMENTO POR MODO
	if (ModeController.isInterviewMode()) {
		if (gptRequestedTurnId !== interviewTurnId && gptAnsweredTurnId !== interviewTurnId) {
			selectedQuestionId = CURRENT_QUESTION_ID;

			console.log('â¡ï¸ closeCurrentQuestion chamou askGpt (vou enviar para o GPT)', {
				interviewTurnId,
				gptRequestedTurnId,
				gptAnsweredTurnId,
			});

			//console.error('closeCurrentQuestion: askGpt() 2318; ğŸ”’ COMENTADA atÃ© transcriÃ§Ã£o em tempo real funcionar');
			askGpt(); // ğŸ”’ COMENTADA atÃ© transcriÃ§Ã£o em tempo real funcionar
		}
	} else {
		console.log('ğŸ”µ modo NORMAL â€” promovendo CURRENT para histÃ³rico sem chamar GPT');

		promoteCurrentToHistory(currentQuestion.text);

		currentQuestion.text = '';
		currentQuestion.lastUpdateTime = null;
		currentQuestion.createdAt = null;
		currentQuestion.finalized = false;

		renderCurrentQuestion();
	}

	debugLogRenderer('Fim da funÃ§Ã£o: "closeCurrentQuestion"');
}

function closeCurrentQuestionForced() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "closeCurrentQuestionForced"');

	// log temporario para testar a aplicaÃ§Ã£o sÃ³ remover depois
	console.log('ğŸšª Fechando pergunta:', currentQuestion.text);

	resetInterviewTurnState();

	if (!currentQuestion.text) return;

	questionsHistory.push({
		id: crypto.randomUUID(),
		text: finalizeQuestion(currentQuestion.text),
		createdAt: currentQuestion.createdAt || Date.now(),
	});

	currentQuestion.text = '';
	selectedQuestionId = null; // ğŸ‘ˆ libera seleÃ§Ã£o
	renderQuestionsHistory();
	renderCurrentQuestion();

	debugLogRenderer('Fim da funÃ§Ã£o: "closeCurrentQuestionForced"');
}

/* ===============================
   VALIDAÃ‡ÃƒO DE API KEY
=============================== */

// ğŸ”¥ Verifica o Status da API
async function checkApiKeyStatus() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "checkApiKeyStatus"');
	try {
		const status = await ipcRenderer.invoke('GET_OPENAI_API_STATUS');
		console.log('ğŸ”‘ Status da API key:', status);

		debugLogRenderer('Fim da funÃ§Ã£o: "checkApiKeyStatus"');
		return status;
	} catch (error) {
		console.warn('âš ï¸ NÃ£o foi possÃ­vel verificar status da API:', error);
		return { initialized: false, hasKey: false };
	}
}

/* ===============================
   GPT
=============================== */
async function askGpt() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "askGpt"');

	// Desabilitado temporariamente (teste)
	if (DESABILITADO_TEMPORARIAMENTE) {
		debugLogRenderer('Fim da funÃ§Ã£o: "askGpt" ğŸ”’ DESABILITADO TEMPORARIAMENTE');
		return;
	}

	const text = getSelectedQuestionText();

	if (!text || text.trim().length < 5) {
		updateStatusMessage('âš ï¸ Pergunta vazia ou incompleta');
		return;
	}

	const isCurrent = selectedQuestionId === CURRENT_QUESTION_ID;
	const normalizedText = normalizeForCompare(text);

	// Evita reenvio da mesma pergunta atual ao GPT (dedupe)
	if (isCurrent && normalizedText && lastAskedQuestionNormalized === normalizedText) {
		updateStatusMessage('â›” Pergunta jÃ¡ enviada');
		console.log('â›” askGpt: mesma pergunta jÃ¡ enviada, pulando');
		return;
	}
	const questionId = isCurrent ? CURRENT_QUESTION_ID : selectedQuestionId;

	// ğŸ›¡ï¸ MODO ENTREVISTA â€” bloqueia duplicaÃ§Ã£o APENAS para histÃ³rico
	if (ModeController.isInterviewMode() && !isCurrent) {
		const existingAnswer = findAnswerByQuestionId(questionId);
		if (existingAnswer) {
			emitUIChange('onAnswerAdd', {
				questionId,
				action: 'showExisting',
			});
			updateStatusMessage('ğŸ“Œ Essa pergunta jÃ¡ foi respondida');
			return;
		}
	}

	// limpa destaque
	emitUIChange('onAnswerAdd', {
		questionId,
		action: 'clearActive',
	});

	// log temporario para testar a aplicaÃ§Ã£o sÃ³ remover depois
	console.log('ğŸ¤– askGpt chamado | questionId:', selectedQuestionId);
	console.log('ğŸ§ª GPT RECEBERIA:', text);

	console.log('ğŸ§¾ askGpt diagnÃ³stico', {
		textLength: text.length,
		selectedQuestionId,
		questionId_variable: questionId, // ğŸ”¥ DEBUG: mostrar a variÃ¡vel questionId
		isInterviewMode: ModeController.isInterviewMode(),
		interviewTurnId,
		gptAnsweredTurnId,
	});

	// marca que este turno teve uma requisiÃ§Ã£o ao GPT (apenas para CURRENT)
	if (isCurrent) {
		gptRequestedTurnId = interviewTurnId;
		gptRequestedQuestionId = CURRENT_QUESTION_ID; // ğŸ”¥ [IMPORTANTE] Rastreia qual pergunta foi solicitada
		lastAskedQuestionNormalized = normalizedText;
		console.log('â„¹ï¸ gptRequestedTurnId definido para turno', gptRequestedTurnId);
		console.log('â„¹ï¸ gptRequestedQuestionId definido para:', gptRequestedQuestionId);
		lastSentQuestionText = text.trim();
		console.log('â„¹ï¸ lastSentQuestionText definido:', lastSentQuestionText);
	}

	// Inicia mediÃ§Ã£o do GPT
	transcriptionMetrics.gptStartTime = Date.now();

	// ï¿½ MODO ENTREVISTA â€” STREAMING
	if (ModeController.isInterviewMode()) {
		const gptStartAt = ENABLE_INTERVIEW_TIMING_DEBUG ? Date.now() : null;
		let streamedText = '';

		console.log('â³ enviando para o GPT via stream...');

		// ğŸ”¥ NÃ£o preparar bloco antes - deixar o primeiro token criar (mais rÃ¡pido!)

		ipcRenderer
			.invoke('ask-gpt-stream', [
				{ role: 'system', content: SYSTEM_PROMPT },
				{ role: 'user', content: text },
			])
			.catch(err => {
				console.error('âŒ Erro ao chamar ask-gpt-stream:', err);
				updateStatusMessage('âŒ Erro ao enviar para GPT');
			});

		const onChunk = (_, token) => {
			streamedText += token;

			// ğŸ”¥ DEBUG: Log para rastrear qual questionId estÃ¡ sendo enviado
			if (streamedText.length <= 50) {
				console.log('ğŸ¬ [onChunk] Enviando para onAnswerStreamChunk:', {
					questionId,
					gptRequestedQuestionId,
					token,
					accumLength: streamedText.length,
				});
			}

			emitUIChange('onAnswerStreamChunk', {
				questionId,
				token,
				accum: streamedText,
			});
			console.log('ğŸŸ¢ GPT_STREAM_CHUNK recebido (token parcial)', token);
		};

		const onEnd = () => {
			console.log('âœ… GPT_STREAM_END recebido (stream finalizado)');
			ipcRenderer.removeListener('GPT_STREAM_CHUNK', onChunk);
			ipcRenderer.removeListener('GPT_STREAM_END', onEnd);

			// Finaliza mediÃ§Ãµes
			transcriptionMetrics.gptEndTime = Date.now();
			transcriptionMetrics.totalTime = Date.now() - transcriptionMetrics.audioStartTime;

			// Log mÃ©tricas
			logTranscriptionMetrics();

			let finalText = streamedText;
			if (ENABLE_INTERVIEW_TIMING_DEBUG && gptStartAt) {
				const endAt = Date.now();
				const elapsed = endAt - gptStartAt;

				const startTime = new Date(gptStartAt).toLocaleTimeString();
				const endTime = new Date(endAt).toLocaleTimeString();

				finalText +=
					`\n\nâ±ï¸ GPT iniciou: ${startTime}` + `\nâ±ï¸ GPT finalizou: ${endTime}` + `\nâ±ï¸ Resposta em ${elapsed}ms`;
			}

			// garante que o turno foi realmente fechado
			const wasRequestedForThisTurn = gptRequestedTurnId === interviewTurnId;
			const requestedQuestionId = gptRequestedQuestionId; // ğŸ”¥ Qual pergunta foi REALMENTE solicitada

			gptAnsweredTurnId = interviewTurnId;
			gptRequestedTurnId = null;
			gptRequestedQuestionId = null; // ğŸ”¥ Limpa apÃ³s usar

			// ğŸ”’ RENDERIZAR A RESPOSTA COM O ID CORRETO
			if (requestedQuestionId) {
				// const finalHtml = marked.parse(finalText); // Resposta jÃ¡ renderizada via streaming

				console.log('âœ… GPT_STREAM_END: Renderizando resposta para pergunta solicitada:', {
					requestedQuestionId,
					wasRequestedForThisTurn,
				});

				// Se a pergunta solicitada foi CURRENT, promover para history ANTES de renderizar
				if (requestedQuestionId === CURRENT_QUESTION_ID && currentQuestion.text) {
					console.log('ğŸ”„ GPT_STREAM_END: Promovendo CURRENT para history antes de renderizar resposta');
					promoteCurrentToHistory(currentQuestion.text);

					// Pega a pergunta recÃ©m-promovida
					const promotedQuestion = questionsHistory[questionsHistory.length - 1];
					if (promotedQuestion) {
						// Renderiza com o ID da pergunta promovida
						// renderGptAnswer(promotedQuestion.id, finalHtml); // Resposta jÃ¡ renderizada via streaming
						promotedQuestion.answered = true;
						answeredQuestions.add(promotedQuestion.id);
						renderQuestionsHistory();
						console.log('âœ… Resposta renderizada para pergunta promovida:', promotedQuestion.id);
					} else {
						console.warn('âš ï¸ Pergunta promovida nÃ£o encontrada');
						// renderGptAnswer(requestedQuestionId, finalHtml); // Resposta jÃ¡ renderizada via streaming
					}
				} else {
					// Para perguntas do histÃ³rico, renderiza com o ID recebido
					// renderGptAnswer(requestedQuestionId, finalHtml); // Resposta jÃ¡ renderizada via streaming
					answeredQuestions.add(requestedQuestionId);

					// Se for do histÃ³rico, atualiza o flag tambÃ©m
					if (requestedQuestionId !== CURRENT_QUESTION_ID) {
						try {
							const q = questionsHistory.find(x => x.id === requestedQuestionId);
							if (q) {
								q.answered = true;
								renderQuestionsHistory();
							}
						} catch (err) {
							console.warn('âš ï¸ falha ao marcar pergunta como respondida:', err);
						}
					}
				}

				resetInterviewTurnState();
			} else {
				// ğŸ”¥ Nenhuma pergunta foi rastreada como solicitada
				console.warn('âš ï¸ GPT_STREAM_END mas nenhuma pergunta solicitada foi encontrada');
				resetInterviewTurnState();
			}

			// ğŸ”¥ Notificar config-manager que stream terminou (para limpar info de streaming)
			globalThis.RendererAPI?.emitUIChange?.('onAnswerStreamEnd', {});
		};

		ipcRenderer.on('GPT_STREAM_CHUNK', onChunk);
		ipcRenderer.once('GPT_STREAM_END', onEnd);
		return;
	}

	// ğŸ”µ MODO NORMAL â€” BATCH
	console.log('â³ enviando para o GPT (batch)...');
	const res = await ipcRenderer.invoke('ask-gpt', [
		{ role: 'system', content: SYSTEM_PROMPT },
		{ role: 'user', content: text },
	]);

	console.log('âœ… resposta do GPT recebida (batch)');

	// Finaliza mediÃ§Ãµes
	transcriptionMetrics.gptEndTime = Date.now();
	transcriptionMetrics.totalTime = Date.now() - transcriptionMetrics.audioStartTime;

	// Log mÃ©tricas
	logTranscriptionMetrics();

	// ğŸ”¥ COMENTADO: renderGptAnswer(questionId, res);
	// Apenas streaming serÃ¡ exibido

	const wasRequestedForThisTurn = gptRequestedTurnId === interviewTurnId;

	console.log(
		'â„¹ï¸ gptRequestedTurnId antes do batch:',
		gptRequestedTurnId,
		'wasRequestedForThisTurn:',
		wasRequestedForThisTurn,
	);

	// ğŸ”’ FECHAMENTO ATÃ”MICO DO CICLO
	if (isCurrent && wasRequestedForThisTurn) {
		promoteCurrentToHistory(text);
		// apÃ³s promover para o histÃ³rico, a pergunta jÃ¡ estÃ¡ no histÃ³rico e resposta vinculada
		try {
			// Encontra a Ãºltima pergunta adicionada (que acabamos de promover)
			const q = questionsHistory[questionsHistory.length - 1];
			if (q) {
				q.answered = true;
				renderQuestionsHistory();
			}
		} catch (err) {
			console.warn('âš ï¸ falha ao marcar pergunta como respondida (batch):', err);
		}
	}

	// marca que o GPT respondeu esse turno (batch)
	gptAnsweredTurnId = interviewTurnId;
	gptRequestedTurnId = null;

	debugLogRenderer('Fim da funÃ§Ã£o: "askGpt"');
}

/* ===============================
   UI (RENDER / SELEÃ‡ÃƒO / SCROLL)
=============================== */

function addTranscript(author, text, time, elementId = null) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "addTranscript"');
	let timeStr;
	if (time) {
		if (typeof time === 'number') timeStr = new Date(time).toLocaleTimeString();
		else if (time instanceof Date) timeStr = time.toLocaleTimeString();
		else timeStr = String(time);
	} else {
		timeStr = new Date().toLocaleTimeString();
	}

	// ğŸ”¥ Apenas EMITE o evento com os dados
	// config-manager.js Ã© responsÃ¡vel por adicionar ao DOM
	const transcriptData = {
		author,
		text,
		timeStr,
		elementId: 'conversation',
		placeholderId: elementId, // ğŸ”¥ PASSAR ID PARA SER ATRIBUÃDO AO ELEMENTO REAL
	};

	emitUIChange('onTranscriptAdd', transcriptData);

	// Retorna um objeto proxy que simula um elemento DOM para compatibilidade
	// Usado quando a transcriÃ§Ã£o Ã© um placeholder que serÃ¡ atualizado depois
	const placeholderProxy = {
		dataset: {
			startAt: typeof time === 'number' ? time : Date.now(),
			stopAt: null,
		},
		// Permite que cÃ³digo posterior trate como elemento DOM
		classList: {
			add: () => {},
			remove: () => {},
			contains: () => false,
			toggle: () => false,
		},
	};

	debugLogRenderer('Fim da funÃ§Ã£o: "addTranscript"');
	return placeholderProxy;
}

function renderCurrentQuestion() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "renderCurrentQuestion"');

	// Desabilitado temporariamente (teste)
	if (DESABILITADO_TEMPORARIAMENTE) {
		debugLogRenderer('Fim da funÃ§Ã£o: "renderCurrentQuestion" ğŸ”’ DESABILITADO TEMPORARIAMENTE');
		return;
	}

	if (!currentQuestion.text) {
		emitUIChange('onCurrentQuestionUpdate', { text: '', isSelected: false });
		return;
	}

	let label = currentQuestion.text;

	if (ENABLE_INTERVIEW_TIMING_DEBUG && currentQuestion.lastUpdateTime) {
		const time = new Date(currentQuestion.lastUpdateTime).toLocaleTimeString();
		label = `â±ï¸ ${time} â€” ${label}`;
	}

	// ğŸ”¥ Apenas EMITE dados - config-manager aplica ao DOM
	const questionData = {
		text: label,
		isSelected: selectedQuestionId === CURRENT_QUESTION_ID,
		rawText: currentQuestion.text,
		createdAt: currentQuestion.createdAt,
		lastUpdateTime: currentQuestion.lastUpdateTime,
	};

	console.log(`ğŸ“¤ renderCurrentQuestion: emitindo onCurrentQuestionUpdate`, {
		label,
		isSelected: selectedQuestionId === CURRENT_QUESTION_ID,
	});
	emitUIChange('onCurrentQuestionUpdate', questionData);

	debugLogRenderer('Fim da funÃ§Ã£o: "renderCurrentQuestion"');
}

function renderQuestionsHistory() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "renderQuestionsHistory"');

	// Desabilitado temporariamente (teste)
	if (DESABILITADO_TEMPORARIAMENTE) {
		debugLogRenderer('Fim da funÃ§Ã£o: "renderCurrentQuestion" ğŸ”’ DESABILITADO TEMPORARIAMENTE');
		return;
	}

	// ğŸ”¥ Gera dados estruturados - config-manager renderiza no DOM
	const historyData = [...questionsHistory].reverse().map(q => {
		let label = q.text;
		if (ENABLE_INTERVIEW_TIMING_DEBUG && q.lastUpdateTime) {
			const time = new Date(q.lastUpdateTime).toLocaleTimeString();
			label = `â±ï¸ ${time} â€” ${label}`;
		}

		return {
			id: q.id,
			text: label,
			isIncomplete: q.incomplete,
			isAnswered: q.answered,
			isSelected: q.id === selectedQuestionId,
		};
	});

	emitUIChange('onQuestionsHistoryUpdate', historyData);

	scrollToSelectedQuestion();

	debugLogRenderer('Fim da funÃ§Ã£o: "renderQuestionsHistory"');
}

function clearAllSelections() {
	// Emite evento para o controller limpar as seleÃ§Ãµes visuais
	emitUIChange('onClearAllSelections', {});
}

function scrollToSelectedQuestion() {
	emitUIChange('onScrollToQuestion', {
		questionId: selectedQuestionId,
	});
}

function getSelectedQuestionText() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "getSelectedQuestionText"');
	// 1ï¸âƒ£ Se existe seleÃ§Ã£o explÃ­cita
	if (selectedQuestionId === CURRENT_QUESTION_ID) {
		return currentQuestion.text;
	}

	if (selectedQuestionId) {
		const q = questionsHistory.find(q => q.id === selectedQuestionId);
		if (q?.text) return q.text;
	}

	// 2ï¸âƒ£ Fallback: CURRENT (se tiver texto)
	if (currentQuestion.text && currentQuestion.text.trim().length > 0) {
		return currentQuestion.text;
	}

	debugLogRenderer('Fim da funÃ§Ã£o: "getSelectedQuestionText"');
	return '';
}

/* ğŸ”¥ COMENTADO: renderGptAnswer - RenderizaÃ§Ã£o formatada desabilitada
   Apenas streaming (tokens em tempo real) serÃ¡ exibido

function renderGptAnswer(questionId, markdownText) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "renderGptAnswer"');

	// ğŸ”¥ Renderiza markdown e retorna HTML - config-manager aplica ao DOM
	const short = shortenAnswer(markdownText, 2);
	const html = marked.parse(short);

	// Encontra texto da pergunta no histÃ³rico ou na pergunta atual
	let questionText = '';
	if (questionId === CURRENT_QUESTION_ID) {
		questionText = currentQuestion?.text || '';
	} else {
		const q = questionsHistory.find(x => x.id === questionId);
		questionText = q?.text || '';
	}

	// ğŸ”’ Marca pergunta como respondida na primeira resposta
	if (questionId) {
		answeredQuestions.add(questionId);
		console.log('âœ… Pergunta marcada como respondida:', questionId);
	}

	const answerData = {
		questionText,
		questionId,
		html,
	};

	emitUIChange('onAnswerAdd', answerData);

	// marca a pergunta como respondida no histÃ³rico (se vinculada)
	try {
		if (questionId && questionId !== CURRENT_QUESTION_ID) {
			const q = questionsHistory.find(x => x.id === questionId);
			if (q) {
				q.answered = true;
				renderQuestionsHistory();
			}
		}
	} catch (err) {
		console.warn('âš ï¸ falha ao marcar pergunta como respondida:', err);
	}

	debugLogRenderer('Fim da funÃ§Ã£o: "renderGptAnswer"');
}
*/

// ğŸ”¥ NOVO: Verifica se existe um modelo de IA ativo e retorna o nome do modelo
function hasActiveModel() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "hasActiveModel"');
	if (!window.configManager) {
		console.warn('âš ï¸ ConfigManager nÃ£o inicializado ainda');
		return { active: false, model: null };
	}

	const config = window.configManager.config;
	if (!config || !config.api) {
		console.warn('âš ï¸ Config ou api nÃ£o disponÃ­vel');
		return { active: false, model: null };
	}

	// Verifica se algum modelo estÃ¡ ativo e retorna o nome
	const providers = ['openai', 'google', 'openrouter', 'custom'];
	for (const provider of providers) {
		if (config.api[provider] && config.api[provider].enabled === true) {
			console.log(`âœ… Modelo ativo encontrado: ${provider}`);
			return { active: true, model: provider };
		}
	}

	console.warn('âš ï¸ Nenhum modelo ativo encontrado');

	debugLogRenderer('Fim da funÃ§Ã£o: "hasActiveModel"');
	return { active: false, model: null };
}

async function listenToggleBtn() {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "listenToggleBtn"');

	// ğŸ”¥ VALIDAÃ‡ÃƒO 1: Modelo de IA ativo
	const { active: hasModel, model: activeModel } = hasActiveModel();
	console.log(`ğŸ“Š DEBUG: hasModel = ${hasModel}, activeModel = ${activeModel}`);

	if (!isRunning && !hasModel) {
		const errorMsg = 'Ative um modelo de IA antes de comeÃ§ar a ouvir';
		console.warn(`âš ï¸ ${errorMsg}`);
		console.log('ğŸ“¡ DEBUG: Emitindo onError:', errorMsg);
		emitUIChange('onError', errorMsg);
		return;
	}

	// ğŸ”¥ VALIDAÃ‡ÃƒO 2: Dispositivo de Ã¡udio de SAÃDA (obrigatÃ³rio para ouvir a reuniÃ£o)
	const hasOutputDevice = UIElements.outputSelect?.value;
	console.log(`ğŸ“Š DEBUG: hasOutputDevice = ${hasOutputDevice}`);

	if (!isRunning && !hasOutputDevice) {
		const errorMsg = 'Selecione um dispositivo de Ã¡udio (output) para ouvir a reuniÃ£o';
		console.warn(`âš ï¸ ${errorMsg}`);
		console.log('ğŸ“¡ DEBUG: Emitindo onError:', errorMsg);
		emitUIChange('onError', errorMsg);
		return;
	}

	// Inverte o estado de isRunning
	isRunning = !isRunning;
	const buttonText = isRunning ? 'Parar a Escuta... (Ctrl+d)' : 'ComeÃ§ar a Ouvir... (Ctrl+d)';
	const statusMsg = isRunning ? 'Status: ouvindo...' : 'Status: parado';

	// Emite o evento 'onListenButtonToggle' para atualizar o botÃ£o de escuta
	emitUIChange('onListenButtonToggle', {
		isRunning,
		buttonText,
	});

	// Atualiza o status da escuta na tela
	updateStatusMessage(statusMsg);

	console.log(`ğŸ¤ Listen toggle: ${isRunning ? 'INICIANDO' : 'PARANDO'} (modelo: ${activeModel})`);

	// Inicia ou para a captura de Ã¡udio
	await (isRunning ? startAudio() : stopAudio());

	debugLogRenderer('Fim da funÃ§Ã£o: "listenToggleBtn"');
}

function handleQuestionClick(questionId) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "handleQuestionClick"');
	selectedQuestionId = questionId;
	clearAllSelections();
	renderQuestionsHistory();
	renderCurrentQuestion();

	// âš ï¸ CURRENT nunca bloqueia resposta
	if (questionId !== CURRENT_QUESTION_ID) {
		const existingAnswer = findAnswerByQuestionId(questionId);

		if (existingAnswer) {
			emitUIChange('onAnswerSelected', {
				questionId: questionId,
				shouldScroll: true,
			});

			updateStatusMessage('ğŸ“Œ Essa pergunta jÃ¡ foi respondida');
			return;
		}
	}

	// Se for uma pergunta do histÃ³rico marcada como incompleta, nÃ£o enviar automaticamente ao GPT
	if (questionId !== CURRENT_QUESTION_ID) {
		const q = questionsHistory.find(q => q.id === questionId);
		if (q && q.incomplete) {
			updateStatusMessage('âš ï¸ Pergunta incompleta â€” pressione o botÃ£o de responder para enviar ao GPT');
			console.log('â„¹ï¸ pergunta incompleta selecionada â€” aguarda envio manual:', q.text);
			return;
		}
	}

	if (
		ModeController.isInterviewMode() &&
		selectedQuestionId === CURRENT_QUESTION_ID &&
		gptAnsweredTurnId === interviewTurnId
	) {
		updateStatusMessage('â›” GPT jÃ¡ respondeu esse turno');
		console.log('â›” GPT jÃ¡ respondeu esse turno');
		return;
	}

	// â“ Ainda nÃ£o respondida â†’ chama GPT (click ou atalho)
	//console.error('closeCurrentQuestion: askGpt() 2978; ğŸ”’ COMENTADA atÃ© transcriÃ§Ã£o em tempo real funcionar');
	askGpt(); // ğŸ”’ COMENTADA atÃ© transcriÃ§Ã£o em tempo real funcionar

	debugLogRenderer('Fim da funÃ§Ã£o: "handleQuestionClick"');
}

function applyOpacity(value) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "applyOpacity"');
	const appOpacity = parseFloat(value);

	// aplica opacidade no conteÃºdo geral
	document.documentElement.style.setProperty('--app-opacity', appOpacity.toFixed(2));

	// topBar nunca abaixo de 0.75
	const topbarOpacity = Math.max(appOpacity, 0.75);
	document.documentElement.style.setProperty('--app-opacity-75', topbarOpacity.toFixed(2));

	localStorage.setItem('overlayOpacity', appOpacity);

	// logs temporÃ¡rios para debug
	console.log('ğŸšï¸ Opacity change | app:', value, '| topBar:', topbarOpacity);

	debugLogRenderer('Fim da funÃ§Ã£o: "applyOpacity"');
}

// ğŸ”¥ Novo: atualizar status sem tocar em DOM
function updateStatusMessage(message) {
	debugLogRenderer('InÃ­cio da funÃ§Ã£o: "updateStatusMessage"');
	emitUIChange('onStatusUpdate', { message });
	debugLogRenderer('Fim da funÃ§Ã£o: "updateStatusMessage"');
}

/* ===============================
   SCREENSHOT CAPTURE - FUNÃ‡Ã•ES
=============================== */

/**
 * Captura screenshot discretamente e armazena em memÃ³ria
 */
async function captureScreenshot() {
	if (isCapturing) {
		console.log('â³ Captura jÃ¡ em andamento...');
		return;
	}

	isCapturing = true;
	updateStatusMessage('ğŸ“¸ Capturando tela...');

	try {
		const result = await ipcRenderer.invoke('CAPTURE_SCREENSHOT');

		if (!result.success) {
			console.warn('âš ï¸ Falha na captura:', result.error);
			updateStatusMessage(`âŒ ${result.error}`);
			emitUIChange('onScreenshotBadgeUpdate', {
				count: capturedScreenshots.length,
				visible: capturedScreenshots.length > 0,
			});
			return;
		}

		// âœ… Armazena referÃªncia do screenshot
		capturedScreenshots.push({
			filepath: result.filepath,
			filename: result.filename,
			timestamp: result.timestamp,
			size: result.size,
		});

		console.log(`âœ… Screenshot capturado: ${result.filename}`);
		console.log(`ğŸ“¦ Total em memÃ³ria: ${capturedScreenshots.length}`);

		// Atualiza UI
		updateStatusMessage(`âœ… ${capturedScreenshots.length} screenshot(s) capturado(s)`);
		emitUIChange('onScreenshotBadgeUpdate', {
			count: capturedScreenshots.length,
			visible: true,
		});
	} catch (error) {
		console.error('âŒ Erro ao capturar screenshot:', error);
		updateStatusMessage('âŒ Erro na captura');
	} finally {
		isCapturing = false;
	}
}

/**
 * Envia screenshots para anÃ¡lise com OpenAI Vision
 */
async function analyzeScreenshots() {
	if (isAnalyzing) {
		console.log('â³ AnÃ¡lise jÃ¡ em andamento...');
		return;
	}

	if (capturedScreenshots.length === 0) {
		console.warn('âš ï¸ Nenhum screenshot para analisar');
		updateStatusMessage('âš ï¸ Nenhum screenshot para analisar (capture com Ctrl+Shift+F)');
		return;
	}

	isAnalyzing = true;
	updateStatusMessage(`ğŸ” Analisando ${capturedScreenshots.length} screenshot(s)...`);

	try {
		// Extrai caminhos dos arquivos
		const filepaths = capturedScreenshots.map(s => s.filepath);

		console.log('ğŸš€ Enviando para anÃ¡lise:', filepaths);

		// Envia para main.js
		const result = await ipcRenderer.invoke('ANALYZE_SCREENSHOTS', filepaths);

		if (!result.success) {
			console.error('âŒ Falha na anÃ¡lise:', result.error);
			updateStatusMessage(`âŒ ${result.error}`);
			return;
		}

		// âœ… Renderiza resposta do GPT
		const questionText = `ğŸ“¸ AnÃ¡lise de ${capturedScreenshots.length} screenshot(s)`;
		// ğŸ”¢ USA ID SEQUENCIAL COMO AS PERGUNTAS NORMAIS (nÃ£o UUID)
		const questionId = String(questionsHistory.length + 1);

		// Adiciona "pergunta" ao histÃ³rico ANTES de renderizar respostas
		questionsHistory.push({
			id: questionId,
			text: questionText,
			createdAt: Date.now(),
			lastUpdateTime: Date.now(),
			answered: true,
		});

		// âœ… MARCA COMO RESPONDIDA (importante para clique nÃ£o gerar duplicata)
		answeredQuestions.add(questionId);

		renderQuestionsHistory();

		// âœ… RENDERIZA VIA STREAMING (fluxo real) - usa onAnswerStreamChunk como GPT normal
		// Divide anÃ¡lise em tokens e emite como se fosse stream
		const analysisText = result.analysis;
		const tokens = analysisText.split(/(\s+|[.,!?;:\-\(\)\[\]{}\n])/g).filter(t => t.length > 0);

		console.log(`ğŸ“¸ [ANÃLISE] Simulando stream: ${tokens.length} tokens`);

		// Emite tokens assim como o GPT faz (permite UI renderizar em tempo real)
		let accumulated = '';
		for (const token of tokens) {
			accumulated += token;

			// âœ… USA O MESMO EVENTO onAnswerStreamChunk (fluxo real)
			emitUIChange('onAnswerStreamChunk', {
				questionId: questionId,
				token: token,
				accum: accumulated,
			});

			// Pequeno delay entre tokens para simular streaming real
			await new Promise(resolve => setTimeout(resolve, 2));
		}

		console.log('âœ… AnÃ¡lise concluÃ­da e renderizada');
		updateStatusMessage('âœ… AnÃ¡lise concluÃ­da');

		// ğŸ—‘ï¸ Limpa screenshots apÃ³s anÃ¡lise
		console.log(`ğŸ—‘ï¸ Limpando ${capturedScreenshots.length} screenshot(s) da memÃ³ria...`);
		capturedScreenshots = [];

		// Atualiza badge
		emitUIChange('onScreenshotBadgeUpdate', {
			count: 0,
			visible: false,
		});

		// ForÃ§a limpeza no sistema
		await ipcRenderer.invoke('CLEANUP_SCREENSHOTS');
	} catch (error) {
		console.error('âŒ Erro ao analisar screenshots:', error);
		updateStatusMessage('âŒ Erro na anÃ¡lise');
	} finally {
		isAnalyzing = false;
	}
}

/**
 * Limpa todos os screenshots armazenados
 */
function clearScreenshots() {
	if (capturedScreenshots.length === 0) return;

	console.log(`ğŸ—‘ï¸ Limpando ${capturedScreenshots.length} screenshot(s)...`);
	capturedScreenshots = [];

	updateStatusMessage('âœ… Screenshots limpos');
	emitUIChange('onScreenshotBadgeUpdate', {
		count: 0,
		visible: false,
	});

	// ForÃ§a limpeza no sistema
	ipcRenderer.invoke('CLEANUP_SCREENSHOTS').catch(err => {
		console.warn('âš ï¸ Erro na limpeza:', err);
	});
}

/* ===============================
   BOOT
=============================== */

marked.setOptions({
	html: true, // ğŸ”¥ Permite renderizaÃ§Ã£o de HTML (nÃ£o escapa entidades)
	breaks: true,
	gfm: true, // GitHub Flavored Markdown
	highlight: function (code, lang) {
		if (lang && hljs.getLanguage(lang)) {
			return hljs.highlight(code, { language: lang }).value;
		}
		return hljs.highlightAuto(code).value;
	},
});

// Exporta funÃ§Ãµes pÃºblicas que o controller pode chamar
const RendererAPI = {
	// Ãudio - GravaÃ§Ã£o
	startInput,
	stopInput: stopInputMonitor,
	listenToggleBtn,
	askGpt,
	startOutput,
	stopOutput: stopOutputMonitor,
	restartAudioPipeline,

	// Ãudio - Monitoramento de volume
	startInputVolumeMonitoring,
	startOutputVolumeMonitoring,
	stopInputVolumeMonitoring,
	stopOutputVolumeMonitoring,
	// Entrevista - Reset (centralizado em resetAppState)
	resetAppState,

	// Modo
	changeMode: mode => {
		CURRENT_MODE = mode;
	},
	getMode: () => CURRENT_MODE,

	// Questions
	handleQuestionClick,
	closeCurrentQuestion,

	// UI
	applyOpacity,
	updateMockBadge: show => {
		emitUIChange('onMockBadgeUpdate', { visible: show });
	},
	setMockToggle: checked => {
		if (UIElements.mockToggle) {
			UIElements.mockToggle.checked = checked;
		}
		APP_CONFIG.MODE_DEBUG = checked;
	},
	setModeSelect: mode => {
		emitUIChange('onModeSelectUpdate', { mode });
	},

	// Drag
	initDragHandle: (dragHandle, documentElement) => {
		if (!dragHandle) return;
		const doc = documentElement || document; // fallback para document global
		dragHandle.addEventListener('pointerdown', async event => {
			console.log('ğŸªŸ Drag iniciado (pointerdown)');
			isDraggingWindow = true;
			dragHandle.classList.add('drag-active');

			const _pid = event.pointerId;
			try {
				dragHandle.setPointerCapture && dragHandle.setPointerCapture(_pid);
			} catch (err) {
				console.warn('setPointerCapture falhou:', err);
			}

			setTimeout(() => ipcRenderer.send('START_WINDOW_DRAG'), 40);

			const startBounds = (await ipcRenderer.invoke('GET_WINDOW_BOUNDS')) || {
				x: 0,
				y: 0,
			};
			const startCursor = { x: event.screenX, y: event.screenY };
			let lastAnimation = 0;

			function onPointerMove(ev) {
				const now = performance.now();
				if (now - lastAnimation < 16) return;
				lastAnimation = now;

				const dx = ev.screenX - startCursor.x;
				const dy = ev.screenY - startCursor.y;

				ipcRenderer.send('MOVE_WINDOW_TO', {
					x: startBounds.x + dx,
					y: startBounds.y + dy,
				});
			}

			function onPointerUp(ev) {
				try {
					dragHandle.removeEventListener('pointermove', onPointerMove);
					dragHandle.removeEventListener('pointerup', onPointerUp);
				} catch (err) {}

				if (dragHandle.classList.contains('drag-active')) {
					dragHandle.classList.remove('drag-active');
				}

				try {
					dragHandle.releasePointerCapture && dragHandle.releasePointerCapture(_pid);
				} catch (err) {}

				isDraggingWindow = false;
			}

			dragHandle.addEventListener('pointermove', onPointerMove);
			dragHandle.addEventListener('pointerup', onPointerUp, { once: true });
			event.stopPropagation();
		});

		doc.addEventListener('pointerup', () => {
			if (!dragHandle.classList.contains('drag-active')) return;
			console.log('ğŸªŸ Drag finalizado (pointerup)');
			dragHandle.classList.remove('drag-active');
			isDraggingWindow = false;
		});

		dragHandle.addEventListener('pointercancel', () => {
			if (dragHandle.classList.contains('drag-active')) {
				dragHandle.classList.remove('drag-active');
				isDraggingWindow = false;
			}
		});
	},

	// Click-through
	setClickThrough: enabled => {
		ipcRenderer.send('SET_CLICK_THROUGH', enabled);
	},
	updateClickThroughButton: (enabled, btnToggle) => {
		if (!btnToggle) return;
		btnToggle.style.opacity = enabled ? '0.5' : '1';
		btnToggle.title = enabled
			? 'Click-through ATIVO (clique para desativar)'
			: 'Click-through INATIVO (clique para ativar)';
		console.log('ğŸ¨ BotÃ£o atualizado - opacity:', btnToggle.style.opacity);
	},

	// UI Registration
	registerUIElements: elements => {
		registerUIElements(elements);
	},
	onUIChange: (eventName, callback) => {
		onUIChange(eventName, callback);
	},

	// API Key
	setAppConfig: config => {
		APP_CONFIG = config;
	},
	getAppConfig: () => APP_CONFIG,

	// Navegacao de perguntas (Ctrl+Shift+ArrowUp/Down via globalShortcut IPC)
	navigateQuestions: direction => {
		const all = getNavigableQuestionIds();
		if (all.length === 0) return;

		let index = all.indexOf(selectedQuestionId);
		if (index === -1) {
			index = direction === 'up' ? all.length - 1 : 0;
		} else {
			index += direction === 'up' ? -1 : 1;
			index = Math.max(0, Math.min(index, all.length - 1));
		}

		selectedQuestionId = all[index];
		clearAllSelections();
		renderQuestionsHistory();
		renderCurrentQuestion();

		if (APP_CONFIG.MODE_DEBUG) {
			const msg = direction === 'up' ? 'ğŸ§ª Ctrl+ArrowUp detectado (teste)' : 'ğŸ§ª Ctrl+ArrowDown detectado (teste)';
			updateStatusMessage(msg);
			console.log('ğŸ“Œ Atalho Selecionou:', selectedQuestionId);
		}
	},

	// IPC Listeners
	onApiKeyUpdated: callback => {
		ipcRenderer.on('API_KEY_UPDATED', callback);
	},
	onToggleAudio: callback => {
		ipcRenderer.on('CMD_TOGGLE_AUDIO', callback);
	},
	onAskGpt: callback => {
		ipcRenderer.on('CMD_ASK_GPT', callback);
	},
	onGptStreamChunk: callback => {
		ipcRenderer.on('GPT_STREAM_CHUNK', callback);
	},
	onGptStreamEnd: callback => {
		ipcRenderer.on('GPT_STREAM_END', callback);
	},
	sendRendererError: error => {
		try {
			console.error('RENDERER ERROR', error.error || error.message || error);
			ipcRenderer.send('RENDERER_ERROR', {
				message: String(error.message || error),
				stack: error.error?.stack || null,
			});
		} catch (err) {
			console.error('Falha ao enviar RENDERER_ERROR', err);
		}
	},

	// ğŸ“¸ NOVO: Screenshot functions
	captureScreenshot,
	analyzeScreenshots,
	clearScreenshots,
	getScreenshotCount: () => capturedScreenshots.length,

	// ğŸ“¸ NOVO: Screenshot shortcuts
	onCaptureScreenshot: callback => {
		ipcRenderer.on('CMD_CAPTURE_SCREENSHOT', callback);
	},
	onAnalyzeScreenshots: callback => {
		ipcRenderer.on('CMD_ANALYZE_SCREENSHOTS', callback);
	},
	// Navegacao de perguntas (Ctrl+Shift+ArrowUp/Down via globalShortcut)
	onNavigateQuestions: callback => {
		ipcRenderer.on('CMD_NAVIGATE_QUESTIONS', (_, direction) => {
			callback(direction);
		});
	},

	// Emit UI changes (para config-manager enviar eventos para renderer)
	emitUIChange,
};

if (typeof module !== 'undefined' && module.exports) {
	module.exports = RendererAPI;
}

// ğŸ”¥ Expor globalmente para que config-manager possa acessar
if (typeof globalThis !== 'undefined') {
	globalThis.RendererAPI = RendererAPI;
	globalThis.runMockAutoPlay = runMockAutoPlay; // ğŸ­ Exportar Mock autoplay
	globalThis.mockScenarioIndex = 0; // ğŸ­ Ãndice global para cenÃ¡rios
	globalThis.mockAutoPlayActive = false; // ğŸ­ Flag global para evitar mÃºltiplas execuÃ§Ãµes
}
function debugLogRenderer(msg) {
	console.log('%cğŸª² â¯â¯â¯â¯ Debug: ' + msg + ' em renderer.js', 'color: brown; font-weight: bold;');
}

/* ===============================
   FUNÃ‡ÃƒO PARA LOGAR MÃ‰TRICAS
=============================== */

function logTranscriptionMetrics() {
	if (!transcriptionMetrics.audioStartTime) return;

	const whisperTime = transcriptionMetrics.whisperEndTime - transcriptionMetrics.whisperStartTime;
	const gptTime = transcriptionMetrics.gptEndTime - transcriptionMetrics.gptStartTime;
	const totalTime = transcriptionMetrics.totalTime;

	console.log(`ğŸ“Š ================================`);
	console.log(`ğŸ“Š MÃ‰TRICAS DE TEMPO DETALHADAS:`);
	console.log(`ğŸ“Š ================================`);
	console.log(`ğŸ“Š TAMANHO ÃUDIO: ${transcriptionMetrics.audioSize} bytes`);
	console.log(`ğŸ“Š WHISPER: ${whisperTime}ms (${Math.round(transcriptionMetrics.audioSize / whisperTime)} bytes/ms)`);
	console.log(`ğŸ“Š GPT: ${gptTime}ms`);
	console.log(`ğŸ“Š TOTAL: ${totalTime}ms`);
	console.log(`ğŸ“Š WHISPER % DO TOTAL: ${Math.round((whisperTime / totalTime) * 100)}%`);
	console.log(`ğŸ“Š GPT % DO TOTAL: ${Math.round((gptTime / totalTime) * 100)}%`);
	console.log(`ğŸ“Š ================================`);

	// Reset para prÃ³xima mediÃ§Ã£o
	transcriptionMetrics = {
		audioStartTime: null,
		whisperStartTime: null,
		whisperEndTime: null,
		gptStartTime: null,
		gptEndTime: null,
		totalTime: null,
		audioSize: 0,
	};
}

/* ===============================
   RESET COMPLETO (TEMPORÃRIO PARA TESTES)
=============================== */

/**
 * ğŸ”„ Limpa tudo na seÃ§Ã£o home como se o app tivesse aberto agora
 * Funcionalidade TEMPORÃRIA para facilitar testes sem fechar a aplicaÃ§Ã£o
 */
function resetHomeSection() {
	console.log('\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
	console.log('ğŸ”„ RESET COMPLETO ACIONADO PELO BOTÃƒO resetHomeBtn');
	console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');

	// ğŸ”¥ Usar a funÃ§Ã£o centralizada de reset
	resetAppState().then(success => {
		if (success) {
			console.log('âœ… Reset via resetAppState() concluÃ­do com sucesso!');
		} else {
			console.error('âŒ Erro ao executar resetAppState()');
		}
		console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');
	});
}

// ğŸ”¥ LISTENER DO BOTÃƒO RESET
document.addEventListener('DOMContentLoaded', () => {
	const resetBtn = document.getElementById('resetHomeBtn');
	if (resetBtn) {
		resetBtn.addEventListener('click', () => {
			const confirmed = confirm('âš ï¸ Isso vai limpar toda transcriÃ§Ã£o, histÃ³rico e respostas.\n\nTem certeza?');
			if (confirmed) {
				resetHomeSection();
			}
		});
		console.log('âœ… Listener do botÃ£o reset instalado');
	} else {
		console.warn('âš ï¸ BotÃ£o reset nÃ£o encontrado no DOM');
	}
});

/* ===============================
   MOCK / DEBUG
=============================== */

/* ===============================
   ğŸ­ MOCK SYSTEM - Intercepta ipcRenderer
   Quando MODE_DEBUG=true, substitui respostas reais por mocks
=============================== */

// ğŸ” Respostas mockadas por pergunta
const MOCK_RESPONSES = {
	'Mock - O que Ã© JVM e para que serve?':
		'Mock - A JVM (Java Virtual Machine) Ã© uma mÃ¡quina virtual que executa bytecode Java. Ela permite que programas Java rodem em qualquer plataforma sem modificaÃ§Ã£o. A JVM gerencia memÃ³ria, garbage collection e fornece um ambiente isolado e seguro para execuÃ§Ã£o de cÃ³digo.',
	'Mock - Qual a diferenÃ§a entre JDK e JRE?':
		'Mock - JDK (Java Development Kit) Ã© o kit completo para desenvolvimento, incluindo compilador, ferramentas e bibliotecas. JRE (Java Runtime Environment) contÃ©m apenas o necessÃ¡rio para executar aplicaÃ§Ãµes Java compiladas. Todo desenvolvedor precisa do JDK, mas usuÃ¡rios finais precisam apenas da JRE.',
	'Mock - O que Ã© uma classe em Java?':
		'Mock - Uma classe Ã© o molde ou blueprint para criar objetos. Define atributos (propriedades) e mÃ©todos (comportamentos). As classes sÃ£o fundamentais na programaÃ§Ã£o orientada a objetos. Por exemplo, uma classe Carro pode ter atributos como cor e velocidade, e mÃ©todos como acelerar e frear.',
	'Mock - Explique sobre heranÃ§a em Java':
		'Mock - HeranÃ§a permite que uma classe herde propriedades e mÃ©todos de outra classe. A classe filha estende a classe pai usando a palavra-chave extends. Isso promove reutilizaÃ§Ã£o de cÃ³digo e cria uma hierarquia de classes. Por exemplo, a classe Bicicleta pode herdar de Veiculo.',
	'Mock - Como funciona polimorfismo?':
		'Mock - Polimorfismo significa muitas formas. Permite que objetos de diferentes tipos respondam a mesma chamada de mÃ©todo de forma diferente. Pode ser atravÃ©s de sobrescrita de mÃ©todos (heranÃ§a) ou interface. Exemplo: diferentes animais implementam o mÃ©todo fazer_som() diferentemente.',
	'Mock - O que Ã© encapsulamento?':
		'Mock - Encapsulamento Ã© o princÃ­pio de ocultar detalhes internos da implementaÃ§Ã£o. Usa modificadores de acesso como private, protected e public. Protege dados e mÃ©todos crÃ­ticos, permitindo controle sobre como sÃ£o acessados. Ã‰ uma pilar da seguranÃ§a e manutenÃ§Ã£o do cÃ³digo orientado a objetos.',
};

// ğŸ¬ CenÃ¡rios automÃ¡ticos para teste
// screenshotsCount: 0 = sem screenshot, 1 = tira 1 foto, 2 = tira 2 fotos, etc
const MOCK_SCENARIOS = [
	{ question: 'Mock - O que Ã© JVM e para que serve?', screenshotsCount: 1 },
	{ question: 'Mock - Qual a diferenÃ§a entre JDK e JRE?', screenshotsCount: 0 },
	{ question: 'Mock - O que Ã© uma classe em Java?', screenshotsCount: 0 },
	{ question: 'Mock - Explique sobre heranÃ§a em Java', screenshotsCount: 2 },
	{ question: 'Mock - Como funciona polimorfismo?', screenshotsCount: 0 },
	{ question: 'Mock - O que Ã© encapsulamento?', screenshotsCount: 0 },
];

let mockScenarioIndex = 0;
let mockAutoPlayActive = false;

/**
 * ğŸ­ Retorna resposta mockada para pergunta (busca exata ou parcial)
 */
function getMockResponse(question) {
	// Match exato
	if (MOCK_RESPONSES[question]) {
		return MOCK_RESPONSES[question];
	}

	// Match parcial
	for (const [key, value] of Object.entries(MOCK_RESPONSES)) {
		if (question.toLowerCase().includes(key.toLowerCase())) {
			return value;
		}
	}

	// Fallback
	return `Resposta mockada para: "${question}"\n\nEste Ã© um teste do sistema em modo Mock.`;
}

/**
 * ğŸ­ Intercepta ipcRenderer.invoke para mockar 'ask-gpt-stream'
 * Emite eventos com pequenos delays para permitir processamento
 */
const originalInvoke = ipcRenderer.invoke;
ipcRenderer.invoke = function (channel, ...args) {
	// Intercepta anÃ¡lise de screenshots quando MODE_DEBUG
	// IMPORTANTE: CAPTURE_SCREENSHOT Ã© REAL (tira foto mesmo), ANALYZE_SCREENSHOTS Ã© MOCK (simula resposta)
	if (channel === 'ANALYZE_SCREENSHOTS' && APP_CONFIG.MODE_DEBUG) {
		console.log('ğŸ“¸ [MOCK] Interceptando ANALYZE_SCREENSHOTS...');
		const filepaths = args[0] || [];
		const screenshotCount = filepaths.length;

		// Retorna anÃ¡lise mockada
		const mockAnalysis = `
## ğŸ“¸ AnÃ¡lise de ${screenshotCount} Screenshot(s) - MOCK

### Esta Ã© uma resposta simulada para o teste do sistema.

Para resolver o problema apresentado na captura de tela, que Ã© o "Remove Element" do LeetCode, vamos implementar uma funÃ§Ã£o em Java que remove todas as ocorrÃªncias de um valor especÃ­fico de um array. A funÃ§Ã£o deve modificar o array in-place e retornar o novo comprimento do array.

Resumo do Problema
Entrada: Um array de inteiros nums e um inteiro val que queremos remover.
SaÃ­da: O novo comprimento do array apÃ³s remover todas as ocorrÃªncias de val.
Passos para a SoluÃ§Ã£o
Iterar pelo array: Vamos percorrer o array e verificar cada elemento.
Manter um Ã­ndice: Usaremos um Ã­ndice para rastrear a posiÃ§Ã£o onde devemos colocar os elementos que nÃ£o sÃ£o iguais a val.
Modificar o array in-place: Sempre que encontrarmos um elemento que nÃ£o Ã© igual a val, colocamos esse elemento na posiÃ§Ã£o do Ã­ndice e incrementamos o Ã­ndice.
Retornar o comprimento: No final, o Ã­ndice representarÃ¡ o novo comprimento do array.
ImplementaÃ§Ã£o do CÃ³digo
Aqui estÃ¡ a implementaÃ§Ã£o em Java:

class Solution {
    public int removeElement(int[] nums, int val) {
        // Inicializa um Ã­ndice para rastrear a nova posiÃ§Ã£o
        int index = 0;

		// Percorre todos os elementos do array
		for (int i = 0; i &lt; nums.length; i++) {
			// Se o elemento atual nÃ£o Ã© igual a val
			if (nums[i] != val) {
				// Coloca o elemento na posiÃ§Ã£o do Ã­ndice
				nums[index] = nums[i];
				// Incrementa o Ã­ndice
				index++;
			}
		}

		// Retorna o novo comprimento do array
		return index;
	}
}

ExplicaÃ§Ã£o do CÃ³digo
Classe e MÃ©todo: Criamos uma classe chamada Solution e um mÃ©todo removeElement que recebe um array de inteiros nums e um inteiro val.
Ãndice Inicial: Inicializamos uma variÃ¡vel index em 0.
		`;

		return Promise.resolve({
			success: true,
			analysis: mockAnalysis,
			filesAnalyzed: screenshotCount,
			timestamp: Date.now(),
		});
	}

	// Intercepta ask-gpt-stream quando MODE_DEBUG
	if (channel === 'ask-gpt-stream' && APP_CONFIG.MODE_DEBUG) {
		console.log('ğŸ­ [MOCK] Interceptando ask-gpt-stream...');

		// ObtÃ©m a pergunta do primeiro argumento (array de mensagens)
		const messages = args[0] || [];
		const userMessage = messages.find(m => m.role === 'user');
		const questionText = userMessage ? userMessage.content : 'Pergunta desconhecida';

		// Busca resposta mockada
		const mockResponse = getMockResponse(questionText);

		// Divide em tokens (remove vazios)
		const tokens = mockResponse.split(/(\s+|[.,!?;:\-\(\)\[\]{}\n])/g).filter(t => t.length > 0);

		console.log(`ğŸ­ [MOCK] Emitindo ${tokens.length} tokens para pergunta: "${questionText.substring(0, 50)}..."`);

		// FunÃ§Ã£o para emitir tokens com pequeno delay entre eles
		async function emitTokens() {
			let accumulated = '';
			for (let i = 0; i < tokens.length; i++) {
				const token = tokens[i];
				accumulated += token;

				// Emite o evento com delay mÃ­nimo
				await new Promise(resolve => {
					setTimeout(() => {
						// âœ… CORRETO: Emite apenas o token como 2Âº argumento
						ipcRenderer.emit('GPT_STREAM_CHUNK', null, token);
						resolve();
					}, 5); // 5ms entre tokens
				});
			}

			// Sinaliza fim do stream apÃ³s todos os tokens
			await new Promise(resolve => {
				setTimeout(() => {
					ipcRenderer.emit('GPT_STREAM_END');
					resolve();
				}, 10);
			});
		}

		// Inicia emissÃ£o de tokens de forma assÃ­ncrona
		emitTokens().catch(err => {
			console.error('âŒ Erro ao emitir tokens mock:', err);
		});

		// Retorna promise resolvida imediatamente (esperado pela API)
		return Promise.resolve({ success: true });
	}

	// Todas as outras chamadas passam para o invoke real
	return originalInvoke.call(this, channel, ...args);
};

/**
 * ğŸ­ Executa cenÃ¡rios de entrevista mock automaticamente
 */
async function runMockAutoPlay() {
	if (mockAutoPlayActive) return;
	mockAutoPlayActive = true;

	while (mockScenarioIndex < MOCK_SCENARIOS.length && APP_CONFIG.MODE_DEBUG && mockAutoPlayActive) {
		const scenario = MOCK_SCENARIOS[mockScenarioIndex];
		console.log(
			`\nğŸ¬ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nğŸ¬ MOCK CENÃRIO ${mockScenarioIndex + 1}/${
				MOCK_SCENARIOS.length
			}\nğŸ¬ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•`,
		);

		// FASE 1: Simula captura de Ã¡udio (2-4s)
		console.log(`ğŸ¤ [FASE-1] Capturando Ã¡udio da pergunta...`);
		const audioStartTime = Date.now();
		const placeholderId = `placeholder-${audioStartTime}-${Math.random()}`;

		// Emite placeholder
		emitUIChange('onTranscriptAdd', {
			author: 'Outros',
			text: '...',
			timeStr: new Date().toLocaleTimeString(),
			elementId: 'conversation',
			placeholderId: placeholderId,
		});

		// Aguarda captura
		await new Promise(resolve => setTimeout(resolve, 2000 + Math.random() * 2000));

		// ğŸ”¥ CHECK: Se modo debug foi desativado, para imediatamente
		if (!APP_CONFIG.MODE_DEBUG || !mockAutoPlayActive) {
			console.log('ğŸ›‘ [PARADA] Modo debug desativado - parando mock autoplay');
			break;
		}

		const audioEndTime = Date.now();
		console.log(`âœ… [FASE-1] Ãudio capturado`);

		// Calcula latÃªncia (arredonda para inteiro - sem casas decimais)
		const latencyMs = Math.round(800 + Math.random() * 400);
		const totalMs = audioEndTime - audioStartTime + latencyMs;

		// Atualiza placeholder com texto real
		emitUIChange('onPlaceholderFulfill', {
			speaker: 'Outros',
			text: scenario.question,
			startStr: new Date(audioStartTime).toLocaleTimeString(),
			stopStr: new Date(audioEndTime).toLocaleTimeString(),
			recordingDuration: audioEndTime - audioStartTime,
			latency: latencyMs,
			total: totalMs,
			placeholderId: placeholderId,
		});

		// FASE 2: Processa pergunta (handleSpeech + closeCurrentQuestion)
		console.log(`ğŸ“ [FASE-2] Processando pergunta...`);
		handleSpeech(OTHER, scenario.question);

		// Aguarda consolidaÃ§Ã£o (800ms para garantir que pergunta saia do CURRENT)
		await new Promise(resolve => setTimeout(resolve, 800));

		// ğŸ”¥ CHECK: Se modo debug foi desativado, para imediatamente
		if (!APP_CONFIG.MODE_DEBUG || !mockAutoPlayActive) {
			console.log('ğŸ›‘ [PARADA] Modo debug desativado - parando mock autoplay');
			break;
		}

		// Simula silÃªncio e fecha pergunta
		console.log(`ğŸ”‡ [FASE-2] SilÃªncio detectado, fechando pergunta...`);
		closeCurrentQuestion();

		// FASE 3: askGpt serÃ¡ acionado automaticamente, o interceptor (ask-gpt-stream) que irÃ¡ mockar
		console.log(`ğŸ¤– [FASE-3] askGpt acionado - mock stream serÃ¡ emitido pelo interceptor`);

		// Aguarda stream terminar (~30ms por token)
		const mockResponse = getMockResponse(scenario.question);
		const estimatedTime = mockResponse.length * 30;
		await new Promise(resolve => setTimeout(resolve, estimatedTime + 1000));

		// ğŸ”¥ CHECK: Se modo debug foi desativado, para imediatamente SEM TIRAR SCREENSHOT
		if (!APP_CONFIG.MODE_DEBUG || !mockAutoPlayActive) {
			console.log('ğŸ›‘ [PARADA] Modo debug desativado - parando sem capturar screenshot');
			break;
		}

		// FASE 4 (Opcional): Captura N screenshots REAIS e depois aciona anÃ¡lise
		if (scenario.screenshotsCount && scenario.screenshotsCount > 0) {
			// FASE 4A: Captura mÃºltiplos screenshots
			for (let i = 1; i <= scenario.screenshotsCount; i++) {
				// ğŸ”¥ CHECK: Verifica antes de cada screenshot
				if (!APP_CONFIG.MODE_DEBUG || !mockAutoPlayActive) {
					console.log(
						`ğŸ›‘ [PARADA] Modo debug desativado - cancelando captura de screenshot ${i}/${scenario.screenshotsCount}`,
					);
					break;
				}

				console.log(`ğŸ“¸ [FASE-4A] Capturando screenshot ${i}/${scenario.screenshotsCount} REAL da resposta...`);
				await captureScreenshot();

				// Delay entre mÃºltiplas capturas para respeitar cooldown de 2s do main.js
				if (i < scenario.screenshotsCount) {
					console.log(`   â³ Aguardando 2200ms antes da prÃ³xima captura (cooldown CAPTURE_COOLDOWN)...`);
					await new Promise(resolve => setTimeout(resolve, 2200));
				}
			}

			// ğŸ”¥ CHECK: Verifica antes de anÃ¡lise
			if (!APP_CONFIG.MODE_DEBUG || !mockAutoPlayActive) {
				console.log('ğŸ›‘ [PARADA] Modo debug desativado - cancelando anÃ¡lise de screenshots');
				break;
			}

			// Log de validaÃ§Ã£o: quantas fotos tem antes de analisar
			console.log(
				`ğŸ“¸ [PRÃ‰-ANÃLISE] Total de screenshots em memÃ³ria: ${capturedScreenshots.length}/${scenario.screenshotsCount}`,
			);

			// FASE 4B: AnÃ¡lise dos screenshots capturados
			console.log(`ğŸ“¸ [FASE-4B] Analisando ${scenario.screenshotsCount} screenshot(s)...`);
			await analyzeScreenshots();
		}

		mockScenarioIndex++;

		if (mockScenarioIndex < MOCK_SCENARIOS.length) {
			console.log(`\nâ³ Aguardando 1s antes do prÃ³ximo cenÃ¡rio...\n`);
			await new Promise(resolve => setTimeout(resolve, 1000));
		}
	}

	console.log('âœ… Mock autoplay finalizado');
	mockAutoPlayActive = false;
}

//console.log('ğŸš€ Entrou no renderer.js');
