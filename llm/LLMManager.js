/**
 * LLMManager - Orquestrador de modelos LLM com resil√™ncia
 *
 * ‚úÖ Features:
 * - Timeout configur√°vel para evitar travamentos
 * - Retry com backoff exponencial para falhas tempor√°rias
 * - Error handling estruturado com Logger
 * - Suporte para m√∫ltiplos providers (OpenAI, Gemini, Anthropic, etc)
 *
 * Uso:
 * const LLM = new LLMManager();
 * LLM.register('openai', openaiHandler);
 * LLM.register('gemini', geminiHandler);
 * const response = await LLM.complete(model, messages);
 */

const Logger = require('../utils/Logger.js');

class LLMManager {
	constructor(options = {}) {
		this.handlers = {};

		// Configura√ß√µes padr√£o (podem ser overridadas por handler individual)
		this.config = {
			timeout: options.timeout || 60000, // 60s padr√£o
			maxRetries: options.maxRetries || 3,
			retryDelayMs: options.retryDelayMs || 1000,
			backoffMultiplier: options.backoffMultiplier || 2,
		};

		Logger.info('LLMManager inicializado', {
			timeout: this.config.timeout,
			maxRetries: this.config.maxRetries,
		});
	}

	/**
	 * Registra handler de LLM
	 * @param {string} name - Nome do modelo (ex: 'openai')
	 * @param {object} handler - Handler com m√©todos: complete(), stream()
	 * @param {object} options - Op√ß√µes do handler (timeout, maxRetries, etc)
	 */
	register(name, handler, options = {}) {
		if (!handler.complete || !handler.stream) {
			throw new Error(`‚ùå Handler ${name} deve ter: complete(), stream()`);
		}

		this.handlers[name] = {
			instance: handler,
			// Permite override de configura√ß√µes por handler
			config: {
				timeout: options.timeout || this.config.timeout,
				maxRetries: options.maxRetries || this.config.maxRetries,
				retryDelayMs: options.retryDelayMs || this.config.retryDelayMs,
				backoffMultiplier: options.backoffMultiplier || this.config.backoffMultiplier,
			},
		};

		Logger.info(`‚úÖ LLM registrado: ${name}`, {
			timeout: this.handlers[name].config.timeout,
			maxRetries: this.handlers[name].config.maxRetries,
		});
	}

	/**
	 * Obt√©m handler
	 */
	getHandler(name) {
		const handlerWrapper = this.handlers[name];
		if (!handlerWrapper) {
			const available = Object.keys(this.handlers).join(', ');
			throw new Error(`‚ùå LLM n√£o encontrado: ${name}. Dispon√≠veis: ${available}`);
		}
		return handlerWrapper.instance;
	}

	/**
	 * Obt√©m configura√ß√£o do handler
	 */
	getHandlerConfig(name) {
		const handlerWrapper = this.handlers[name];
		if (!handlerWrapper) {
			throw new Error(`‚ùå LLM n√£o encontrado: ${name}`);
		}
		return handlerWrapper.config;
	}

	/**
	 * Timeout wrapper para Promises
	 * @param {Promise} promise - Promise a executar
	 * @param {number} timeoutMs - Timeout em milissegundos
	 * @throws {Error} Se timeout ultrapassado
	 */
	async _withTimeout(promise, timeoutMs) {
		return Promise.race([
			promise,
			new Promise((_, reject) => setTimeout(() => reject(new Error(`‚è±Ô∏è Timeout ap√≥s ${timeoutMs}ms`)), timeoutMs)),
		]);
	}

	/**
	 * Retry wrapper com backoff exponencial
	 * @param {Function} asyncFn - Fun√ß√£o ass√≠ncrona a executar
	 * @param {object} config - {maxRetries, retryDelayMs, backoffMultiplier}
	 */
	async _withRetry(asyncFn, config) {
		let lastError;
		for (let attempt = 0; attempt <= config.maxRetries; attempt++) {
			try {
				return await asyncFn();
			} catch (error) {
				lastError = error;
				const isLastAttempt = attempt === config.maxRetries;
				const isRetryable = this._isRetryableError(error);

				Logger.warn(`üîÑ Tentativa ${attempt + 1} falhou`, {
					error: error.message,
					isRetryable,
					maxRetries: config.maxRetries,
				});

				if (isLastAttempt || !isRetryable) {
					break;
				}

				// Backoff exponencial: delay = baseDelay * (multiplier ^ attempt)
				const delayMs = config.retryDelayMs * Math.pow(config.backoffMultiplier, attempt);
				await new Promise(resolve => setTimeout(resolve, delayMs));
			}
		}

		throw lastError;
	}

	/**
	 * Determina se erro √© retent√°vel
	 * Erros de rede/timeout: sim
	 * Erros de autentica√ß√£o/valida√ß√£o: n√£o
	 */
	_isRetryableError(error) {
		const message = error.message?.toLowerCase() || '';
		const nonRetryable = [
			'unauthorized', // 401
			'forbidden', // 403
			'not found', // 404
			'n√£o autenticado',
			'api key',
			'invalid',
			'pergunta vazia',
			'j√° foi respondida',
		];

		return !nonRetryable.some(keyword => message.includes(keyword));
	}

	/**
	 * Obt√©m resposta completa (batch) com timeout e retry
	 */
	async complete(model, messages) {
		const handler = this.getHandler(model);
		const config = this.getHandlerConfig(model);

		Logger.info(`üì§ Complete LLM: ${model}`, {
			messagesCount: messages.length,
			timeout: config.timeout,
		});

		try {
			const result = await this._withRetry(async () => {
				return await this._withTimeout(handler.complete(messages), config.timeout);
			}, config);

			Logger.info(`‚úÖ Complete LLM conclu√≠do: ${model}`);
			return result;
		} catch (error) {
			Logger.error(`‚ùå Erro em complete(${model})`, {
				error: error.message,
				stack: error.stack,
			});
			throw error;
		}
	}

	/**
	 * Obt√©m resposta com streaming com timeout e retry
	 */
	async stream(model, messages) {
		const handler = this.getHandler(model);
		const config = this.getHandlerConfig(model);

		Logger.info(`üì§ Stream LLM: ${model}`, {
			messagesCount: messages.length,
			timeout: config.timeout,
		});

		try {
			// Para stream, timeout se aplica √† inicializa√ß√£o, n√£o ao stream completo
			const streamGenerator = await this._withTimeout(handler.stream(messages), config.timeout);

			Logger.info(`‚úÖ Stream LLM iniciado: ${model}`);
			return streamGenerator;
		} catch (error) {
			Logger.error(`‚ùå Erro em stream(${model})`, {
				error: error.message,
				stack: error.stack,
			});
			throw error;
		}
	}
}

if (typeof module !== 'undefined' && module.exports) {
	module.exports = LLMManager;
}
